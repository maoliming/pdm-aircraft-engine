{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training The NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working directory in /Users/syahrulhamdani/Desktop/thesis/predictive-maintenance-of-aircraft-engine..\n"
     ]
    }
   ],
   "source": [
    "# import things\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "# change directory\n",
    "os.chdir('/Users/syahrulhamdani/Desktop/thesis/predictive-maintenance-of-aircraft-engine/')\n",
    "print('working directory in {}..'.format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "from src.data import LoadData, list_dataset\n",
    "from src.models import NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_FD001': 'train_FD001.txt',\n",
       " 'train_FD003': 'train_FD003.txt',\n",
       " 'RUL_FD004': 'RUL_FD004.txt',\n",
       " 'train_FD002': 'train_FD002.txt',\n",
       " 'RUL_FD001': 'RUL_FD001.txt',\n",
       " 'RUL_FD003': 'RUL_FD003.txt',\n",
       " 'RUL_FD002': 'RUL_FD002.txt',\n",
       " 'train_FD004': 'train_FD004.txt',\n",
       " 'test_FD003': 'test_FD003.txt',\n",
       " 'test_FD002': 'test_FD002.txt',\n",
       " 'test_FD001': 'test_FD001.txt',\n",
       " 'test_FD004': 'test_FD004.txt'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datalist = list_dataset()\n",
    "datalist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EngineID',\n",
       " 'Cycle',\n",
       " 'OpSetting1',\n",
       " 'OpSetting2',\n",
       " 'OpSetting3',\n",
       " 'T2',\n",
       " 'T24',\n",
       " 'T30',\n",
       " 'T50',\n",
       " 'P2',\n",
       " 'P15',\n",
       " 'P30',\n",
       " 'Nf',\n",
       " 'Nc',\n",
       " 'epr',\n",
       " 'Ps30',\n",
       " 'phi',\n",
       " 'NRf',\n",
       " 'NRc',\n",
       " 'BPR',\n",
       " 'farB',\n",
       " 'htBleed',\n",
       " 'Nf_dmd',\n",
       " 'PCNfR_dmd',\n",
       " 'W31',\n",
       " 'W32']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('references/col_to_feat.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "feature_names = list(data.values())\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20631, 26) (20631,)\n"
     ]
    }
   ],
   "source": [
    "data = LoadData(datalist['train_FD001'], names=feature_names, sep='\\s+')\n",
    "print(data.features.shape, data.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.save_interim('data/interim', names=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing preprocessing above, now we can define the training step as follows:\n",
    "1. Define the model with required arguments\n",
    "2. Initialize model hyperparameters\n",
    "3. Initialize criterion and optimizer\n",
    "4. Build the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20631, 24)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.features[:, 2:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensor (both features and targets)\n",
    "featureset = torch.from_numpy(data.features[:,2:])\n",
    "labelset = torch.from_numpy(data.target)\n",
    "\n",
    "# Load into dataloader (create loader both for feature and label without shuffling when loading)\n",
    "featureloader = torch.utils.data.DataLoader(featureset, batch_size=26)\n",
    "labelloader = torch.utils.data.DataLoader(labelset, batch_size=26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    featureloader,\n",
    "    labelloader,\n",
    "    epochs=5,\n",
    "    print_every=40\n",
    "):\n",
    "    epoch_loss = 0\n",
    "    steps = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for features, labels in zip(featureloader, labelloader):\n",
    "            steps += 1\n",
    "            features, labels = features.type(torch.FloatTensor), labels.type(torch.FloatTensor)\n",
    "            labels.resize_(labels.shape[0], 1)\n",
    "            \n",
    "            output = model.forward(features)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            if steps % print_every == 0:\n",
    "                print('Epoch: {}/{}..'.format(epoch+1, epochs),\n",
    "                      'Training loss: {:.3f}'.format(epoch_loss/print_every))\n",
    "\n",
    "                epoch_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    \"\"\"Neural Network Generator.\"\"\"\n",
    "    def __init__(self, input_size, output_size, drop_p, hidden_sizes=[30, 15]):\n",
    "        \"\"\"Generate fully-connected neural network.\n",
    "\n",
    "        parameters\n",
    "        ----------\n",
    "        input_size (int): size of the input\n",
    "        hidden_sizes (list of int): size of the hidden layers\n",
    "        output_layer (int): size of the output layer\n",
    "        drop_p (float): dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Linear(input_size, hidden_sizes[0])\n",
    "        ])\n",
    "        layers = zip(hidden_sizes[:-1], hidden_sizes[1:])\n",
    "        self.hidden_layers.extend([nn.Linear(h1,h2) for h1,h2 in layers])\n",
    "        self.output = nn.Linear(hidden_sizes[-1], output_size)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = F.sigmoid(self.hidden_layers[0](X))\n",
    "        X = F.relu(self.hidden_layers[1](X))\n",
    "        X = self.output(X)\n",
    "\n",
    "        return F.relu(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NN(\n",
       "  (hidden_layers): ModuleList(\n",
       "    (0): Linear(in_features=24, out_features=30, bias=True)\n",
       "    (1): Linear(in_features=30, out_features=15, bias=True)\n",
       "  )\n",
       "  (output): Linear(in_features=15, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.6)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NN(data.features[:,2:].shape[1], 1, 0.6)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1207,  0.0471,  0.1043,  0.0299, -0.0484, -0.1376, -0.0278,  0.1144,\n",
      "          0.1431, -0.1252, -0.0893,  0.1281, -0.0762, -0.0864,  0.1334,  0.0138,\n",
      "          0.1178, -0.1651, -0.1836, -0.1306, -0.0981,  0.1644,  0.0123,  0.1227],\n",
      "        [ 0.0880, -0.1577, -0.1467,  0.1576, -0.0168, -0.1775, -0.1326,  0.1045,\n",
      "          0.0675,  0.1972,  0.1339, -0.0182,  0.0996, -0.1759,  0.1979,  0.0365,\n",
      "         -0.1898,  0.1890,  0.0852, -0.0679,  0.0930, -0.1474, -0.1149,  0.1184],\n",
      "        [-0.1668, -0.2031,  0.1146, -0.0452,  0.0125,  0.0664,  0.1021, -0.2012,\n",
      "          0.0670,  0.1502,  0.0364,  0.1192,  0.1024, -0.0330, -0.0641, -0.0269,\n",
      "          0.2003,  0.0928,  0.0284, -0.1100, -0.1588,  0.2030, -0.0438,  0.0484],\n",
      "        [ 0.0156, -0.0329,  0.1410,  0.0768, -0.1651, -0.0024,  0.1840,  0.1567,\n",
      "          0.0768,  0.1071,  0.1930,  0.1541, -0.1584,  0.0420,  0.1902, -0.0860,\n",
      "         -0.0718, -0.1903,  0.0810,  0.1797,  0.0189,  0.1516, -0.1392,  0.1323],\n",
      "        [-0.0956,  0.1448, -0.1899, -0.0160,  0.1354, -0.1198,  0.0547,  0.0458,\n",
      "          0.0274, -0.1299, -0.0280,  0.0789,  0.0788,  0.0696,  0.1026, -0.1169,\n",
      "          0.1211,  0.1118, -0.0845, -0.1123,  0.0537,  0.0610,  0.1875, -0.0093],\n",
      "        [ 0.0154, -0.1179,  0.0539, -0.0364,  0.1657, -0.1409, -0.0838, -0.0549,\n",
      "          0.1860, -0.1796,  0.0398, -0.0533, -0.1632,  0.1855, -0.1778,  0.1713,\n",
      "         -0.0990, -0.1327, -0.2024,  0.0968, -0.1075, -0.1652, -0.1325,  0.1371],\n",
      "        [-0.1060,  0.0217,  0.1489,  0.0153, -0.1536, -0.0285,  0.1475, -0.1660,\n",
      "         -0.1355,  0.1814, -0.0857, -0.1614, -0.1835, -0.0382,  0.1830,  0.0897,\n",
      "          0.0783, -0.1647, -0.1202,  0.0064, -0.1861,  0.1577, -0.1692,  0.1975],\n",
      "        [-0.0898, -0.0177, -0.0933,  0.1375,  0.1922,  0.1257, -0.0352, -0.1675,\n",
      "          0.1365,  0.2022,  0.0834,  0.0270, -0.0147,  0.0819,  0.0048, -0.1507,\n",
      "         -0.2004,  0.0654, -0.1137,  0.0828,  0.1738, -0.0710, -0.0666,  0.0708],\n",
      "        [ 0.1088, -0.1074,  0.1119,  0.0935, -0.0694,  0.0280, -0.0331, -0.1843,\n",
      "          0.0082, -0.0344, -0.1744,  0.1501,  0.1899, -0.2034,  0.1975,  0.1403,\n",
      "         -0.1554, -0.1637, -0.1192,  0.0375, -0.1664, -0.1758, -0.0609, -0.0059],\n",
      "        [ 0.0839,  0.0987,  0.0618, -0.0415, -0.1937,  0.0657, -0.1050, -0.1996,\n",
      "          0.1689, -0.0107,  0.1697,  0.1602, -0.1857, -0.1852, -0.0560, -0.0669,\n",
      "         -0.0764, -0.1648,  0.0574,  0.0108, -0.0931, -0.0294, -0.1360, -0.1582],\n",
      "        [ 0.0368,  0.0100, -0.0427, -0.0107, -0.0514,  0.0389,  0.0646, -0.0800,\n",
      "          0.1887, -0.1029, -0.0201,  0.1271,  0.0033, -0.1129, -0.0188,  0.0669,\n",
      "         -0.0972,  0.0690,  0.0240, -0.2038,  0.1022, -0.1260,  0.0825, -0.0591],\n",
      "        [-0.1736, -0.1266,  0.1176, -0.0698,  0.0563,  0.1693, -0.0732,  0.0060,\n",
      "         -0.0468,  0.1703, -0.1760, -0.1748,  0.1067,  0.1243, -0.1193,  0.0738,\n",
      "         -0.1039,  0.0220,  0.2027,  0.1599, -0.1894, -0.0101, -0.1468, -0.1819],\n",
      "        [ 0.0647, -0.1927, -0.0745, -0.0071, -0.0713, -0.1205,  0.1201, -0.1556,\n",
      "         -0.0623, -0.0088,  0.0834, -0.1436, -0.1723, -0.2019,  0.1432,  0.0724,\n",
      "         -0.0134, -0.0079, -0.1416,  0.0519,  0.1644, -0.1103, -0.1108,  0.1311],\n",
      "        [-0.0408,  0.1744, -0.0342,  0.0533, -0.1835, -0.1342,  0.0496,  0.1632,\n",
      "          0.1246,  0.1898, -0.0621, -0.0766,  0.0406,  0.0260,  0.0520, -0.0367,\n",
      "          0.0973,  0.0300, -0.0200, -0.0986, -0.1071, -0.0388, -0.1344, -0.0942],\n",
      "        [ 0.1757, -0.1332,  0.1948, -0.1290, -0.1688,  0.1638,  0.0283, -0.1481,\n",
      "         -0.1781,  0.1948,  0.0918,  0.1605,  0.0867,  0.1103, -0.0188,  0.0873,\n",
      "          0.0643,  0.0900, -0.1866, -0.1066,  0.0786, -0.0898, -0.1137, -0.1516],\n",
      "        [ 0.0977, -0.1402, -0.0399,  0.0990,  0.1911,  0.1249,  0.0689,  0.1362,\n",
      "         -0.0939, -0.0527,  0.1248,  0.0130,  0.0423,  0.1259, -0.1560,  0.0545,\n",
      "         -0.1397, -0.0539, -0.0329,  0.1212, -0.1970,  0.1117, -0.0516,  0.0402],\n",
      "        [ 0.0838,  0.0473, -0.1502,  0.1754,  0.1385,  0.0121,  0.0018,  0.1856,\n",
      "         -0.0292,  0.1575, -0.1505, -0.0992,  0.1507,  0.1333, -0.0387,  0.1934,\n",
      "          0.1284,  0.0036,  0.1803, -0.0350,  0.1484, -0.0422,  0.1693,  0.0899],\n",
      "        [ 0.1082, -0.0508,  0.0724,  0.0642,  0.0903,  0.1874, -0.0889,  0.0948,\n",
      "         -0.1079, -0.0283,  0.1442,  0.1519, -0.0040, -0.1958,  0.1616,  0.0160,\n",
      "         -0.0459, -0.1501,  0.1611, -0.1243,  0.0984,  0.1036,  0.0069, -0.1849],\n",
      "        [-0.1848,  0.0003, -0.1864, -0.0361, -0.0415,  0.2002, -0.1333, -0.1423,\n",
      "         -0.1195,  0.1650,  0.0247, -0.1253, -0.1954, -0.0173, -0.0247, -0.1445,\n",
      "          0.1025,  0.0467,  0.0568,  0.0432,  0.0098,  0.1072, -0.1224,  0.0359],\n",
      "        [-0.0522, -0.1054,  0.2018, -0.0715, -0.1550, -0.0408, -0.1312,  0.1968,\n",
      "          0.1141, -0.1424, -0.1754, -0.0103, -0.0405,  0.1538,  0.1446,  0.1744,\n",
      "         -0.0048, -0.0331, -0.0996,  0.1907, -0.1103,  0.1011,  0.1750, -0.0258],\n",
      "        [-0.1786,  0.1403, -0.1606, -0.0782, -0.0243, -0.1939,  0.1079, -0.0144,\n",
      "          0.0155, -0.0542, -0.0743,  0.1513,  0.1357,  0.1878,  0.0761, -0.0152,\n",
      "         -0.1263,  0.0966, -0.1365,  0.1843, -0.0021,  0.0983,  0.1361,  0.1917],\n",
      "        [ 0.0756, -0.1257,  0.1950,  0.0210,  0.1264,  0.1759,  0.1294,  0.0983,\n",
      "         -0.0879,  0.0596, -0.0391, -0.1264,  0.1016,  0.0642, -0.1784, -0.1039,\n",
      "         -0.0160,  0.0840, -0.0988,  0.0021, -0.0205, -0.0051, -0.0517, -0.1436],\n",
      "        [ 0.0147, -0.1891, -0.1272, -0.1667, -0.1269, -0.0790, -0.0006, -0.1661,\n",
      "         -0.0851,  0.1772, -0.1738, -0.0295, -0.1990,  0.0516, -0.0576, -0.1618,\n",
      "         -0.0447, -0.0850,  0.1942, -0.1381, -0.1184,  0.1385, -0.0222, -0.0138],\n",
      "        [-0.1297,  0.0077, -0.0190,  0.1437, -0.0801,  0.1148,  0.1268, -0.1392,\n",
      "          0.0581,  0.0831,  0.0426,  0.0291, -0.1302,  0.0350, -0.0126, -0.1758,\n",
      "          0.1347, -0.1060,  0.0956,  0.0633, -0.0026, -0.1811, -0.0721, -0.0928],\n",
      "        [-0.1561, -0.0260, -0.0483,  0.2016, -0.1497,  0.1913, -0.1731, -0.0626,\n",
      "          0.1501,  0.1762, -0.1413, -0.1666,  0.0713,  0.0171,  0.1422,  0.1821,\n",
      "         -0.0010, -0.1906, -0.1761, -0.0580,  0.1674,  0.0413,  0.0049,  0.1825],\n",
      "        [ 0.0195, -0.1921,  0.1345,  0.0872,  0.0687, -0.0689,  0.0701, -0.2023,\n",
      "          0.1141, -0.1780, -0.0455, -0.1683,  0.0368,  0.0725,  0.0963,  0.1381,\n",
      "         -0.0772, -0.1378,  0.0811, -0.0909, -0.0370, -0.0233,  0.1483,  0.0542],\n",
      "        [ 0.1645, -0.0358,  0.1284, -0.0745, -0.0532,  0.0265,  0.0407, -0.0838,\n",
      "         -0.0169, -0.0823,  0.1198, -0.1677, -0.0988,  0.1412,  0.1837, -0.1271,\n",
      "          0.1491, -0.0490,  0.1010,  0.0103,  0.1692, -0.0200, -0.0614,  0.1616],\n",
      "        [ 0.1216, -0.0729, -0.0205,  0.1039,  0.0403,  0.0462,  0.0557,  0.1259,\n",
      "          0.0391, -0.1374, -0.1536, -0.0866,  0.0222, -0.1523,  0.1255,  0.0854,\n",
      "          0.1019,  0.0615, -0.0771, -0.0457,  0.1343, -0.1370,  0.0515,  0.1685],\n",
      "        [-0.0724,  0.1744, -0.1452,  0.1568, -0.1725,  0.0233, -0.1449,  0.1548,\n",
      "          0.0608, -0.0952,  0.0930,  0.1162, -0.1567,  0.1409, -0.0959, -0.0907,\n",
      "          0.1024, -0.0371, -0.0785, -0.0274,  0.1891, -0.0925, -0.1411,  0.1440],\n",
      "        [ 0.1716,  0.1321,  0.1164, -0.0018, -0.1445, -0.1306, -0.1747,  0.1494,\n",
      "         -0.2039, -0.0450,  0.0144, -0.1877,  0.1941,  0.0363,  0.0875,  0.0472,\n",
      "          0.0912, -0.1319, -0.0724,  0.1693, -0.0367,  0.1395, -0.0908, -0.1131]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1713, -0.1501,  0.1564,  0.1686, -0.0759, -0.0909,  0.0125, -0.0239,\n",
      "         0.0576,  0.0097,  0.1350,  0.1797,  0.1855, -0.0768, -0.0832,  0.0593,\n",
      "        -0.1533, -0.1441,  0.1639,  0.0548,  0.1815,  0.1158,  0.0473,  0.1781,\n",
      "         0.1367,  0.1127, -0.1448,  0.1180, -0.1746, -0.0177],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1339,  0.0545, -0.0432,  0.1703, -0.1539, -0.0039,  0.1232, -0.1076,\n",
      "         -0.0326, -0.1262,  0.0277, -0.1517,  0.1160,  0.1795,  0.1299, -0.0822,\n",
      "         -0.0402, -0.1195,  0.0772, -0.1078,  0.0258,  0.0096, -0.1006, -0.1432,\n",
      "         -0.1523, -0.0417, -0.1337, -0.1597,  0.1566, -0.0026],\n",
      "        [-0.0241,  0.1082,  0.1136,  0.0639, -0.0701,  0.1443,  0.0867, -0.1754,\n",
      "         -0.1055, -0.0220, -0.1563,  0.0409, -0.1204,  0.0620,  0.0949, -0.0119,\n",
      "          0.1510, -0.0463, -0.0980, -0.1206,  0.1586,  0.0733,  0.0446, -0.1147,\n",
      "         -0.0372,  0.0754, -0.0166, -0.1513,  0.1307,  0.1112],\n",
      "        [-0.0444, -0.1214,  0.0246, -0.0661,  0.1443,  0.1462, -0.1055, -0.0608,\n",
      "         -0.0621,  0.1053, -0.1326,  0.1532,  0.1717, -0.1632, -0.1482,  0.0561,\n",
      "         -0.1093,  0.0007,  0.1529, -0.0224,  0.1132, -0.1041, -0.1308, -0.1756,\n",
      "         -0.1493,  0.0769,  0.0577, -0.1391, -0.0034,  0.1547],\n",
      "        [-0.0921, -0.0411,  0.0319,  0.0092, -0.1756, -0.1662,  0.0232, -0.1091,\n",
      "         -0.1803, -0.1538, -0.1026, -0.1313, -0.0076,  0.1039,  0.0854, -0.1050,\n",
      "          0.1081, -0.0184, -0.1778,  0.0052, -0.1567, -0.0501,  0.0394,  0.0479,\n",
      "         -0.1705,  0.1342,  0.1518, -0.0756,  0.1400,  0.1205],\n",
      "        [-0.0386, -0.0261,  0.0072, -0.0121,  0.0816, -0.0544, -0.0148, -0.1465,\n",
      "          0.0947,  0.1093, -0.0981,  0.0967, -0.0105, -0.0370, -0.0494, -0.0885,\n",
      "          0.1639,  0.0629, -0.0001,  0.0732, -0.1413,  0.1555, -0.1768, -0.0213,\n",
      "          0.0290,  0.0281, -0.0854,  0.0653,  0.1627,  0.0454],\n",
      "        [-0.0654, -0.0346, -0.0279,  0.0388, -0.0174,  0.0375,  0.1462,  0.0289,\n",
      "         -0.0849,  0.1253,  0.0240, -0.1420,  0.1652, -0.0825,  0.0887, -0.0684,\n",
      "         -0.0440, -0.1680,  0.0507, -0.0826,  0.0745,  0.0852,  0.0308, -0.0380,\n",
      "          0.1055,  0.0412,  0.0216,  0.0773,  0.0407,  0.1060],\n",
      "        [-0.0770, -0.0102,  0.0190, -0.0568,  0.1533, -0.0948,  0.0074,  0.0603,\n",
      "         -0.0565,  0.0595, -0.1244, -0.0868,  0.1114, -0.1267,  0.1744, -0.1796,\n",
      "          0.1270, -0.0795,  0.1047,  0.1688,  0.1304,  0.1263, -0.0026, -0.0491,\n",
      "         -0.0828, -0.1244, -0.1267,  0.1229,  0.0857,  0.1628],\n",
      "        [ 0.0506,  0.1185,  0.0598, -0.0524, -0.0465,  0.1275,  0.1248,  0.0701,\n",
      "          0.0185,  0.0829,  0.0739, -0.1687,  0.1573,  0.0540, -0.1304, -0.1463,\n",
      "         -0.0196, -0.1728, -0.1157,  0.0863,  0.1059,  0.1719,  0.1061,  0.1211,\n",
      "         -0.1676,  0.0554, -0.0232, -0.0352,  0.0082, -0.0190],\n",
      "        [-0.1352,  0.1809, -0.1651, -0.1759,  0.1121,  0.0769,  0.0716, -0.1796,\n",
      "         -0.0390,  0.0655,  0.0672,  0.1614, -0.1537,  0.1148, -0.0187,  0.1099,\n",
      "         -0.0334,  0.1715, -0.0414, -0.1592, -0.1456,  0.1459, -0.1514,  0.0046,\n",
      "         -0.1574, -0.1328, -0.1205, -0.0545, -0.1704,  0.0514],\n",
      "        [-0.0242, -0.0164,  0.0237, -0.0978, -0.1802, -0.1140, -0.1164,  0.1535,\n",
      "         -0.1452, -0.0137,  0.0120,  0.0765,  0.1242,  0.0217, -0.1567, -0.0722,\n",
      "          0.1401,  0.1822, -0.0866, -0.0454, -0.0181, -0.0402,  0.1156, -0.0842,\n",
      "          0.1548,  0.1422,  0.1071,  0.0057, -0.0025, -0.1664],\n",
      "        [-0.1097,  0.0788, -0.1611, -0.0544, -0.0821,  0.0888, -0.0769, -0.0305,\n",
      "         -0.1588,  0.1516,  0.0704,  0.0581, -0.0964, -0.0334, -0.1641, -0.1595,\n",
      "          0.1003,  0.0267,  0.1379, -0.1256,  0.0875, -0.1300,  0.1681, -0.0863,\n",
      "         -0.1448,  0.0411,  0.0854, -0.1041, -0.1446,  0.0729],\n",
      "        [-0.0731,  0.1559,  0.0589, -0.1147, -0.0029,  0.1733, -0.1276,  0.0759,\n",
      "          0.0494, -0.1421,  0.0252,  0.0216, -0.1678,  0.0514, -0.0817,  0.1823,\n",
      "          0.0874,  0.0157,  0.0594, -0.0292, -0.1336, -0.1814, -0.0006,  0.1193,\n",
      "          0.0340, -0.1223, -0.0801,  0.0848,  0.1591, -0.1469],\n",
      "        [ 0.1550,  0.1421, -0.1796, -0.1716,  0.0456,  0.0845, -0.0293, -0.0515,\n",
      "         -0.0961,  0.1484,  0.0942, -0.0968, -0.1551,  0.0417,  0.0664, -0.0102,\n",
      "          0.1569,  0.0211,  0.0130, -0.1225, -0.0362,  0.0263, -0.0800, -0.1339,\n",
      "          0.1031,  0.1599,  0.0417, -0.0264,  0.0004,  0.0334],\n",
      "        [ 0.1505, -0.0908, -0.1572,  0.1019,  0.1177, -0.0930, -0.0430, -0.1156,\n",
      "         -0.1668,  0.0305,  0.1305, -0.1134,  0.1356,  0.1066,  0.1202, -0.1769,\n",
      "         -0.1718,  0.1586, -0.0275,  0.0008, -0.0565,  0.1327, -0.1561, -0.1289,\n",
      "          0.0779, -0.0453, -0.0157, -0.1726, -0.1198,  0.0282],\n",
      "        [-0.1013,  0.1151,  0.1026,  0.1333,  0.1727, -0.0337,  0.1288, -0.0712,\n",
      "         -0.1802,  0.0575,  0.0075, -0.0641,  0.0900,  0.1607,  0.1247, -0.0714,\n",
      "         -0.1473,  0.1719, -0.0984, -0.1265, -0.0734,  0.0957,  0.0064,  0.0331,\n",
      "         -0.0357, -0.0210,  0.1501, -0.0443,  0.1393, -0.0082]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0495,  0.1267,  0.0306,  0.0248, -0.0056, -0.1439, -0.1433, -0.1745,\n",
      "         0.0414, -0.1676, -0.1505,  0.0614,  0.0412, -0.0806, -0.0374],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.2039, -0.0508, -0.0321, -0.0579, -0.0108,  0.0121,  0.1021,  0.2168,\n",
      "          0.2016, -0.0811,  0.2305,  0.0677, -0.0217, -0.1202, -0.1337]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1241], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/syahrulhamdani/anaconda3/envs/100DaysOfMLCode/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50.. Training loss: 19789.023\n",
      "Epoch: 1/50.. Training loss: 14793.422\n",
      "Epoch: 1/50.. Training loss: 12829.431\n",
      "Epoch: 1/50.. Training loss: 16141.982\n",
      "Epoch: 1/50.. Training loss: 13074.153\n",
      "Epoch: 1/50.. Training loss: 10904.154\n",
      "Epoch: 1/50.. Training loss: 10493.633\n",
      "Epoch: 1/50.. Training loss: 11931.201\n",
      "Epoch: 1/50.. Training loss: 14308.731\n",
      "Epoch: 1/50.. Training loss: 12547.993\n",
      "Epoch: 1/50.. Training loss: 14376.928\n",
      "Epoch: 1/50.. Training loss: 9279.416\n",
      "Epoch: 1/50.. Training loss: 19270.188\n",
      "Epoch: 1/50.. Training loss: 18794.553\n",
      "Epoch: 1/50.. Training loss: 11073.011\n",
      "Epoch: 1/50.. Training loss: 12195.018\n",
      "Epoch: 1/50.. Training loss: 15375.745\n",
      "Epoch: 1/50.. Training loss: 14816.861\n",
      "Epoch: 1/50.. Training loss: 18590.410\n",
      "Epoch: 2/50.. Training loss: 6220.941\n",
      "Epoch: 2/50.. Training loss: 12141.911\n",
      "Epoch: 2/50.. Training loss: 9545.015\n",
      "Epoch: 2/50.. Training loss: 5415.592\n",
      "Epoch: 2/50.. Training loss: 8955.735\n",
      "Epoch: 2/50.. Training loss: 5706.934\n",
      "Epoch: 2/50.. Training loss: 5157.633\n",
      "Epoch: 2/50.. Training loss: 4812.213\n",
      "Epoch: 2/50.. Training loss: 5145.343\n",
      "Epoch: 2/50.. Training loss: 7021.108\n",
      "Epoch: 2/50.. Training loss: 6085.769\n",
      "Epoch: 2/50.. Training loss: 7314.358\n",
      "Epoch: 2/50.. Training loss: 6087.857\n",
      "Epoch: 2/50.. Training loss: 10402.073\n",
      "Epoch: 2/50.. Training loss: 8231.502\n",
      "Epoch: 2/50.. Training loss: 4589.445\n",
      "Epoch: 2/50.. Training loss: 7310.897\n",
      "Epoch: 2/50.. Training loss: 6895.561\n",
      "Epoch: 2/50.. Training loss: 7979.299\n",
      "Epoch: 2/50.. Training loss: 10748.073\n",
      "Epoch: 3/50.. Training loss: 5103.540\n",
      "Epoch: 3/50.. Training loss: 5153.776\n",
      "Epoch: 3/50.. Training loss: 4245.202\n",
      "Epoch: 3/50.. Training loss: 4723.060\n",
      "Epoch: 3/50.. Training loss: 3497.981\n",
      "Epoch: 3/50.. Training loss: 3129.884\n",
      "Epoch: 3/50.. Training loss: 3379.561\n",
      "Epoch: 3/50.. Training loss: 2682.172\n",
      "Epoch: 3/50.. Training loss: 3509.100\n",
      "Epoch: 3/50.. Training loss: 4656.027\n",
      "Epoch: 3/50.. Training loss: 4481.324\n",
      "Epoch: 3/50.. Training loss: 4588.375\n",
      "Epoch: 3/50.. Training loss: 4435.823\n",
      "Epoch: 3/50.. Training loss: 11104.918\n",
      "Epoch: 3/50.. Training loss: 3130.742\n",
      "Epoch: 3/50.. Training loss: 3855.364\n",
      "Epoch: 3/50.. Training loss: 6601.364\n",
      "Epoch: 3/50.. Training loss: 4421.845\n",
      "Epoch: 3/50.. Training loss: 7269.109\n",
      "Epoch: 3/50.. Training loss: 8261.581\n",
      "Epoch: 4/50.. Training loss: 4663.918\n",
      "Epoch: 4/50.. Training loss: 4716.966\n",
      "Epoch: 4/50.. Training loss: 3905.153\n",
      "Epoch: 4/50.. Training loss: 4406.252\n",
      "Epoch: 4/50.. Training loss: 3384.319\n",
      "Epoch: 4/50.. Training loss: 3307.377\n",
      "Epoch: 4/50.. Training loss: 3482.242\n",
      "Epoch: 4/50.. Training loss: 2866.200\n",
      "Epoch: 4/50.. Training loss: 3381.079\n",
      "Epoch: 4/50.. Training loss: 4473.339\n",
      "Epoch: 4/50.. Training loss: 4256.442\n",
      "Epoch: 4/50.. Training loss: 4314.238\n",
      "Epoch: 4/50.. Training loss: 4561.348\n",
      "Epoch: 4/50.. Training loss: 10205.786\n",
      "Epoch: 4/50.. Training loss: 3756.808\n",
      "Epoch: 4/50.. Training loss: 3365.263\n",
      "Epoch: 4/50.. Training loss: 6205.624\n",
      "Epoch: 4/50.. Training loss: 4590.431\n",
      "Epoch: 4/50.. Training loss: 7251.472\n",
      "Epoch: 4/50.. Training loss: 7665.233\n",
      "Epoch: 5/50.. Training loss: 4592.412\n",
      "Epoch: 5/50.. Training loss: 4751.125\n",
      "Epoch: 5/50.. Training loss: 3935.347\n",
      "Epoch: 5/50.. Training loss: 4299.964\n",
      "Epoch: 5/50.. Training loss: 3548.952\n",
      "Epoch: 5/50.. Training loss: 3539.140\n",
      "Epoch: 5/50.. Training loss: 3470.303\n",
      "Epoch: 5/50.. Training loss: 3245.889\n",
      "Epoch: 5/50.. Training loss: 3816.245\n",
      "Epoch: 5/50.. Training loss: 3988.711\n",
      "Epoch: 5/50.. Training loss: 4780.891\n",
      "Epoch: 5/50.. Training loss: 3468.840\n",
      "Epoch: 5/50.. Training loss: 4449.812\n",
      "Epoch: 5/50.. Training loss: 10316.789\n",
      "Epoch: 5/50.. Training loss: 3788.115\n",
      "Epoch: 5/50.. Training loss: 3886.577\n",
      "Epoch: 5/50.. Training loss: 5544.629\n",
      "Epoch: 5/50.. Training loss: 4527.270\n",
      "Epoch: 5/50.. Training loss: 8565.896\n",
      "Epoch: 5/50.. Training loss: 6234.985\n",
      "Epoch: 6/50.. Training loss: 4673.129\n",
      "Epoch: 6/50.. Training loss: 4875.209\n",
      "Epoch: 6/50.. Training loss: 3693.019\n",
      "Epoch: 6/50.. Training loss: 4409.947\n",
      "Epoch: 6/50.. Training loss: 4030.239\n",
      "Epoch: 6/50.. Training loss: 3034.052\n",
      "Epoch: 6/50.. Training loss: 3531.636\n",
      "Epoch: 6/50.. Training loss: 3336.595\n",
      "Epoch: 6/50.. Training loss: 4030.787\n",
      "Epoch: 6/50.. Training loss: 3826.419\n",
      "Epoch: 6/50.. Training loss: 4854.514\n",
      "Epoch: 6/50.. Training loss: 3327.034\n",
      "Epoch: 6/50.. Training loss: 6869.759\n",
      "Epoch: 6/50.. Training loss: 7697.887\n",
      "Epoch: 6/50.. Training loss: 3780.088\n",
      "Epoch: 6/50.. Training loss: 4034.412\n",
      "Epoch: 6/50.. Training loss: 6813.955\n",
      "Epoch: 6/50.. Training loss: 3252.927\n",
      "Epoch: 6/50.. Training loss: 8467.600\n",
      "Epoch: 6/50.. Training loss: 6444.938\n",
      "Epoch: 7/50.. Training loss: 5664.598\n",
      "Epoch: 7/50.. Training loss: 4138.284\n",
      "Epoch: 7/50.. Training loss: 3417.805\n",
      "Epoch: 7/50.. Training loss: 4751.390\n",
      "Epoch: 7/50.. Training loss: 3482.279\n",
      "Epoch: 7/50.. Training loss: 3412.875\n",
      "Epoch: 7/50.. Training loss: 3278.504\n",
      "Epoch: 7/50.. Training loss: 3185.584\n",
      "Epoch: 7/50.. Training loss: 4078.517\n",
      "Epoch: 7/50.. Training loss: 3792.100\n",
      "Epoch: 7/50.. Training loss: 4954.909\n",
      "Epoch: 7/50.. Training loss: 3383.031\n",
      "Epoch: 7/50.. Training loss: 6715.472\n",
      "Epoch: 7/50.. Training loss: 8032.006\n",
      "Epoch: 7/50.. Training loss: 3560.186\n",
      "Epoch: 7/50.. Training loss: 4000.622\n",
      "Epoch: 7/50.. Training loss: 6724.701\n",
      "Epoch: 7/50.. Training loss: 6591.734\n",
      "Epoch: 7/50.. Training loss: 8282.845\n",
      "Epoch: 8/50.. Training loss: 3698.156\n",
      "Epoch: 8/50.. Training loss: 5246.417\n",
      "Epoch: 8/50.. Training loss: 4249.758\n",
      "Epoch: 8/50.. Training loss: 3716.501\n",
      "Epoch: 8/50.. Training loss: 4626.379\n",
      "Epoch: 8/50.. Training loss: 3301.499\n",
      "Epoch: 8/50.. Training loss: 3577.161\n",
      "Epoch: 8/50.. Training loss: 3197.805\n",
      "Epoch: 8/50.. Training loss: 3219.856\n",
      "Epoch: 8/50.. Training loss: 4265.976\n",
      "Epoch: 8/50.. Training loss: 3589.186\n",
      "Epoch: 8/50.. Training loss: 4869.808\n",
      "Epoch: 8/50.. Training loss: 4470.843\n",
      "Epoch: 8/50.. Training loss: 5631.786\n",
      "Epoch: 8/50.. Training loss: 8339.999\n",
      "Epoch: 8/50.. Training loss: 3647.219\n",
      "Epoch: 8/50.. Training loss: 5366.826\n",
      "Epoch: 8/50.. Training loss: 4976.664\n",
      "Epoch: 8/50.. Training loss: 6870.052\n",
      "Epoch: 8/50.. Training loss: 8141.036\n",
      "Epoch: 9/50.. Training loss: 3987.324\n",
      "Epoch: 9/50.. Training loss: 4794.181\n",
      "Epoch: 9/50.. Training loss: 4565.678\n",
      "Epoch: 9/50.. Training loss: 3932.979\n",
      "Epoch: 9/50.. Training loss: 4152.750\n",
      "Epoch: 9/50.. Training loss: 3264.129\n",
      "Epoch: 9/50.. Training loss: 3726.394\n",
      "Epoch: 9/50.. Training loss: 2911.959\n",
      "Epoch: 9/50.. Training loss: 3483.569\n",
      "Epoch: 9/50.. Training loss: 4468.843\n",
      "Epoch: 9/50.. Training loss: 4166.755\n",
      "Epoch: 9/50.. Training loss: 4454.672\n",
      "Epoch: 9/50.. Training loss: 4187.840\n",
      "Epoch: 9/50.. Training loss: 9394.312\n",
      "Epoch: 9/50.. Training loss: 4364.165\n",
      "Epoch: 9/50.. Training loss: 3831.152\n",
      "Epoch: 9/50.. Training loss: 5052.608\n",
      "Epoch: 9/50.. Training loss: 5409.328\n",
      "Epoch: 9/50.. Training loss: 6571.986\n",
      "Epoch: 9/50.. Training loss: 8357.692\n",
      "Epoch: 10/50.. Training loss: 4644.920\n",
      "Epoch: 10/50.. Training loss: 4862.101\n",
      "Epoch: 10/50.. Training loss: 3673.711\n",
      "Epoch: 10/50.. Training loss: 4376.063\n",
      "Epoch: 10/50.. Training loss: 3528.121\n",
      "Epoch: 10/50.. Training loss: 3397.358\n",
      "Epoch: 10/50.. Training loss: 3651.369\n",
      "Epoch: 10/50.. Training loss: 3158.446\n",
      "Epoch: 10/50.. Training loss: 3287.999\n",
      "Epoch: 10/50.. Training loss: 4532.598\n",
      "Epoch: 10/50.. Training loss: 3985.885\n",
      "Epoch: 10/50.. Training loss: 4612.268\n",
      "Epoch: 10/50.. Training loss: 4137.555\n",
      "Epoch: 10/50.. Training loss: 10242.241\n",
      "Epoch: 10/50.. Training loss: 3490.160\n",
      "Epoch: 10/50.. Training loss: 3911.721\n",
      "Epoch: 10/50.. Training loss: 6137.661\n",
      "Epoch: 10/50.. Training loss: 4548.087\n",
      "Epoch: 10/50.. Training loss: 7205.464\n",
      "Epoch: 10/50.. Training loss: 7500.027\n",
      "Epoch: 11/50.. Training loss: 4706.184\n",
      "Epoch: 11/50.. Training loss: 4770.052\n",
      "Epoch: 11/50.. Training loss: 3714.352\n",
      "Epoch: 11/50.. Training loss: 4513.106\n",
      "Epoch: 11/50.. Training loss: 3351.622\n",
      "Epoch: 11/50.. Training loss: 3407.944\n",
      "Epoch: 11/50.. Training loss: 3567.659\n",
      "Epoch: 11/50.. Training loss: 3095.211\n",
      "Epoch: 11/50.. Training loss: 4077.041\n",
      "Epoch: 11/50.. Training loss: 3743.660\n",
      "Epoch: 11/50.. Training loss: 4122.127\n",
      "Epoch: 11/50.. Training loss: 4649.930\n",
      "Epoch: 11/50.. Training loss: 4312.462\n",
      "Epoch: 11/50.. Training loss: 10000.018\n",
      "Epoch: 11/50.. Training loss: 3719.840\n",
      "Epoch: 11/50.. Training loss: 3594.697\n",
      "Epoch: 11/50.. Training loss: 6286.583\n",
      "Epoch: 11/50.. Training loss: 4383.385\n",
      "Epoch: 11/50.. Training loss: 7203.362\n",
      "Epoch: 11/50.. Training loss: 7744.485\n",
      "Epoch: 12/50.. Training loss: 4682.303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/50.. Training loss: 4757.614\n",
      "Epoch: 12/50.. Training loss: 3736.756\n",
      "Epoch: 12/50.. Training loss: 4438.272\n",
      "Epoch: 12/50.. Training loss: 3359.463\n",
      "Epoch: 12/50.. Training loss: 3533.952\n",
      "Epoch: 12/50.. Training loss: 3441.559\n",
      "Epoch: 12/50.. Training loss: 3271.716\n",
      "Epoch: 12/50.. Training loss: 3847.918\n",
      "Epoch: 12/50.. Training loss: 3957.938\n",
      "Epoch: 12/50.. Training loss: 5262.359\n",
      "Epoch: 12/50.. Training loss: 3225.813\n",
      "Epoch: 12/50.. Training loss: 5617.149\n",
      "Epoch: 12/50.. Training loss: 9057.895\n",
      "Epoch: 12/50.. Training loss: 3741.464\n",
      "Epoch: 12/50.. Training loss: 3778.801\n",
      "Epoch: 12/50.. Training loss: 5682.094\n",
      "Epoch: 12/50.. Training loss: 4528.731\n",
      "Epoch: 12/50.. Training loss: 8641.961\n",
      "Epoch: 12/50.. Training loss: 6345.141\n",
      "Epoch: 13/50.. Training loss: 4538.139\n",
      "Epoch: 13/50.. Training loss: 4722.704\n",
      "Epoch: 13/50.. Training loss: 4015.152\n",
      "Epoch: 13/50.. Training loss: 4506.376\n",
      "Epoch: 13/50.. Training loss: 3590.794\n",
      "Epoch: 13/50.. Training loss: 3514.967\n",
      "Epoch: 13/50.. Training loss: 3003.092\n",
      "Epoch: 13/50.. Training loss: 3445.930\n",
      "Epoch: 13/50.. Training loss: 3980.477\n",
      "Epoch: 13/50.. Training loss: 3970.157\n",
      "Epoch: 13/50.. Training loss: 4816.110\n",
      "Epoch: 13/50.. Training loss: 3267.223\n",
      "Epoch: 13/50.. Training loss: 6845.021\n",
      "Epoch: 13/50.. Training loss: 8025.364\n",
      "Epoch: 13/50.. Training loss: 3629.899\n",
      "Epoch: 13/50.. Training loss: 4083.004\n",
      "Epoch: 13/50.. Training loss: 6701.633\n",
      "Epoch: 13/50.. Training loss: 3370.227\n",
      "Epoch: 13/50.. Training loss: 8499.380\n",
      "Epoch: 13/50.. Training loss: 6196.996\n",
      "Epoch: 14/50.. Training loss: 5652.405\n",
      "Epoch: 14/50.. Training loss: 4072.200\n",
      "Epoch: 14/50.. Training loss: 3548.362\n",
      "Epoch: 14/50.. Training loss: 4622.970\n",
      "Epoch: 14/50.. Training loss: 3772.618\n",
      "Epoch: 14/50.. Training loss: 3216.034\n",
      "Epoch: 14/50.. Training loss: 3162.762\n",
      "Epoch: 14/50.. Training loss: 3580.758\n",
      "Epoch: 14/50.. Training loss: 4211.072\n",
      "Epoch: 14/50.. Training loss: 3622.418\n",
      "Epoch: 14/50.. Training loss: 4887.369\n",
      "Epoch: 14/50.. Training loss: 3127.223\n",
      "Epoch: 14/50.. Training loss: 7013.128\n",
      "Epoch: 14/50.. Training loss: 7772.434\n",
      "Epoch: 14/50.. Training loss: 4001.648\n",
      "Epoch: 14/50.. Training loss: 3783.502\n",
      "Epoch: 14/50.. Training loss: 6769.819\n",
      "Epoch: 14/50.. Training loss: 6800.835\n",
      "Epoch: 14/50.. Training loss: 8184.479\n",
      "Epoch: 15/50.. Training loss: 3240.951\n",
      "Epoch: 15/50.. Training loss: 5570.365\n",
      "Epoch: 15/50.. Training loss: 4480.456\n",
      "Epoch: 15/50.. Training loss: 3161.795\n",
      "Epoch: 15/50.. Training loss: 4637.458\n",
      "Epoch: 15/50.. Training loss: 3594.227\n",
      "Epoch: 15/50.. Training loss: 3283.014\n",
      "Epoch: 15/50.. Training loss: 3290.399\n",
      "Epoch: 15/50.. Training loss: 3292.219\n",
      "Epoch: 15/50.. Training loss: 4206.888\n",
      "Epoch: 15/50.. Training loss: 3619.878\n",
      "Epoch: 15/50.. Training loss: 5102.910\n",
      "Epoch: 15/50.. Training loss: 4597.141\n",
      "Epoch: 15/50.. Training loss: 5396.945\n",
      "Epoch: 15/50.. Training loss: 8085.363\n",
      "Epoch: 15/50.. Training loss: 3681.231\n",
      "Epoch: 15/50.. Training loss: 5492.835\n",
      "Epoch: 15/50.. Training loss: 5052.124\n",
      "Epoch: 15/50.. Training loss: 6748.847\n",
      "Epoch: 15/50.. Training loss: 8196.252\n",
      "Epoch: 16/50.. Training loss: 4704.907\n",
      "Epoch: 16/50.. Training loss: 4217.781\n",
      "Epoch: 16/50.. Training loss: 4357.375\n",
      "Epoch: 16/50.. Training loss: 4501.113\n",
      "Epoch: 16/50.. Training loss: 3653.430\n",
      "Epoch: 16/50.. Training loss: 3379.168\n",
      "Epoch: 16/50.. Training loss: 3469.512\n",
      "Epoch: 16/50.. Training loss: 3053.247\n",
      "Epoch: 16/50.. Training loss: 3323.006\n",
      "Epoch: 16/50.. Training loss: 4448.698\n",
      "Epoch: 16/50.. Training loss: 4271.356\n",
      "Epoch: 16/50.. Training loss: 4349.162\n",
      "Epoch: 16/50.. Training loss: 4338.239\n",
      "Epoch: 16/50.. Training loss: 10111.594\n",
      "Epoch: 16/50.. Training loss: 3530.214\n",
      "Epoch: 16/50.. Training loss: 3798.536\n",
      "Epoch: 16/50.. Training loss: 5655.047\n",
      "Epoch: 16/50.. Training loss: 4810.036\n",
      "Epoch: 16/50.. Training loss: 6698.779\n",
      "Epoch: 16/50.. Training loss: 8232.823\n",
      "Epoch: 17/50.. Training loss: 4657.070\n",
      "Epoch: 17/50.. Training loss: 4862.703\n",
      "Epoch: 17/50.. Training loss: 3709.434\n",
      "Epoch: 17/50.. Training loss: 4392.053\n",
      "Epoch: 17/50.. Training loss: 3789.376\n",
      "Epoch: 17/50.. Training loss: 3294.972\n",
      "Epoch: 17/50.. Training loss: 3675.147\n",
      "Epoch: 17/50.. Training loss: 3022.470\n",
      "Epoch: 17/50.. Training loss: 3186.814\n",
      "Epoch: 17/50.. Training loss: 4651.562\n",
      "Epoch: 17/50.. Training loss: 4181.107\n",
      "Epoch: 17/50.. Training loss: 4278.196\n",
      "Epoch: 17/50.. Training loss: 4308.790\n",
      "Epoch: 17/50.. Training loss: 10099.627\n",
      "Epoch: 17/50.. Training loss: 3945.605\n",
      "Epoch: 17/50.. Training loss: 3507.804\n",
      "Epoch: 17/50.. Training loss: 6065.192\n",
      "Epoch: 17/50.. Training loss: 4649.596\n",
      "Epoch: 17/50.. Training loss: 7218.023\n",
      "Epoch: 17/50.. Training loss: 7388.169\n",
      "Epoch: 18/50.. Training loss: 4694.570\n",
      "Epoch: 18/50.. Training loss: 4812.631\n",
      "Epoch: 18/50.. Training loss: 3709.521\n",
      "Epoch: 18/50.. Training loss: 4475.279\n",
      "Epoch: 18/50.. Training loss: 3691.993\n",
      "Epoch: 18/50.. Training loss: 3255.423\n",
      "Epoch: 18/50.. Training loss: 3720.756\n",
      "Epoch: 18/50.. Training loss: 3197.470\n",
      "Epoch: 18/50.. Training loss: 3948.826\n",
      "Epoch: 18/50.. Training loss: 3855.581\n",
      "Epoch: 18/50.. Training loss: 3955.402\n",
      "Epoch: 18/50.. Training loss: 4450.901\n",
      "Epoch: 18/50.. Training loss: 4259.427\n",
      "Epoch: 18/50.. Training loss: 10088.549\n",
      "Epoch: 18/50.. Training loss: 3773.813\n",
      "Epoch: 18/50.. Training loss: 4040.896\n",
      "Epoch: 18/50.. Training loss: 5793.323\n",
      "Epoch: 18/50.. Training loss: 4527.073\n",
      "Epoch: 18/50.. Training loss: 7779.902\n",
      "Epoch: 18/50.. Training loss: 6930.228\n",
      "Epoch: 19/50.. Training loss: 4731.165\n",
      "Epoch: 19/50.. Training loss: 4885.986\n",
      "Epoch: 19/50.. Training loss: 3550.454\n",
      "Epoch: 19/50.. Training loss: 4437.929\n",
      "Epoch: 19/50.. Training loss: 3781.861\n",
      "Epoch: 19/50.. Training loss: 3137.967\n",
      "Epoch: 19/50.. Training loss: 3603.958\n",
      "Epoch: 19/50.. Training loss: 3189.262\n",
      "Epoch: 19/50.. Training loss: 4035.989\n",
      "Epoch: 19/50.. Training loss: 3769.080\n",
      "Epoch: 19/50.. Training loss: 5265.934\n",
      "Epoch: 19/50.. Training loss: 3330.800\n",
      "Epoch: 19/50.. Training loss: 6443.466\n",
      "Epoch: 19/50.. Training loss: 8020.445\n",
      "Epoch: 19/50.. Training loss: 3738.317\n",
      "Epoch: 19/50.. Training loss: 3828.263\n",
      "Epoch: 19/50.. Training loss: 6490.085\n",
      "Epoch: 19/50.. Training loss: 3882.926\n",
      "Epoch: 19/50.. Training loss: 8450.844\n",
      "Epoch: 19/50.. Training loss: 6492.797\n",
      "Epoch: 20/50.. Training loss: 5211.972\n",
      "Epoch: 20/50.. Training loss: 4309.428\n",
      "Epoch: 20/50.. Training loss: 3701.229\n",
      "Epoch: 20/50.. Training loss: 4712.487\n",
      "Epoch: 20/50.. Training loss: 3284.458\n",
      "Epoch: 20/50.. Training loss: 3652.091\n",
      "Epoch: 20/50.. Training loss: 3027.049\n",
      "Epoch: 20/50.. Training loss: 3272.050\n",
      "Epoch: 20/50.. Training loss: 3996.341\n",
      "Epoch: 20/50.. Training loss: 3958.553\n",
      "Epoch: 20/50.. Training loss: 5008.327\n",
      "Epoch: 20/50.. Training loss: 3382.005\n",
      "Epoch: 20/50.. Training loss: 6539.391\n",
      "Epoch: 20/50.. Training loss: 8228.911\n",
      "Epoch: 20/50.. Training loss: 3419.251\n",
      "Epoch: 20/50.. Training loss: 4117.854\n",
      "Epoch: 20/50.. Training loss: 6672.764\n",
      "Epoch: 20/50.. Training loss: 5434.435\n",
      "Epoch: 20/50.. Training loss: 8429.359\n",
      "Epoch: 20/50.. Training loss: 4639.394\n",
      "Epoch: 21/50.. Training loss: 5213.380\n",
      "Epoch: 21/50.. Training loss: 4109.516\n",
      "Epoch: 21/50.. Training loss: 3921.637\n",
      "Epoch: 21/50.. Training loss: 4496.455\n",
      "Epoch: 21/50.. Training loss: 3518.374\n",
      "Epoch: 21/50.. Training loss: 3538.595\n",
      "Epoch: 21/50.. Training loss: 2907.924\n",
      "Epoch: 21/50.. Training loss: 3580.656\n",
      "Epoch: 21/50.. Training loss: 4267.469\n",
      "Epoch: 21/50.. Training loss: 3589.931\n",
      "Epoch: 21/50.. Training loss: 4814.319\n",
      "Epoch: 21/50.. Training loss: 3383.689\n",
      "Epoch: 21/50.. Training loss: 6751.189\n",
      "Epoch: 21/50.. Training loss: 8118.001\n",
      "Epoch: 21/50.. Training loss: 3871.239\n",
      "Epoch: 21/50.. Training loss: 4766.177\n",
      "Epoch: 21/50.. Training loss: 5579.924\n",
      "Epoch: 21/50.. Training loss: 6887.823\n",
      "Epoch: 21/50.. Training loss: 8137.207\n",
      "Epoch: 22/50.. Training loss: 3265.074\n",
      "Epoch: 22/50.. Training loss: 5478.547\n",
      "Epoch: 22/50.. Training loss: 4606.856\n",
      "Epoch: 22/50.. Training loss: 3159.150\n",
      "Epoch: 22/50.. Training loss: 4554.792\n",
      "Epoch: 22/50.. Training loss: 3603.055\n",
      "Epoch: 22/50.. Training loss: 3423.211\n",
      "Epoch: 22/50.. Training loss: 3075.891\n",
      "Epoch: 22/50.. Training loss: 3614.182\n",
      "Epoch: 22/50.. Training loss: 4312.238\n",
      "Epoch: 22/50.. Training loss: 3874.516\n",
      "Epoch: 22/50.. Training loss: 4858.408\n",
      "Epoch: 22/50.. Training loss: 4274.499\n",
      "Epoch: 22/50.. Training loss: 7695.567\n",
      "Epoch: 22/50.. Training loss: 5727.649\n",
      "Epoch: 22/50.. Training loss: 3965.965\n",
      "Epoch: 22/50.. Training loss: 5152.697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22/50.. Training loss: 5437.632\n",
      "Epoch: 22/50.. Training loss: 6644.490\n",
      "Epoch: 22/50.. Training loss: 8289.767\n",
      "Epoch: 23/50.. Training loss: 4666.959\n",
      "Epoch: 23/50.. Training loss: 4633.116\n",
      "Epoch: 23/50.. Training loss: 3933.629\n",
      "Epoch: 23/50.. Training loss: 4382.436\n",
      "Epoch: 23/50.. Training loss: 3445.454\n",
      "Epoch: 23/50.. Training loss: 3474.760\n",
      "Epoch: 23/50.. Training loss: 3393.302\n",
      "Epoch: 23/50.. Training loss: 3089.553\n",
      "Epoch: 23/50.. Training loss: 3543.303\n",
      "Epoch: 23/50.. Training loss: 4252.481\n",
      "Epoch: 23/50.. Training loss: 4188.574\n",
      "Epoch: 23/50.. Training loss: 4541.446\n",
      "Epoch: 23/50.. Training loss: 4369.019\n",
      "Epoch: 23/50.. Training loss: 10206.293\n",
      "Epoch: 23/50.. Training loss: 3239.933\n",
      "Epoch: 23/50.. Training loss: 3852.158\n",
      "Epoch: 23/50.. Training loss: 6268.648\n",
      "Epoch: 23/50.. Training loss: 4331.421\n",
      "Epoch: 23/50.. Training loss: 7121.279\n",
      "Epoch: 23/50.. Training loss: 7819.306\n",
      "Epoch: 24/50.. Training loss: 4703.499\n",
      "Epoch: 24/50.. Training loss: 4634.897\n",
      "Epoch: 24/50.. Training loss: 4020.907\n",
      "Epoch: 24/50.. Training loss: 4408.030\n",
      "Epoch: 24/50.. Training loss: 3439.791\n",
      "Epoch: 24/50.. Training loss: 3450.278\n",
      "Epoch: 24/50.. Training loss: 3512.019\n",
      "Epoch: 24/50.. Training loss: 3007.707\n",
      "Epoch: 24/50.. Training loss: 3450.912\n",
      "Epoch: 24/50.. Training loss: 4387.254\n",
      "Epoch: 24/50.. Training loss: 4231.926\n",
      "Epoch: 24/50.. Training loss: 4333.720\n",
      "Epoch: 24/50.. Training loss: 4571.893\n",
      "Epoch: 24/50.. Training loss: 10025.143\n",
      "Epoch: 24/50.. Training loss: 3771.309\n",
      "Epoch: 24/50.. Training loss: 3393.688\n",
      "Epoch: 24/50.. Training loss: 6132.293\n",
      "Epoch: 24/50.. Training loss: 4577.102\n",
      "Epoch: 24/50.. Training loss: 7217.121\n",
      "Epoch: 24/50.. Training loss: 7614.036\n",
      "Epoch: 25/50.. Training loss: 4597.845\n",
      "Epoch: 25/50.. Training loss: 4747.915\n",
      "Epoch: 25/50.. Training loss: 3960.357\n",
      "Epoch: 25/50.. Training loss: 4295.445\n",
      "Epoch: 25/50.. Training loss: 3574.157\n",
      "Epoch: 25/50.. Training loss: 3569.625\n",
      "Epoch: 25/50.. Training loss: 3478.731\n",
      "Epoch: 25/50.. Training loss: 3274.543\n",
      "Epoch: 25/50.. Training loss: 3819.022\n",
      "Epoch: 25/50.. Training loss: 3986.709\n",
      "Epoch: 25/50.. Training loss: 4775.581\n",
      "Epoch: 25/50.. Training loss: 3476.677\n",
      "Epoch: 25/50.. Training loss: 4454.565\n",
      "Epoch: 25/50.. Training loss: 10278.169\n",
      "Epoch: 25/50.. Training loss: 3790.631\n",
      "Epoch: 25/50.. Training loss: 3888.485\n",
      "Epoch: 25/50.. Training loss: 5531.867\n",
      "Epoch: 25/50.. Training loss: 4527.264\n",
      "Epoch: 25/50.. Training loss: 8552.704\n",
      "Epoch: 25/50.. Training loss: 6227.741\n",
      "Epoch: 26/50.. Training loss: 4674.339\n",
      "Epoch: 26/50.. Training loss: 4874.355\n",
      "Epoch: 26/50.. Training loss: 3698.747\n",
      "Epoch: 26/50.. Training loss: 4410.439\n",
      "Epoch: 26/50.. Training loss: 4033.979\n",
      "Epoch: 26/50.. Training loss: 3042.195\n",
      "Epoch: 26/50.. Training loss: 3534.020\n",
      "Epoch: 26/50.. Training loss: 3343.181\n",
      "Epoch: 26/50.. Training loss: 4030.684\n",
      "Epoch: 26/50.. Training loss: 3826.590\n",
      "Epoch: 26/50.. Training loss: 4849.879\n",
      "Epoch: 26/50.. Training loss: 3331.881\n",
      "Epoch: 26/50.. Training loss: 6865.434\n",
      "Epoch: 26/50.. Training loss: 7693.000\n",
      "Epoch: 26/50.. Training loss: 3780.974\n",
      "Epoch: 26/50.. Training loss: 4035.695\n",
      "Epoch: 26/50.. Training loss: 6809.406\n",
      "Epoch: 26/50.. Training loss: 3254.616\n",
      "Epoch: 26/50.. Training loss: 8463.459\n",
      "Epoch: 26/50.. Training loss: 6443.827\n",
      "Epoch: 27/50.. Training loss: 5664.330\n",
      "Epoch: 27/50.. Training loss: 4138.617\n",
      "Epoch: 27/50.. Training loss: 3418.668\n",
      "Epoch: 27/50.. Training loss: 4751.232\n",
      "Epoch: 27/50.. Training loss: 3483.640\n",
      "Epoch: 27/50.. Training loss: 3413.838\n",
      "Epoch: 27/50.. Training loss: 3280.149\n",
      "Epoch: 27/50.. Training loss: 3186.980\n",
      "Epoch: 27/50.. Training loss: 4078.678\n",
      "Epoch: 27/50.. Training loss: 3791.700\n",
      "Epoch: 27/50.. Training loss: 4954.295\n",
      "Epoch: 27/50.. Training loss: 3384.091\n",
      "Epoch: 27/50.. Training loss: 6714.151\n",
      "Epoch: 27/50.. Training loss: 8030.494\n",
      "Epoch: 27/50.. Training loss: 3560.710\n",
      "Epoch: 27/50.. Training loss: 4000.504\n",
      "Epoch: 27/50.. Training loss: 6723.763\n",
      "Epoch: 27/50.. Training loss: 6591.692\n",
      "Epoch: 27/50.. Training loss: 8282.206\n",
      "Epoch: 28/50.. Training loss: 3698.192\n",
      "Epoch: 28/50.. Training loss: 5246.332\n",
      "Epoch: 28/50.. Training loss: 4249.815\n",
      "Epoch: 28/50.. Training loss: 3716.577\n",
      "Epoch: 28/50.. Training loss: 4626.371\n",
      "Epoch: 28/50.. Training loss: 3301.680\n",
      "Epoch: 28/50.. Training loss: 3577.441\n",
      "Epoch: 28/50.. Training loss: 3198.285\n",
      "Epoch: 28/50.. Training loss: 3220.071\n",
      "Epoch: 28/50.. Training loss: 4265.900\n",
      "Epoch: 28/50.. Training loss: 3589.257\n",
      "Epoch: 28/50.. Training loss: 4869.745\n",
      "Epoch: 28/50.. Training loss: 4470.978\n",
      "Epoch: 28/50.. Training loss: 5631.401\n",
      "Epoch: 28/50.. Training loss: 8339.608\n",
      "Epoch: 28/50.. Training loss: 3647.268\n",
      "Epoch: 28/50.. Training loss: 5366.750\n",
      "Epoch: 28/50.. Training loss: 4976.557\n",
      "Epoch: 28/50.. Training loss: 6869.970\n",
      "Epoch: 28/50.. Training loss: 8141.024\n",
      "Epoch: 29/50.. Training loss: 3987.257\n",
      "Epoch: 29/50.. Training loss: 4794.183\n",
      "Epoch: 29/50.. Training loss: 4565.670\n",
      "Epoch: 29/50.. Training loss: 3932.962\n",
      "Epoch: 29/50.. Training loss: 4152.746\n",
      "Epoch: 29/50.. Training loss: 3264.162\n",
      "Epoch: 29/50.. Training loss: 3726.457\n",
      "Epoch: 29/50.. Training loss: 2912.090\n",
      "Epoch: 29/50.. Training loss: 3483.642\n",
      "Epoch: 29/50.. Training loss: 4468.802\n",
      "Epoch: 29/50.. Training loss: 4166.744\n",
      "Epoch: 29/50.. Training loss: 4454.651\n",
      "Epoch: 29/50.. Training loss: 4187.848\n",
      "Epoch: 29/50.. Training loss: 9394.152\n",
      "Epoch: 29/50.. Training loss: 4364.138\n",
      "Epoch: 29/50.. Training loss: 3831.158\n",
      "Epoch: 29/50.. Training loss: 5052.575\n",
      "Epoch: 29/50.. Training loss: 5409.306\n",
      "Epoch: 29/50.. Training loss: 6571.968\n",
      "Epoch: 29/50.. Training loss: 8357.734\n",
      "Epoch: 30/50.. Training loss: 4644.890\n",
      "Epoch: 30/50.. Training loss: 4862.097\n",
      "Epoch: 30/50.. Training loss: 3673.687\n",
      "Epoch: 30/50.. Training loss: 4376.047\n",
      "Epoch: 30/50.. Training loss: 3528.108\n",
      "Epoch: 30/50.. Training loss: 3397.360\n",
      "Epoch: 30/50.. Training loss: 3651.387\n",
      "Epoch: 30/50.. Training loss: 3158.505\n",
      "Epoch: 30/50.. Training loss: 3288.018\n",
      "Epoch: 30/50.. Training loss: 4532.579\n",
      "Epoch: 30/50.. Training loss: 3985.869\n",
      "Epoch: 30/50.. Training loss: 4612.268\n",
      "Epoch: 30/50.. Training loss: 4137.560\n",
      "Epoch: 30/50.. Training loss: 10242.147\n",
      "Epoch: 30/50.. Training loss: 3490.171\n",
      "Epoch: 30/50.. Training loss: 3911.724\n",
      "Epoch: 30/50.. Training loss: 6137.643\n",
      "Epoch: 30/50.. Training loss: 4548.085\n",
      "Epoch: 30/50.. Training loss: 7205.460\n",
      "Epoch: 30/50.. Training loss: 7500.049\n",
      "Epoch: 31/50.. Training loss: 4706.171\n",
      "Epoch: 31/50.. Training loss: 4770.049\n",
      "Epoch: 31/50.. Training loss: 3714.338\n",
      "Epoch: 31/50.. Training loss: 4513.101\n",
      "Epoch: 31/50.. Training loss: 3351.611\n",
      "Epoch: 31/50.. Training loss: 3407.941\n",
      "Epoch: 31/50.. Training loss: 3567.662\n",
      "Epoch: 31/50.. Training loss: 3095.232\n",
      "Epoch: 31/50.. Training loss: 4077.048\n",
      "Epoch: 31/50.. Training loss: 3743.653\n",
      "Epoch: 31/50.. Training loss: 4122.120\n",
      "Epoch: 31/50.. Training loss: 4649.935\n",
      "Epoch: 31/50.. Training loss: 4312.459\n",
      "Epoch: 31/50.. Training loss: 9999.981\n",
      "Epoch: 31/50.. Training loss: 3719.840\n",
      "Epoch: 31/50.. Training loss: 3594.699\n",
      "Epoch: 31/50.. Training loss: 6286.575\n",
      "Epoch: 31/50.. Training loss: 4383.385\n",
      "Epoch: 31/50.. Training loss: 7203.361\n",
      "Epoch: 31/50.. Training loss: 7744.490\n",
      "Epoch: 32/50.. Training loss: 4682.297\n",
      "Epoch: 32/50.. Training loss: 4757.612\n",
      "Epoch: 32/50.. Training loss: 3736.751\n",
      "Epoch: 32/50.. Training loss: 4438.269\n",
      "Epoch: 32/50.. Training loss: 3359.460\n",
      "Epoch: 32/50.. Training loss: 3533.952\n",
      "Epoch: 32/50.. Training loss: 3441.561\n",
      "Epoch: 32/50.. Training loss: 3271.723\n",
      "Epoch: 32/50.. Training loss: 3847.920\n",
      "Epoch: 32/50.. Training loss: 3957.936\n",
      "Epoch: 32/50.. Training loss: 5262.354\n",
      "Epoch: 32/50.. Training loss: 3225.822\n",
      "Epoch: 32/50.. Training loss: 5617.146\n",
      "Epoch: 32/50.. Training loss: 9057.879\n",
      "Epoch: 32/50.. Training loss: 3741.464\n",
      "Epoch: 32/50.. Training loss: 3778.801\n",
      "Epoch: 32/50.. Training loss: 5682.092\n",
      "Epoch: 32/50.. Training loss: 4528.730\n",
      "Epoch: 32/50.. Training loss: 8641.963\n",
      "Epoch: 32/50.. Training loss: 6345.140\n",
      "Epoch: 33/50.. Training loss: 4538.137\n",
      "Epoch: 33/50.. Training loss: 4722.704\n",
      "Epoch: 33/50.. Training loss: 4015.149\n",
      "Epoch: 33/50.. Training loss: 4506.375\n",
      "Epoch: 33/50.. Training loss: 3590.794\n",
      "Epoch: 33/50.. Training loss: 3514.968\n",
      "Epoch: 33/50.. Training loss: 3003.094\n",
      "Epoch: 33/50.. Training loss: 3445.935\n",
      "Epoch: 33/50.. Training loss: 3980.477\n",
      "Epoch: 33/50.. Training loss: 3970.157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33/50.. Training loss: 4816.107\n",
      "Epoch: 33/50.. Training loss: 3267.227\n",
      "Epoch: 33/50.. Training loss: 6845.013\n",
      "Epoch: 33/50.. Training loss: 8025.360\n",
      "Epoch: 33/50.. Training loss: 3629.899\n",
      "Epoch: 33/50.. Training loss: 4083.004\n",
      "Epoch: 33/50.. Training loss: 6701.633\n",
      "Epoch: 33/50.. Training loss: 3370.225\n",
      "Epoch: 33/50.. Training loss: 8499.380\n",
      "Epoch: 33/50.. Training loss: 6196.996\n",
      "Epoch: 34/50.. Training loss: 5652.406\n",
      "Epoch: 34/50.. Training loss: 4072.199\n",
      "Epoch: 34/50.. Training loss: 3548.360\n",
      "Epoch: 34/50.. Training loss: 4622.970\n",
      "Epoch: 34/50.. Training loss: 3772.617\n",
      "Epoch: 34/50.. Training loss: 3216.035\n",
      "Epoch: 34/50.. Training loss: 3162.764\n",
      "Epoch: 34/50.. Training loss: 3580.761\n",
      "Epoch: 34/50.. Training loss: 4211.072\n",
      "Epoch: 34/50.. Training loss: 3622.419\n",
      "Epoch: 34/50.. Training loss: 4887.367\n",
      "Epoch: 34/50.. Training loss: 3127.225\n",
      "Epoch: 34/50.. Training loss: 7013.123\n",
      "Epoch: 34/50.. Training loss: 7772.431\n",
      "Epoch: 34/50.. Training loss: 4001.648\n",
      "Epoch: 34/50.. Training loss: 3783.502\n",
      "Epoch: 34/50.. Training loss: 6769.818\n",
      "Epoch: 34/50.. Training loss: 6800.835\n",
      "Epoch: 34/50.. Training loss: 8184.481\n",
      "Epoch: 35/50.. Training loss: 3240.948\n",
      "Epoch: 35/50.. Training loss: 5570.366\n",
      "Epoch: 35/50.. Training loss: 4480.455\n",
      "Epoch: 35/50.. Training loss: 3161.793\n",
      "Epoch: 35/50.. Training loss: 4637.458\n",
      "Epoch: 35/50.. Training loss: 3594.227\n",
      "Epoch: 35/50.. Training loss: 3283.014\n",
      "Epoch: 35/50.. Training loss: 3290.401\n",
      "Epoch: 35/50.. Training loss: 3292.221\n",
      "Epoch: 35/50.. Training loss: 4206.887\n",
      "Epoch: 35/50.. Training loss: 3619.879\n",
      "Epoch: 35/50.. Training loss: 5102.909\n",
      "Epoch: 35/50.. Training loss: 4597.141\n",
      "Epoch: 35/50.. Training loss: 5396.944\n",
      "Epoch: 35/50.. Training loss: 8085.361\n",
      "Epoch: 35/50.. Training loss: 3681.231\n",
      "Epoch: 35/50.. Training loss: 5492.835\n",
      "Epoch: 35/50.. Training loss: 5052.123\n",
      "Epoch: 35/50.. Training loss: 6748.847\n",
      "Epoch: 35/50.. Training loss: 8196.255\n",
      "Epoch: 36/50.. Training loss: 4704.905\n",
      "Epoch: 36/50.. Training loss: 4217.780\n",
      "Epoch: 36/50.. Training loss: 4357.375\n",
      "Epoch: 36/50.. Training loss: 4501.111\n",
      "Epoch: 36/50.. Training loss: 3653.429\n",
      "Epoch: 36/50.. Training loss: 3379.167\n",
      "Epoch: 36/50.. Training loss: 3469.512\n",
      "Epoch: 36/50.. Training loss: 3053.248\n",
      "Epoch: 36/50.. Training loss: 3323.006\n",
      "Epoch: 36/50.. Training loss: 4448.697\n",
      "Epoch: 36/50.. Training loss: 4271.356\n",
      "Epoch: 36/50.. Training loss: 4349.162\n",
      "Epoch: 36/50.. Training loss: 4338.239\n",
      "Epoch: 36/50.. Training loss: 10111.593\n",
      "Epoch: 36/50.. Training loss: 3530.213\n",
      "Epoch: 36/50.. Training loss: 3798.536\n",
      "Epoch: 36/50.. Training loss: 5655.046\n",
      "Epoch: 36/50.. Training loss: 4810.036\n",
      "Epoch: 36/50.. Training loss: 6698.778\n",
      "Epoch: 36/50.. Training loss: 8232.827\n",
      "Epoch: 37/50.. Training loss: 4657.069\n",
      "Epoch: 37/50.. Training loss: 4862.704\n",
      "Epoch: 37/50.. Training loss: 3709.432\n",
      "Epoch: 37/50.. Training loss: 4392.053\n",
      "Epoch: 37/50.. Training loss: 3789.375\n",
      "Epoch: 37/50.. Training loss: 3294.972\n",
      "Epoch: 37/50.. Training loss: 3675.147\n",
      "Epoch: 37/50.. Training loss: 3022.472\n",
      "Epoch: 37/50.. Training loss: 3186.815\n",
      "Epoch: 37/50.. Training loss: 4651.561\n",
      "Epoch: 37/50.. Training loss: 4181.107\n",
      "Epoch: 37/50.. Training loss: 4278.196\n",
      "Epoch: 37/50.. Training loss: 4308.790\n",
      "Epoch: 37/50.. Training loss: 10099.626\n",
      "Epoch: 37/50.. Training loss: 3945.604\n",
      "Epoch: 37/50.. Training loss: 3507.804\n",
      "Epoch: 37/50.. Training loss: 6065.193\n",
      "Epoch: 37/50.. Training loss: 4649.597\n",
      "Epoch: 37/50.. Training loss: 7218.024\n",
      "Epoch: 37/50.. Training loss: 7388.171\n",
      "Epoch: 38/50.. Training loss: 4694.569\n",
      "Epoch: 38/50.. Training loss: 4812.631\n",
      "Epoch: 38/50.. Training loss: 3709.520\n",
      "Epoch: 38/50.. Training loss: 4475.279\n",
      "Epoch: 38/50.. Training loss: 3691.992\n",
      "Epoch: 38/50.. Training loss: 3255.423\n",
      "Epoch: 38/50.. Training loss: 3720.757\n",
      "Epoch: 38/50.. Training loss: 3197.472\n",
      "Epoch: 38/50.. Training loss: 3948.826\n",
      "Epoch: 38/50.. Training loss: 3855.581\n",
      "Epoch: 38/50.. Training loss: 3955.402\n",
      "Epoch: 38/50.. Training loss: 4450.901\n",
      "Epoch: 38/50.. Training loss: 4259.427\n",
      "Epoch: 38/50.. Training loss: 10088.548\n",
      "Epoch: 38/50.. Training loss: 3773.812\n",
      "Epoch: 38/50.. Training loss: 4040.896\n",
      "Epoch: 38/50.. Training loss: 5793.323\n",
      "Epoch: 38/50.. Training loss: 4527.073\n",
      "Epoch: 38/50.. Training loss: 7779.903\n",
      "Epoch: 38/50.. Training loss: 6930.229\n",
      "Epoch: 39/50.. Training loss: 4731.165\n",
      "Epoch: 39/50.. Training loss: 4885.986\n",
      "Epoch: 39/50.. Training loss: 3550.452\n",
      "Epoch: 39/50.. Training loss: 4437.929\n",
      "Epoch: 39/50.. Training loss: 3781.859\n",
      "Epoch: 39/50.. Training loss: 3137.967\n",
      "Epoch: 39/50.. Training loss: 3603.958\n",
      "Epoch: 39/50.. Training loss: 3189.263\n",
      "Epoch: 39/50.. Training loss: 4035.989\n",
      "Epoch: 39/50.. Training loss: 3769.079\n",
      "Epoch: 39/50.. Training loss: 5265.933\n",
      "Epoch: 39/50.. Training loss: 3330.801\n",
      "Epoch: 39/50.. Training loss: 6443.464\n",
      "Epoch: 39/50.. Training loss: 8020.444\n",
      "Epoch: 39/50.. Training loss: 3738.317\n",
      "Epoch: 39/50.. Training loss: 3828.263\n",
      "Epoch: 39/50.. Training loss: 6490.085\n",
      "Epoch: 39/50.. Training loss: 3882.925\n",
      "Epoch: 39/50.. Training loss: 8450.847\n",
      "Epoch: 39/50.. Training loss: 6492.797\n",
      "Epoch: 40/50.. Training loss: 5211.972\n",
      "Epoch: 40/50.. Training loss: 4309.427\n",
      "Epoch: 40/50.. Training loss: 3701.228\n",
      "Epoch: 40/50.. Training loss: 4712.487\n",
      "Epoch: 40/50.. Training loss: 3284.457\n",
      "Epoch: 40/50.. Training loss: 3652.090\n",
      "Epoch: 40/50.. Training loss: 3027.050\n",
      "Epoch: 40/50.. Training loss: 3272.051\n",
      "Epoch: 40/50.. Training loss: 3996.341\n",
      "Epoch: 40/50.. Training loss: 3958.553\n",
      "Epoch: 40/50.. Training loss: 5008.326\n",
      "Epoch: 40/50.. Training loss: 3382.006\n",
      "Epoch: 40/50.. Training loss: 6539.390\n",
      "Epoch: 40/50.. Training loss: 8228.911\n",
      "Epoch: 40/50.. Training loss: 3419.250\n",
      "Epoch: 40/50.. Training loss: 4117.854\n",
      "Epoch: 40/50.. Training loss: 6672.765\n",
      "Epoch: 40/50.. Training loss: 5434.433\n",
      "Epoch: 40/50.. Training loss: 8429.363\n",
      "Epoch: 40/50.. Training loss: 4639.392\n",
      "Epoch: 41/50.. Training loss: 5213.381\n",
      "Epoch: 41/50.. Training loss: 4109.515\n",
      "Epoch: 41/50.. Training loss: 3921.635\n",
      "Epoch: 41/50.. Training loss: 4496.455\n",
      "Epoch: 41/50.. Training loss: 3518.373\n",
      "Epoch: 41/50.. Training loss: 3538.595\n",
      "Epoch: 41/50.. Training loss: 2907.925\n",
      "Epoch: 41/50.. Training loss: 3580.657\n",
      "Epoch: 41/50.. Training loss: 4267.469\n",
      "Epoch: 41/50.. Training loss: 3589.931\n",
      "Epoch: 41/50.. Training loss: 4814.318\n",
      "Epoch: 41/50.. Training loss: 3383.690\n",
      "Epoch: 41/50.. Training loss: 6751.187\n",
      "Epoch: 41/50.. Training loss: 8118.000\n",
      "Epoch: 41/50.. Training loss: 3871.238\n",
      "Epoch: 41/50.. Training loss: 4766.177\n",
      "Epoch: 41/50.. Training loss: 5579.925\n",
      "Epoch: 41/50.. Training loss: 6887.823\n",
      "Epoch: 41/50.. Training loss: 8137.212\n",
      "Epoch: 42/50.. Training loss: 3265.070\n",
      "Epoch: 42/50.. Training loss: 5478.548\n",
      "Epoch: 42/50.. Training loss: 4606.856\n",
      "Epoch: 42/50.. Training loss: 3159.148\n",
      "Epoch: 42/50.. Training loss: 4554.792\n",
      "Epoch: 42/50.. Training loss: 3603.055\n",
      "Epoch: 42/50.. Training loss: 3423.211\n",
      "Epoch: 42/50.. Training loss: 3075.892\n",
      "Epoch: 42/50.. Training loss: 3614.183\n",
      "Epoch: 42/50.. Training loss: 4312.238\n",
      "Epoch: 42/50.. Training loss: 3874.515\n",
      "Epoch: 42/50.. Training loss: 4858.407\n",
      "Epoch: 42/50.. Training loss: 4274.499\n",
      "Epoch: 42/50.. Training loss: 7695.565\n",
      "Epoch: 42/50.. Training loss: 5727.648\n",
      "Epoch: 42/50.. Training loss: 3965.965\n",
      "Epoch: 42/50.. Training loss: 5152.697\n",
      "Epoch: 42/50.. Training loss: 5437.632\n",
      "Epoch: 42/50.. Training loss: 6644.490\n",
      "Epoch: 42/50.. Training loss: 8289.770\n",
      "Epoch: 43/50.. Training loss: 4666.958\n",
      "Epoch: 43/50.. Training loss: 4633.116\n",
      "Epoch: 43/50.. Training loss: 3933.629\n",
      "Epoch: 43/50.. Training loss: 4382.435\n",
      "Epoch: 43/50.. Training loss: 3445.453\n",
      "Epoch: 43/50.. Training loss: 3474.760\n",
      "Epoch: 43/50.. Training loss: 3393.302\n",
      "Epoch: 43/50.. Training loss: 3089.554\n",
      "Epoch: 43/50.. Training loss: 3543.304\n",
      "Epoch: 43/50.. Training loss: 4252.480\n",
      "Epoch: 43/50.. Training loss: 4188.574\n",
      "Epoch: 43/50.. Training loss: 4541.447\n",
      "Epoch: 43/50.. Training loss: 4369.019\n",
      "Epoch: 43/50.. Training loss: 10206.292\n",
      "Epoch: 43/50.. Training loss: 3239.933\n",
      "Epoch: 43/50.. Training loss: 3852.157\n",
      "Epoch: 43/50.. Training loss: 6268.648\n",
      "Epoch: 43/50.. Training loss: 4331.421\n",
      "Epoch: 43/50.. Training loss: 7121.280\n",
      "Epoch: 43/50.. Training loss: 7819.309\n",
      "Epoch: 44/50.. Training loss: 4703.497\n",
      "Epoch: 44/50.. Training loss: 4634.898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44/50.. Training loss: 4020.906\n",
      "Epoch: 44/50.. Training loss: 4408.030\n",
      "Epoch: 44/50.. Training loss: 3439.790\n",
      "Epoch: 44/50.. Training loss: 3450.278\n",
      "Epoch: 44/50.. Training loss: 3512.019\n",
      "Epoch: 44/50.. Training loss: 3007.709\n",
      "Epoch: 44/50.. Training loss: 3450.913\n",
      "Epoch: 44/50.. Training loss: 4387.253\n",
      "Epoch: 44/50.. Training loss: 4231.926\n",
      "Epoch: 44/50.. Training loss: 4333.720\n",
      "Epoch: 44/50.. Training loss: 4571.893\n",
      "Epoch: 44/50.. Training loss: 10025.141\n",
      "Epoch: 44/50.. Training loss: 3771.309\n",
      "Epoch: 44/50.. Training loss: 3393.687\n",
      "Epoch: 44/50.. Training loss: 6132.293\n",
      "Epoch: 44/50.. Training loss: 4577.102\n",
      "Epoch: 44/50.. Training loss: 7217.122\n",
      "Epoch: 44/50.. Training loss: 7614.037\n",
      "Epoch: 45/50.. Training loss: 4597.845\n",
      "Epoch: 45/50.. Training loss: 4747.915\n",
      "Epoch: 45/50.. Training loss: 3960.356\n",
      "Epoch: 45/50.. Training loss: 4295.445\n",
      "Epoch: 45/50.. Training loss: 3574.156\n",
      "Epoch: 45/50.. Training loss: 3569.625\n",
      "Epoch: 45/50.. Training loss: 3478.731\n",
      "Epoch: 45/50.. Training loss: 3274.544\n",
      "Epoch: 45/50.. Training loss: 3819.022\n",
      "Epoch: 45/50.. Training loss: 3986.709\n",
      "Epoch: 45/50.. Training loss: 4775.580\n",
      "Epoch: 45/50.. Training loss: 3476.677\n",
      "Epoch: 45/50.. Training loss: 4454.565\n",
      "Epoch: 45/50.. Training loss: 10278.168\n",
      "Epoch: 45/50.. Training loss: 3790.630\n",
      "Epoch: 45/50.. Training loss: 3888.485\n",
      "Epoch: 45/50.. Training loss: 5531.866\n",
      "Epoch: 45/50.. Training loss: 4527.263\n",
      "Epoch: 45/50.. Training loss: 8552.705\n",
      "Epoch: 45/50.. Training loss: 6227.742\n",
      "Epoch: 46/50.. Training loss: 4674.338\n",
      "Epoch: 46/50.. Training loss: 4874.355\n",
      "Epoch: 46/50.. Training loss: 3698.746\n",
      "Epoch: 46/50.. Training loss: 4410.439\n",
      "Epoch: 46/50.. Training loss: 4033.979\n",
      "Epoch: 46/50.. Training loss: 3042.195\n",
      "Epoch: 46/50.. Training loss: 3534.020\n",
      "Epoch: 46/50.. Training loss: 3343.182\n",
      "Epoch: 46/50.. Training loss: 4030.683\n",
      "Epoch: 46/50.. Training loss: 3826.590\n",
      "Epoch: 46/50.. Training loss: 4849.878\n",
      "Epoch: 46/50.. Training loss: 3331.882\n",
      "Epoch: 46/50.. Training loss: 6865.432\n",
      "Epoch: 46/50.. Training loss: 7692.999\n",
      "Epoch: 46/50.. Training loss: 3780.974\n",
      "Epoch: 46/50.. Training loss: 4035.694\n",
      "Epoch: 46/50.. Training loss: 6809.407\n",
      "Epoch: 46/50.. Training loss: 3254.615\n",
      "Epoch: 46/50.. Training loss: 8463.461\n",
      "Epoch: 46/50.. Training loss: 6443.827\n",
      "Epoch: 47/50.. Training loss: 5664.330\n",
      "Epoch: 47/50.. Training loss: 4138.617\n",
      "Epoch: 47/50.. Training loss: 3418.667\n",
      "Epoch: 47/50.. Training loss: 4751.232\n",
      "Epoch: 47/50.. Training loss: 3483.639\n",
      "Epoch: 47/50.. Training loss: 3413.838\n",
      "Epoch: 47/50.. Training loss: 3280.150\n",
      "Epoch: 47/50.. Training loss: 3186.981\n",
      "Epoch: 47/50.. Training loss: 4078.678\n",
      "Epoch: 47/50.. Training loss: 3791.699\n",
      "Epoch: 47/50.. Training loss: 4954.294\n",
      "Epoch: 47/50.. Training loss: 3384.092\n",
      "Epoch: 47/50.. Training loss: 6714.150\n",
      "Epoch: 47/50.. Training loss: 8030.493\n",
      "Epoch: 47/50.. Training loss: 3560.710\n",
      "Epoch: 47/50.. Training loss: 4000.505\n",
      "Epoch: 47/50.. Training loss: 6723.764\n",
      "Epoch: 47/50.. Training loss: 6591.692\n",
      "Epoch: 47/50.. Training loss: 8282.209\n",
      "Epoch: 48/50.. Training loss: 3698.191\n",
      "Epoch: 48/50.. Training loss: 5246.332\n",
      "Epoch: 48/50.. Training loss: 4249.814\n",
      "Epoch: 48/50.. Training loss: 3716.576\n",
      "Epoch: 48/50.. Training loss: 4626.371\n",
      "Epoch: 48/50.. Training loss: 3301.680\n",
      "Epoch: 48/50.. Training loss: 3577.440\n",
      "Epoch: 48/50.. Training loss: 3198.286\n",
      "Epoch: 48/50.. Training loss: 3220.071\n",
      "Epoch: 48/50.. Training loss: 4265.899\n",
      "Epoch: 48/50.. Training loss: 3589.257\n",
      "Epoch: 48/50.. Training loss: 4869.745\n",
      "Epoch: 48/50.. Training loss: 4470.978\n",
      "Epoch: 48/50.. Training loss: 5631.400\n",
      "Epoch: 48/50.. Training loss: 8339.607\n",
      "Epoch: 48/50.. Training loss: 3647.268\n",
      "Epoch: 48/50.. Training loss: 5366.750\n",
      "Epoch: 48/50.. Training loss: 4976.557\n",
      "Epoch: 48/50.. Training loss: 6869.970\n",
      "Epoch: 48/50.. Training loss: 8141.027\n",
      "Epoch: 49/50.. Training loss: 3987.253\n",
      "Epoch: 49/50.. Training loss: 4794.184\n",
      "Epoch: 49/50.. Training loss: 4565.671\n",
      "Epoch: 49/50.. Training loss: 3932.960\n",
      "Epoch: 49/50.. Training loss: 4152.746\n",
      "Epoch: 49/50.. Training loss: 3264.162\n",
      "Epoch: 49/50.. Training loss: 3726.457\n",
      "Epoch: 49/50.. Training loss: 2912.091\n",
      "Epoch: 49/50.. Training loss: 3483.643\n",
      "Epoch: 49/50.. Training loss: 4468.801\n",
      "Epoch: 49/50.. Training loss: 4166.744\n",
      "Epoch: 49/50.. Training loss: 4454.650\n",
      "Epoch: 49/50.. Training loss: 4187.848\n",
      "Epoch: 49/50.. Training loss: 9394.150\n",
      "Epoch: 49/50.. Training loss: 4364.137\n",
      "Epoch: 49/50.. Training loss: 3831.158\n",
      "Epoch: 49/50.. Training loss: 5052.575\n",
      "Epoch: 49/50.. Training loss: 5409.307\n",
      "Epoch: 49/50.. Training loss: 6571.967\n",
      "Epoch: 49/50.. Training loss: 8357.738\n",
      "Epoch: 50/50.. Training loss: 4644.888\n",
      "Epoch: 50/50.. Training loss: 4862.097\n",
      "Epoch: 50/50.. Training loss: 3673.686\n",
      "Epoch: 50/50.. Training loss: 4376.046\n",
      "Epoch: 50/50.. Training loss: 3528.108\n",
      "Epoch: 50/50.. Training loss: 3397.360\n",
      "Epoch: 50/50.. Training loss: 3651.388\n",
      "Epoch: 50/50.. Training loss: 3158.507\n",
      "Epoch: 50/50.. Training loss: 3288.019\n",
      "Epoch: 50/50.. Training loss: 4532.578\n",
      "Epoch: 50/50.. Training loss: 3985.869\n",
      "Epoch: 50/50.. Training loss: 4612.268\n",
      "Epoch: 50/50.. Training loss: 4137.560\n",
      "Epoch: 50/50.. Training loss: 10242.145\n",
      "Epoch: 50/50.. Training loss: 3490.171\n",
      "Epoch: 50/50.. Training loss: 3911.724\n",
      "Epoch: 50/50.. Training loss: 6137.643\n",
      "Epoch: 50/50.. Training loss: 4548.085\n",
      "Epoch: 50/50.. Training loss: 7205.460\n",
      "Epoch: 50/50.. Training loss: 7500.051\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "training(model, criterion, optimizer, featureloader, labelloader, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
