{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.visualization import cycle_plot, save_plot\n",
    "from src.data import LoadData, list_dataset, get_json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EngineID',\n",
       " 'Cycle',\n",
       " 'OpSetting1',\n",
       " 'OpSetting2',\n",
       " 'OpSetting3',\n",
       " 'T2',\n",
       " 'T24',\n",
       " 'T30',\n",
       " 'T50',\n",
       " 'P2',\n",
       " 'P15',\n",
       " 'P30',\n",
       " 'Nf',\n",
       " 'Nc',\n",
       " 'epr',\n",
       " 'Ps30',\n",
       " 'phi',\n",
       " 'NRf',\n",
       " 'NRc',\n",
       " 'BPR',\n",
       " 'farB',\n",
       " 'htBleed',\n",
       " 'Nf_dmd',\n",
       " 'PCNfR_dmd',\n",
       " 'W31',\n",
       " 'W32']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featurenames = get_json('references/col_to_feat.json')\n",
    "featurenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EngineID</th>\n",
       "      <th>Cycle</th>\n",
       "      <th>OpSetting1</th>\n",
       "      <th>OpSetting2</th>\n",
       "      <th>OpSetting3</th>\n",
       "      <th>T2</th>\n",
       "      <th>T24</th>\n",
       "      <th>T30</th>\n",
       "      <th>T50</th>\n",
       "      <th>P2</th>\n",
       "      <th>...</th>\n",
       "      <th>NRf</th>\n",
       "      <th>NRc</th>\n",
       "      <th>BPR</th>\n",
       "      <th>farB</th>\n",
       "      <th>htBleed</th>\n",
       "      <th>Nf_dmd</th>\n",
       "      <th>PCNfR_dmd</th>\n",
       "      <th>W31</th>\n",
       "      <th>W32</th>\n",
       "      <th>RUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.82</td>\n",
       "      <td>1589.70</td>\n",
       "      <td>1400.60</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.02</td>\n",
       "      <td>8138.62</td>\n",
       "      <td>8.419</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392.0</td>\n",
       "      <td>2388.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.06</td>\n",
       "      <td>23.419</td>\n",
       "      <td>191.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.15</td>\n",
       "      <td>1591.82</td>\n",
       "      <td>1403.14</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.07</td>\n",
       "      <td>8131.49</td>\n",
       "      <td>8.432</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392.0</td>\n",
       "      <td>2388.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.00</td>\n",
       "      <td>23.424</td>\n",
       "      <td>190.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1587.99</td>\n",
       "      <td>1404.20</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.03</td>\n",
       "      <td>8133.23</td>\n",
       "      <td>8.418</td>\n",
       "      <td>0.03</td>\n",
       "      <td>390.0</td>\n",
       "      <td>2388.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.95</td>\n",
       "      <td>23.344</td>\n",
       "      <td>189.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1582.79</td>\n",
       "      <td>1401.87</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.08</td>\n",
       "      <td>8133.83</td>\n",
       "      <td>8.368</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392.0</td>\n",
       "      <td>2388.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.88</td>\n",
       "      <td>23.374</td>\n",
       "      <td>188.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.37</td>\n",
       "      <td>1582.85</td>\n",
       "      <td>1406.22</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.04</td>\n",
       "      <td>8133.80</td>\n",
       "      <td>8.429</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393.0</td>\n",
       "      <td>2388.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.90</td>\n",
       "      <td>23.404</td>\n",
       "      <td>187.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   EngineID  Cycle  OpSetting1  OpSetting2  OpSetting3      T2     T24  \\\n",
       "0       1.0    1.0      -0.001        -0.0       100.0  518.67  641.82   \n",
       "1       1.0    2.0       0.002        -0.0       100.0  518.67  642.15   \n",
       "2       1.0    3.0      -0.004         0.0       100.0  518.67  642.35   \n",
       "3       1.0    4.0       0.001         0.0       100.0  518.67  642.35   \n",
       "4       1.0    5.0      -0.002        -0.0       100.0  518.67  642.37   \n",
       "\n",
       "       T30      T50     P2  ...        NRf      NRc    BPR  farB  htBleed  \\\n",
       "0  1589.70  1400.60  14.62  ...    2388.02  8138.62  8.419  0.03    392.0   \n",
       "1  1591.82  1403.14  14.62  ...    2388.07  8131.49  8.432  0.03    392.0   \n",
       "2  1587.99  1404.20  14.62  ...    2388.03  8133.23  8.418  0.03    390.0   \n",
       "3  1582.79  1401.87  14.62  ...    2388.08  8133.83  8.368  0.03    392.0   \n",
       "4  1582.85  1406.22  14.62  ...    2388.04  8133.80  8.429  0.03    393.0   \n",
       "\n",
       "   Nf_dmd  PCNfR_dmd    W31     W32    RUL  \n",
       "0  2388.0      100.0  39.06  23.419  191.0  \n",
       "1  2388.0      100.0  39.00  23.424  190.0  \n",
       "2  2388.0      100.0  38.95  23.344  189.0  \n",
       "3  2388.0      100.0  38.88  23.374  188.0  \n",
       "4  2388.0      100.0  38.90  23.404  187.0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/processed/processed.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20631, 26)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = LoadData('train_FD001.txt', sep='\\s+', names=featurenames)\n",
    "raw.features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] figure is saved\n"
     ]
    }
   ],
   "source": [
    "ax = cycle_plot(data, 1, 'NRf')\n",
    "save_plot(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20631, 26)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.72808356, -1.56517022, -0.31597968, -1.3729532 ,  0.        ,\n",
       "         0.        , -1.72172539, -0.13425518, -0.92593596, -1.        ,\n",
       "         0.14168333,  1.12114057, -0.51633841, -0.86281332,  0.        ,\n",
       "        -0.2664666 ,  0.33426184, -1.05889024, -0.26907104, -0.6038157 ,\n",
       "        -1.        , -0.78170979,  0.        ,  0.        ,  1.34849274,\n",
       "         1.19442705],\n",
       "       [-1.72808356, -1.55065208,  0.87272193, -1.03172035,  0.        ,\n",
       "         0.        , -1.06177971,  0.21152849, -0.64372587, -1.        ,\n",
       "         0.14168333,  0.43193018, -0.798093  , -0.95881761,  0.        ,\n",
       "        -0.19158293,  1.17489932, -0.363646  , -0.64284474, -0.27585181,\n",
       "        -1.        , -0.78170979,  0.        ,  0.        ,  1.01652793,\n",
       "         1.23692196],\n",
       "       [-1.72808356, -1.53613393, -1.96187421,  1.01567674,  0.        ,\n",
       "         0.        , -0.66181262, -0.41316559, -0.52595315, -1.        ,\n",
       "         0.14168333,  1.00815526, -0.23458381, -0.55713925,  0.        ,\n",
       "        -1.0153033 ,  1.36472069, -0.91984139, -0.55162928, -0.64914404,\n",
       "        -1.        , -2.07309423,  0.        ,  0.        ,  0.73989059,\n",
       "         0.50342281],\n",
       "       [-1.72808356, -1.52161579,  0.32409042, -0.00802181,  0.        ,\n",
       "         0.        , -0.66181262, -1.26131421, -0.78483092, -1.        ,\n",
       "         0.14168333,  1.22282735,  0.18804808, -0.71382551,  0.        ,\n",
       "        -1.53948899,  1.96130213, -0.22459715, -0.52017567, -1.97166509,\n",
       "        -1.        , -0.78170979,  0.        ,  0.        ,  0.3525983 ,\n",
       "         0.77779214],\n",
       "       [-1.72808356, -1.50709764, -0.86461119, -0.69048751,  0.        ,\n",
       "         0.        , -0.62181592, -1.25152788, -0.30151835, -1.        ,\n",
       "         0.14168333,  0.71439345, -0.51633841, -0.45705929,  0.        ,\n",
       "        -0.97786146,  1.0528713 , -0.78079254, -0.52174835, -0.33984476,\n",
       "        -1.        , -0.13601757,  0.        ,  0.        ,  0.46325324,\n",
       "         1.05955189]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_raw = raw.standardize()\n",
    "print(scaled_raw.shape)\n",
    "scaled_raw[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    \"\"\"Neural Network Generator.\"\"\"\n",
    "    def __init__(self, input_size=24, hidden_sizes=[20,15], output_size=1, drop_p=0.4):\n",
    "        \"\"\"Generate fully-connected neural network.\n",
    "\n",
    "        parameters\n",
    "        ----------\n",
    "        input_size (int): size of the input\n",
    "        hidden_sizes (list of int): size of the hidden layers\n",
    "        output_layer (int): size of the output layer\n",
    "        drop_p (float): dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Linear(input_size, hidden_sizes[0])\n",
    "        ])\n",
    "        layers = zip(hidden_sizes[:-1], hidden_sizes[1:])\n",
    "        self.hidden_layers.extend([nn.Linear(h1,h2) for h1,h2 in layers])\n",
    "        self.output = nn.Linear(hidden_sizes[-1], output_size)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        for index, linear in enumerate(self.hidden_layers):\n",
    "            if index % 2 == 0:\n",
    "                X = linear(X)\n",
    "                X = self.dropout(X)\n",
    "            elif index % 2 != 0:\n",
    "                X = torch.tanh(linear(X))\n",
    "        \n",
    "        X = self.output(X)\n",
    "\n",
    "        return F.relu(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NN(\n",
       "  (hidden_layers): ModuleList(\n",
       "    (0): Linear(in_features=24, out_features=24, bias=True)\n",
       "    (1): Linear(in_features=24, out_features=10, bias=True)\n",
       "  )\n",
       "  (output): Linear(in_features=10, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.4)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NN(hidden_sizes=[24, 10])\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-7.1143e-02, -7.3430e-02, -3.0977e-02,  9.2739e-02, -1.8801e-01,\n",
      "          1.5223e-01, -5.0554e-02,  7.6459e-02,  7.3142e-02,  2.4187e-02,\n",
      "          1.5516e-01, -2.1548e-02,  1.8929e-01,  9.6096e-02, -1.3130e-01,\n",
      "         -1.1802e-01,  9.5124e-03, -1.4601e-01,  6.6404e-02, -2.0171e-02,\n",
      "         -6.3556e-03, -1.3996e-01, -7.2078e-02,  4.7393e-02],\n",
      "        [-2.6084e-03,  1.6199e-02, -1.0984e-01, -5.1767e-02,  1.9739e-01,\n",
      "         -1.4952e-01, -1.7496e-01,  1.4858e-01,  9.1671e-02, -1.2103e-01,\n",
      "          1.1901e-01, -6.9988e-02,  1.7554e-01, -1.0293e-01,  4.7561e-02,\n",
      "          1.3348e-01, -4.9979e-02, -3.9201e-02, -1.8800e-01, -3.0802e-02,\n",
      "          5.4438e-02,  1.5493e-01,  2.0072e-01,  8.4845e-02],\n",
      "        [ 1.8276e-01, -1.2165e-01,  4.9307e-02,  5.3011e-02, -1.4169e-01,\n",
      "         -5.7339e-02, -1.1851e-02, -1.7536e-01, -1.5572e-02, -5.9955e-02,\n",
      "          8.9461e-02,  3.5372e-02, -7.9280e-02,  3.1429e-02,  6.0229e-02,\n",
      "          1.9308e-01,  1.5943e-01,  8.0365e-02,  5.7093e-02, -2.0261e-01,\n",
      "         -2.3383e-03,  1.6606e-01, -4.3193e-02, -2.7244e-02],\n",
      "        [ 9.8410e-02, -9.7883e-03, -1.7041e-01,  6.3628e-02,  5.9902e-02,\n",
      "          8.5690e-02,  1.9458e-01, -1.0223e-02,  4.9439e-02, -1.4984e-01,\n",
      "          7.6980e-02,  1.5326e-01, -1.7428e-01,  2.5023e-02, -2.2358e-02,\n",
      "          7.6223e-02,  1.1805e-01,  3.1796e-02,  8.4534e-02, -1.0169e-01,\n",
      "          1.6003e-01,  1.3574e-01, -1.3042e-01, -1.7447e-01],\n",
      "        [ 6.4224e-04,  1.6902e-01,  1.2592e-01,  1.2062e-01,  3.0411e-02,\n",
      "         -1.0961e-01, -1.8398e-01,  1.5741e-01, -1.7740e-01, -1.7476e-01,\n",
      "          2.8578e-03, -1.5193e-01, -1.5688e-01, -2.0258e-01,  1.0727e-01,\n",
      "          4.5086e-03,  1.1291e-01,  9.1859e-02,  1.8763e-01,  1.3853e-01,\n",
      "          6.3675e-02,  1.5084e-01,  1.9731e-01, -1.7287e-01],\n",
      "        [ 8.5496e-02, -1.0619e-01,  6.6983e-02,  1.7486e-01,  1.6232e-01,\n",
      "          6.1911e-02, -1.4928e-01, -3.7064e-02,  1.5902e-01, -1.6275e-01,\n",
      "          6.7216e-02, -1.1347e-01,  6.0771e-02,  1.5160e-01, -4.4836e-02,\n",
      "         -1.7506e-01, -9.4307e-02,  5.7132e-02,  3.6332e-02,  4.6045e-06,\n",
      "         -1.0660e-01,  1.0461e-01,  4.2560e-02,  1.6272e-01],\n",
      "        [-7.0479e-02,  1.7208e-01,  1.5165e-01,  1.7128e-01, -3.0472e-02,\n",
      "         -7.9452e-02, -1.6606e-03,  1.6915e-01, -2.0832e-02,  2.4900e-02,\n",
      "          1.2723e-02,  1.3594e-01, -6.3956e-02,  2.2897e-03,  8.4937e-02,\n",
      "         -1.4342e-01,  1.6294e-01,  4.7862e-02, -7.7404e-02, -3.9588e-02,\n",
      "         -2.5523e-02,  1.6282e-01,  9.3674e-02, -1.8443e-01],\n",
      "        [-7.8284e-02,  1.7933e-01,  3.2702e-02,  6.6873e-02,  1.2640e-01,\n",
      "          1.1426e-01,  1.1949e-01,  1.3545e-02,  6.4466e-02, -4.5525e-02,\n",
      "          9.5506e-02,  1.8183e-01,  1.6455e-01, -1.4742e-01, -2.4558e-02,\n",
      "         -1.5723e-01, -7.6300e-02, -8.5065e-02, -1.0060e-01, -2.7431e-02,\n",
      "          1.4137e-01,  1.9896e-01, -1.3862e-01, -1.7114e-01],\n",
      "        [-1.1155e-01, -3.1741e-02,  1.0134e-03,  7.1944e-02, -3.5888e-02,\n",
      "         -7.6370e-03,  3.4778e-02,  1.2079e-01,  1.1090e-01,  1.7862e-01,\n",
      "         -1.3721e-01, -4.3684e-02, -1.9279e-01,  5.4921e-02, -7.5671e-02,\n",
      "         -1.3098e-01,  6.7547e-02, -1.8036e-01, -4.0095e-02, -1.8548e-01,\n",
      "          1.6079e-01,  4.2337e-02, -1.5529e-01,  1.8700e-01],\n",
      "        [ 1.4448e-01,  3.1317e-03,  8.8630e-03,  3.7137e-02,  7.1294e-02,\n",
      "         -1.2687e-01, -6.7538e-02, -7.0352e-02,  4.6578e-02, -5.4249e-02,\n",
      "         -1.9066e-01, -1.1617e-01, -8.6593e-02,  1.6257e-01, -4.8696e-02,\n",
      "          8.4772e-02, -4.3221e-02,  1.0487e-01, -1.1150e-01, -1.9509e-01,\n",
      "          7.5270e-02, -7.8630e-02, -6.7685e-02,  1.1597e-01],\n",
      "        [-1.6060e-01, -1.0420e-01,  5.8663e-02,  1.4904e-02,  6.8758e-02,\n",
      "          8.6899e-04,  1.0076e-01, -1.2700e-01, -6.5908e-03,  6.3940e-02,\n",
      "         -6.2310e-02, -1.7589e-01, -3.4747e-02, -7.9077e-02, -1.0976e-01,\n",
      "         -2.0167e-01,  1.1651e-01,  1.0999e-02, -1.2376e-01, -7.6935e-02,\n",
      "         -1.4374e-01, -9.0165e-02, -5.4205e-02, -1.0536e-01],\n",
      "        [-2.1444e-02, -9.3582e-02,  6.3237e-02, -4.5122e-02,  1.8155e-01,\n",
      "          1.6077e-01,  9.9387e-02, -1.1940e-01,  1.0534e-01, -8.7066e-03,\n",
      "         -8.2003e-02,  1.2162e-01, -1.5466e-01, -1.6951e-01, -7.6958e-02,\n",
      "          1.5713e-01,  2.0794e-02, -1.2816e-01, -3.9378e-02,  1.2934e-01,\n",
      "         -1.7794e-01,  5.4005e-02, -1.0436e-01,  2.8169e-02],\n",
      "        [-1.8653e-01, -1.2728e-01,  1.7060e-01,  8.8716e-02, -7.8342e-02,\n",
      "          8.1862e-02,  3.0944e-02, -1.9836e-02,  1.2874e-01,  4.7216e-02,\n",
      "          1.3036e-01,  1.0893e-01, -1.8744e-02, -4.5124e-02, -3.3547e-02,\n",
      "          7.1539e-02, -1.0689e-01,  9.2648e-02,  1.2784e-01, -1.6914e-01,\n",
      "         -4.9474e-02, -1.6704e-01, -1.0840e-01, -1.0138e-01],\n",
      "        [ 2.0311e-01, -2.0245e-01,  2.8882e-02, -1.7182e-01,  1.7951e-01,\n",
      "         -1.9853e-01,  1.6993e-01, -1.7618e-01,  1.2331e-02, -6.9037e-02,\n",
      "          9.8527e-02,  5.3695e-02, -1.6779e-01,  1.2613e-01,  1.0027e-01,\n",
      "          7.3332e-02,  1.9567e-01,  1.4251e-02,  1.2441e-01,  8.0080e-02,\n",
      "          1.4614e-01,  1.8623e-01,  1.0007e-01,  9.0200e-02],\n",
      "        [-4.8162e-02,  1.5033e-01, -2.9755e-02, -1.7234e-01, -6.3666e-02,\n",
      "          1.4199e-01,  1.2438e-01,  1.0466e-01,  2.4729e-02, -1.7126e-01,\n",
      "         -1.9639e-03,  1.9728e-01,  2.4549e-02,  9.4612e-02,  2.9114e-02,\n",
      "         -3.8072e-02,  9.1505e-02, -3.0176e-02, -1.9352e-01, -1.1017e-01,\n",
      "          4.3657e-02,  1.4439e-01, -1.0402e-01, -8.0047e-02],\n",
      "        [-1.4780e-01,  1.9493e-01,  1.8360e-01, -1.4363e-01,  1.6359e-01,\n",
      "         -1.4277e-02, -1.2353e-01, -1.2953e-01, -1.6940e-01, -5.7567e-02,\n",
      "          1.1859e-01,  8.6834e-02,  5.6867e-02,  1.1264e-01,  2.6662e-02,\n",
      "         -1.5878e-01,  6.7329e-02, -1.7262e-01, -1.1875e-01, -1.0354e-02,\n",
      "          9.7106e-02, -8.1098e-02, -6.4273e-02,  1.3030e-01],\n",
      "        [ 3.2429e-02,  2.4959e-02, -8.7481e-02, -4.2895e-02,  4.5960e-02,\n",
      "          6.6031e-02, -1.9768e-01, -9.8914e-02, -1.0012e-01, -9.6808e-02,\n",
      "          6.2706e-02,  1.9473e-01,  2.6995e-02,  1.8085e-01,  1.4005e-02,\n",
      "         -9.8981e-03, -1.3535e-01, -7.3043e-02, -2.9034e-02, -1.6299e-01,\n",
      "          6.5791e-02,  5.6200e-02, -5.2389e-02, -1.0643e-01],\n",
      "        [ 3.2539e-02, -3.7893e-02,  4.9292e-02,  1.2551e-01, -7.8064e-02,\n",
      "         -4.4593e-02, -2.8701e-02,  5.5706e-02,  8.7492e-02, -1.5908e-01,\n",
      "          2.5116e-02,  1.1221e-01, -2.0095e-01,  1.1586e-01,  4.4023e-02,\n",
      "          1.4374e-01, -1.8181e-01,  7.2526e-02, -6.2536e-02, -1.4724e-01,\n",
      "         -8.5912e-02, -1.2546e-01, -1.5576e-01,  6.4792e-02],\n",
      "        [ 1.6235e-01, -2.8226e-02,  1.3820e-01,  1.0524e-01, -2.2124e-02,\n",
      "         -3.9174e-02, -3.2082e-02, -8.3380e-02,  1.2132e-01, -1.5582e-01,\n",
      "          1.8477e-01,  6.1086e-02,  1.0308e-02, -1.7531e-02,  1.2621e-02,\n",
      "         -1.9771e-01, -2.2343e-02, -2.0373e-01, -1.7714e-01,  1.3245e-01,\n",
      "          1.7403e-01,  3.3850e-02,  2.0360e-01,  1.2453e-01],\n",
      "        [ 4.4816e-02, -5.7302e-02, -3.1295e-03,  1.3764e-01,  1.1358e-01,\n",
      "          9.0004e-02, -1.8159e-01, -7.5246e-03, -9.1950e-02,  1.4070e-01,\n",
      "          1.3000e-01,  9.1772e-02, -4.5511e-02, -1.9027e-02,  1.1207e-01,\n",
      "          9.3935e-02, -7.8966e-02,  1.1006e-01,  4.2247e-02,  1.9695e-02,\n",
      "         -2.3728e-02,  1.6474e-01,  5.0016e-02,  1.6595e-01],\n",
      "        [-3.7238e-03, -1.6263e-01,  8.2265e-02, -8.8295e-02, -1.9663e-01,\n",
      "          5.3867e-02, -1.8670e-01,  1.5399e-01, -1.4154e-01,  2.0179e-01,\n",
      "          3.9128e-02, -8.6814e-02, -2.5344e-02, -1.7832e-01, -5.9313e-02,\n",
      "          4.4013e-03, -1.6132e-01, -1.4150e-01,  1.7409e-01, -1.5273e-01,\n",
      "          9.6613e-02,  7.1696e-02,  1.9084e-01, -6.9051e-02],\n",
      "        [ 7.7742e-03,  1.9745e-01, -7.1269e-02, -1.1921e-01,  1.7966e-01,\n",
      "          2.8964e-02,  1.2449e-01, -1.5764e-01, -4.7559e-02, -7.0274e-02,\n",
      "         -1.2794e-01, -2.7221e-02, -4.6313e-02,  1.8145e-01, -6.0203e-02,\n",
      "         -1.6886e-01, -4.4353e-02,  8.5449e-03, -1.7608e-01, -1.7412e-02,\n",
      "          1.1209e-01,  1.6068e-01, -1.5229e-01,  1.6174e-01],\n",
      "        [-2.7153e-02, -1.3047e-01,  1.2913e-01,  1.3772e-01, -4.6437e-02,\n",
      "          1.9698e-01, -4.6305e-02,  2.0393e-01, -1.4010e-01,  5.9117e-02,\n",
      "         -1.1195e-01, -1.8837e-01,  8.5903e-02, -1.5193e-01,  3.9807e-02,\n",
      "         -1.6169e-01,  8.7660e-02,  9.9501e-02, -1.0092e-01, -1.4177e-01,\n",
      "         -1.9981e-03,  1.7045e-01, -5.8860e-02,  1.0387e-01],\n",
      "        [-1.0437e-01, -4.4499e-02, -1.7948e-01, -7.8095e-02, -8.1931e-02,\n",
      "         -8.3183e-02, -8.8679e-02,  1.7602e-01, -1.0157e-01,  1.8435e-01,\n",
      "         -1.9606e-01,  7.8037e-02, -1.3669e-01,  8.9308e-02,  1.0396e-01,\n",
      "         -6.1953e-02,  1.8538e-01, -4.4560e-02, -5.1638e-02,  1.9053e-01,\n",
      "         -5.3685e-02,  3.6797e-02, -9.8377e-02,  3.6863e-03]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0548,  0.0167, -0.0659, -0.0355,  0.0855, -0.1427,  0.1815,  0.1780,\n",
      "         0.1798, -0.1930, -0.1442, -0.1634, -0.0696,  0.0542,  0.1775,  0.0420,\n",
      "        -0.1949,  0.0940,  0.0981, -0.0833,  0.0665,  0.0736, -0.1079, -0.0136],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1421, -0.1004,  0.1723,  0.0646,  0.0165, -0.1485,  0.1681,  0.1714,\n",
      "          0.0090,  0.1985,  0.1925, -0.1261, -0.1351,  0.0363,  0.0061, -0.0431,\n",
      "         -0.1981,  0.0659, -0.0996,  0.1382,  0.0170,  0.0076,  0.0220,  0.0783],\n",
      "        [-0.0114,  0.0480, -0.1824, -0.0067, -0.1847, -0.0628,  0.0068, -0.0493,\n",
      "         -0.1657, -0.1688, -0.0454,  0.1881,  0.1239,  0.0215,  0.0841, -0.0205,\n",
      "         -0.0411, -0.0908, -0.0851, -0.0337, -0.1114, -0.0301,  0.1463, -0.0884],\n",
      "        [ 0.0239, -0.0855, -0.0932, -0.0069, -0.0058, -0.0908,  0.0363,  0.0041,\n",
      "          0.0891,  0.0672, -0.0708,  0.0485, -0.0357, -0.1790,  0.0610, -0.1357,\n",
      "         -0.0959, -0.0811, -0.0066,  0.0890,  0.0364,  0.0880, -0.0179, -0.0289],\n",
      "        [ 0.1542, -0.1935, -0.0388,  0.0626, -0.1629,  0.0212, -0.0378,  0.0653,\n",
      "          0.1621,  0.0462, -0.1932, -0.1853, -0.0428, -0.1974, -0.1635,  0.0542,\n",
      "          0.1461,  0.1273,  0.0545,  0.0479, -0.0419,  0.0197, -0.0162,  0.1229],\n",
      "        [-0.1811,  0.1563, -0.1235,  0.1951, -0.1726, -0.0694,  0.0141,  0.0936,\n",
      "          0.0682, -0.0078, -0.0895,  0.0530,  0.1657, -0.0062,  0.1872, -0.0427,\n",
      "          0.1852,  0.1072,  0.0188, -0.0792, -0.1507,  0.0351,  0.0687,  0.0571],\n",
      "        [-0.0264,  0.1900, -0.1751,  0.1824,  0.1823, -0.0285, -0.1770, -0.1549,\n",
      "         -0.1409,  0.0737, -0.1025,  0.0405, -0.0423,  0.0807, -0.1337,  0.1776,\n",
      "         -0.0425, -0.0348, -0.1446,  0.0508, -0.0928,  0.1696,  0.1527, -0.1522],\n",
      "        [ 0.1196,  0.1159, -0.0675, -0.1092, -0.0144,  0.1571, -0.0588,  0.1028,\n",
      "          0.1345, -0.1998, -0.0221,  0.1964,  0.1591, -0.0276, -0.1276,  0.1222,\n",
      "         -0.1955, -0.1185, -0.1193, -0.0945, -0.1124, -0.1633,  0.0282, -0.0699],\n",
      "        [-0.2009,  0.0036,  0.1930, -0.1727,  0.1498, -0.0057, -0.0122, -0.2001,\n",
      "         -0.1120,  0.0738,  0.2020, -0.0558,  0.0172,  0.1182, -0.1817, -0.0826,\n",
      "         -0.1923,  0.0111,  0.1529, -0.1253, -0.0753, -0.0347, -0.1686, -0.2022],\n",
      "        [-0.0337,  0.0075,  0.1533, -0.0058,  0.1468, -0.1229, -0.1026,  0.2032,\n",
      "          0.1603,  0.1313,  0.0359,  0.1521,  0.0490, -0.1151,  0.0527,  0.0665,\n",
      "          0.1404,  0.1611, -0.1788,  0.1243,  0.1481,  0.0376,  0.1402,  0.0903],\n",
      "        [ 0.1286, -0.1219, -0.0070, -0.0814, -0.1013, -0.1550, -0.0548,  0.0395,\n",
      "         -0.1969, -0.1737, -0.1371,  0.0589,  0.0413, -0.1880, -0.0570, -0.0002,\n",
      "          0.1081,  0.0778, -0.1349, -0.0558,  0.1072, -0.1535,  0.0440, -0.0117],\n",
      "        [-0.1824, -0.1917,  0.1022, -0.1737, -0.1397,  0.0944, -0.1015, -0.0376,\n",
      "         -0.0461,  0.1232, -0.0910,  0.1482,  0.1334, -0.0046,  0.1885, -0.0113,\n",
      "          0.0952,  0.0609, -0.0916,  0.1596, -0.0080,  0.0727,  0.0805, -0.1813],\n",
      "        [-0.1057, -0.1210,  0.1037, -0.0459, -0.1101, -0.1817, -0.1185,  0.1152,\n",
      "          0.1268,  0.0389, -0.1852,  0.0012,  0.2017,  0.0978, -0.0784,  0.1734,\n",
      "          0.0202, -0.1814, -0.1427,  0.0711, -0.1952,  0.0055,  0.0742,  0.1451]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1989,  0.0171, -0.1885, -0.0652, -0.0277, -0.1046, -0.0217, -0.0487,\n",
      "         0.0758, -0.0401, -0.1002, -0.0213], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0780, -0.0336, -0.0673, -0.0851, -0.0473, -0.2759,  0.1759,  0.0895,\n",
      "         -0.2087,  0.1743, -0.0001,  0.1092],\n",
      "        [-0.1950,  0.0004, -0.2734, -0.2275, -0.1967, -0.0503,  0.0312,  0.0754,\n",
      "          0.0562,  0.1137,  0.2822,  0.0497],\n",
      "        [-0.1418, -0.1893,  0.0388, -0.2766,  0.0511, -0.1831,  0.1305, -0.0008,\n",
      "         -0.2000, -0.2235,  0.0969,  0.1786],\n",
      "        [-0.2291, -0.0673,  0.1049,  0.0971,  0.0026,  0.1132,  0.2337,  0.0011,\n",
      "          0.1050, -0.1243,  0.1598,  0.2185],\n",
      "        [ 0.0765, -0.2115, -0.1455,  0.2429,  0.1098, -0.1791,  0.1938,  0.0397,\n",
      "          0.0980,  0.2328,  0.1281, -0.1456],\n",
      "        [-0.0883,  0.0139, -0.2868, -0.0929,  0.1637,  0.1016, -0.0351, -0.1221,\n",
      "         -0.0384, -0.1428,  0.1504,  0.0834],\n",
      "        [ 0.1421, -0.2832, -0.1157,  0.1285, -0.1899, -0.0028, -0.1004,  0.1664,\n",
      "         -0.0989,  0.2716,  0.0175, -0.1940],\n",
      "        [-0.2856, -0.2723, -0.1908, -0.0556,  0.0191, -0.0606,  0.1446, -0.1624,\n",
      "         -0.2035, -0.0763,  0.1195,  0.1869]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1200,  0.2268,  0.0847,  0.0288,  0.0775, -0.2051,  0.1263,  0.2080],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1811, -0.1062, -0.3014,  0.0252,  0.1050,  0.0539,  0.2062, -0.0953]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1551], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    featureloader,\n",
    "    labelloader,\n",
    "    epochs=5,\n",
    "    print_every=40\n",
    "):\n",
    "    epoch_loss = 0\n",
    "    steps = 0\n",
    "    1\n",
    "    for epoch in range(epochs):\n",
    "        for features, labels in zip(featureloader, labelloader):\n",
    "            steps += 1\n",
    "            features, labels = features.type(torch.FloatTensor), labels.type(torch.FloatTensor)\n",
    "            labels.resize_(labels.shape[0], 1)\n",
    "            \n",
    "            output = model.forward(features)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            if steps % print_every == 0:\n",
    "                print('Epoch: {}/{}..'.format(epoch+1, epochs),\n",
    "                      'Training loss: {:.3f}'.format(epoch_loss/print_every))\n",
    "\n",
    "                epoch_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.RMSprop(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20631, 24])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureset = torch.from_numpy(scaled_raw[:,2:])\n",
    "labelset = torch.from_numpy(raw.target)\n",
    "\n",
    "featureloader = torch.utils.data.DataLoader(featureset, batch_size=32)\n",
    "labelloader = torch.utils.data.DataLoader(labelset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100.. Training loss: 16909.851\n",
      "Epoch: 1/100.. Training loss: 11363.752\n",
      "Epoch: 1/100.. Training loss: 11295.594\n",
      "Epoch: 1/100.. Training loss: 9072.098\n",
      "Epoch: 1/100.. Training loss: 7857.110\n",
      "Epoch: 1/100.. Training loss: 10524.231\n",
      "Epoch: 1/100.. Training loss: 10896.970\n",
      "Epoch: 1/100.. Training loss: 8121.215\n",
      "Epoch: 1/100.. Training loss: 15541.844\n",
      "Epoch: 1/100.. Training loss: 7254.205\n",
      "Epoch: 1/100.. Training loss: 11748.486\n",
      "Epoch: 1/100.. Training loss: 10489.483\n",
      "Epoch: 2/100.. Training loss: 10459.455\n",
      "Epoch: 2/100.. Training loss: 8864.036\n",
      "Epoch: 2/100.. Training loss: 6347.583\n",
      "Epoch: 2/100.. Training loss: 5402.298\n",
      "Epoch: 2/100.. Training loss: 4103.582\n",
      "Epoch: 2/100.. Training loss: 3587.880\n",
      "Epoch: 2/100.. Training loss: 5480.083\n",
      "Epoch: 2/100.. Training loss: 5649.137\n",
      "Epoch: 2/100.. Training loss: 4765.783\n",
      "Epoch: 2/100.. Training loss: 9859.831\n",
      "Epoch: 2/100.. Training loss: 3888.400\n",
      "Epoch: 2/100.. Training loss: 6863.742\n",
      "Epoch: 2/100.. Training loss: 7650.365\n",
      "Epoch: 3/100.. Training loss: 6694.547\n",
      "Epoch: 3/100.. Training loss: 3618.469\n",
      "Epoch: 3/100.. Training loss: 3820.834\n",
      "Epoch: 3/100.. Training loss: 2438.839\n",
      "Epoch: 3/100.. Training loss: 2176.191\n",
      "Epoch: 3/100.. Training loss: 1777.251\n",
      "Epoch: 3/100.. Training loss: 2855.798\n",
      "Epoch: 3/100.. Training loss: 3244.987\n",
      "Epoch: 3/100.. Training loss: 2803.649\n",
      "Epoch: 3/100.. Training loss: 6827.005\n",
      "Epoch: 3/100.. Training loss: 2234.674\n",
      "Epoch: 3/100.. Training loss: 4276.498\n",
      "Epoch: 3/100.. Training loss: 5111.857\n",
      "Epoch: 4/100.. Training loss: 4424.376\n",
      "Epoch: 4/100.. Training loss: 2351.273\n",
      "Epoch: 4/100.. Training loss: 2244.460\n",
      "Epoch: 4/100.. Training loss: 1452.488\n",
      "Epoch: 4/100.. Training loss: 1343.101\n",
      "Epoch: 4/100.. Training loss: 1178.510\n",
      "Epoch: 4/100.. Training loss: 1807.559\n",
      "Epoch: 4/100.. Training loss: 2441.720\n",
      "Epoch: 4/100.. Training loss: 3275.815\n",
      "Epoch: 4/100.. Training loss: 3811.759\n",
      "Epoch: 4/100.. Training loss: 1633.984\n",
      "Epoch: 4/100.. Training loss: 3085.180\n",
      "Epoch: 4/100.. Training loss: 5635.505\n",
      "Epoch: 5/100.. Training loss: 1812.107\n",
      "Epoch: 5/100.. Training loss: 1856.755\n",
      "Epoch: 5/100.. Training loss: 1476.891\n",
      "Epoch: 5/100.. Training loss: 1146.507\n",
      "Epoch: 5/100.. Training loss: 1174.562\n",
      "Epoch: 5/100.. Training loss: 1008.927\n",
      "Epoch: 5/100.. Training loss: 1383.415\n",
      "Epoch: 5/100.. Training loss: 2079.087\n",
      "Epoch: 5/100.. Training loss: 2821.267\n",
      "Epoch: 5/100.. Training loss: 3551.768\n",
      "Epoch: 5/100.. Training loss: 1673.667\n",
      "Epoch: 5/100.. Training loss: 2114.397\n",
      "Epoch: 5/100.. Training loss: 4873.260\n",
      "Epoch: 6/100.. Training loss: 1612.834\n",
      "Epoch: 6/100.. Training loss: 1532.586\n",
      "Epoch: 6/100.. Training loss: 1677.913\n",
      "Epoch: 6/100.. Training loss: 962.169\n",
      "Epoch: 6/100.. Training loss: 1047.547\n",
      "Epoch: 6/100.. Training loss: 1060.365\n",
      "Epoch: 6/100.. Training loss: 1087.117\n",
      "Epoch: 6/100.. Training loss: 1985.378\n",
      "Epoch: 6/100.. Training loss: 2629.428\n",
      "Epoch: 6/100.. Training loss: 3304.982\n",
      "Epoch: 6/100.. Training loss: 1666.522\n",
      "Epoch: 6/100.. Training loss: 2051.289\n",
      "Epoch: 6/100.. Training loss: 4208.619\n",
      "Epoch: 7/100.. Training loss: 1965.728\n",
      "Epoch: 7/100.. Training loss: 1005.598\n",
      "Epoch: 7/100.. Training loss: 1577.717\n",
      "Epoch: 7/100.. Training loss: 954.999\n",
      "Epoch: 7/100.. Training loss: 1088.623\n",
      "Epoch: 7/100.. Training loss: 1172.057\n",
      "Epoch: 7/100.. Training loss: 869.400\n",
      "Epoch: 7/100.. Training loss: 2138.653\n",
      "Epoch: 7/100.. Training loss: 4246.902\n",
      "Epoch: 7/100.. Training loss: 1342.626\n",
      "Epoch: 7/100.. Training loss: 2083.181\n",
      "Epoch: 7/100.. Training loss: 2407.307\n",
      "Epoch: 7/100.. Training loss: 3236.369\n",
      "Epoch: 8/100.. Training loss: 1766.634\n",
      "Epoch: 8/100.. Training loss: 942.654\n",
      "Epoch: 8/100.. Training loss: 1573.634\n",
      "Epoch: 8/100.. Training loss: 1018.409\n",
      "Epoch: 8/100.. Training loss: 970.449\n",
      "Epoch: 8/100.. Training loss: 1235.786\n",
      "Epoch: 8/100.. Training loss: 812.198\n",
      "Epoch: 8/100.. Training loss: 2090.720\n",
      "Epoch: 8/100.. Training loss: 4426.072\n",
      "Epoch: 8/100.. Training loss: 1385.353\n",
      "Epoch: 8/100.. Training loss: 1922.163\n",
      "Epoch: 8/100.. Training loss: 2930.838\n",
      "Epoch: 8/100.. Training loss: 2661.733\n",
      "Epoch: 9/100.. Training loss: 1654.677\n",
      "Epoch: 9/100.. Training loss: 912.274\n",
      "Epoch: 9/100.. Training loss: 1585.803\n",
      "Epoch: 9/100.. Training loss: 1090.578\n",
      "Epoch: 9/100.. Training loss: 1003.712\n",
      "Epoch: 9/100.. Training loss: 1034.397\n",
      "Epoch: 9/100.. Training loss: 727.278\n",
      "Epoch: 9/100.. Training loss: 2181.781\n",
      "Epoch: 9/100.. Training loss: 4294.376\n",
      "Epoch: 9/100.. Training loss: 1372.848\n",
      "Epoch: 9/100.. Training loss: 1894.526\n",
      "Epoch: 9/100.. Training loss: 2745.826\n",
      "Epoch: 9/100.. Training loss: 2838.611\n",
      "Epoch: 10/100.. Training loss: 1650.514\n",
      "Epoch: 10/100.. Training loss: 851.943\n",
      "Epoch: 10/100.. Training loss: 1456.657\n",
      "Epoch: 10/100.. Training loss: 1197.165\n",
      "Epoch: 10/100.. Training loss: 930.898\n",
      "Epoch: 10/100.. Training loss: 992.344\n",
      "Epoch: 10/100.. Training loss: 1206.650\n",
      "Epoch: 10/100.. Training loss: 2223.795\n",
      "Epoch: 10/100.. Training loss: 4003.596\n",
      "Epoch: 10/100.. Training loss: 1419.949\n",
      "Epoch: 10/100.. Training loss: 2337.902\n",
      "Epoch: 10/100.. Training loss: 2683.353\n",
      "Epoch: 10/100.. Training loss: 2374.601\n",
      "Epoch: 11/100.. Training loss: 1622.547\n",
      "Epoch: 11/100.. Training loss: 1163.466\n",
      "Epoch: 11/100.. Training loss: 1204.374\n",
      "Epoch: 11/100.. Training loss: 1169.486\n",
      "Epoch: 11/100.. Training loss: 945.815\n",
      "Epoch: 11/100.. Training loss: 1047.138\n",
      "Epoch: 11/100.. Training loss: 1354.405\n",
      "Epoch: 11/100.. Training loss: 2027.596\n",
      "Epoch: 11/100.. Training loss: 3999.504\n",
      "Epoch: 11/100.. Training loss: 1520.123\n",
      "Epoch: 11/100.. Training loss: 2155.352\n",
      "Epoch: 11/100.. Training loss: 2733.840\n",
      "Epoch: 12/100.. Training loss: 2490.186\n",
      "Epoch: 12/100.. Training loss: 1361.767\n",
      "Epoch: 12/100.. Training loss: 1423.741\n",
      "Epoch: 12/100.. Training loss: 1155.218\n",
      "Epoch: 12/100.. Training loss: 1000.795\n",
      "Epoch: 12/100.. Training loss: 893.009\n",
      "Epoch: 12/100.. Training loss: 1061.931\n",
      "Epoch: 12/100.. Training loss: 1615.167\n",
      "Epoch: 12/100.. Training loss: 2082.220\n",
      "Epoch: 12/100.. Training loss: 3704.213\n",
      "Epoch: 12/100.. Training loss: 1437.701\n",
      "Epoch: 12/100.. Training loss: 2112.567\n",
      "Epoch: 12/100.. Training loss: 3013.543\n",
      "Epoch: 13/100.. Training loss: 2537.291\n",
      "Epoch: 13/100.. Training loss: 1055.855\n",
      "Epoch: 13/100.. Training loss: 1375.083\n",
      "Epoch: 13/100.. Training loss: 1097.697\n",
      "Epoch: 13/100.. Training loss: 1149.935\n",
      "Epoch: 13/100.. Training loss: 817.629\n",
      "Epoch: 13/100.. Training loss: 1022.466\n",
      "Epoch: 13/100.. Training loss: 1811.063\n",
      "Epoch: 13/100.. Training loss: 1988.031\n",
      "Epoch: 13/100.. Training loss: 3799.720\n",
      "Epoch: 13/100.. Training loss: 1336.325\n",
      "Epoch: 13/100.. Training loss: 2093.290\n",
      "Epoch: 13/100.. Training loss: 2976.588\n",
      "Epoch: 14/100.. Training loss: 2511.053\n",
      "Epoch: 14/100.. Training loss: 1171.111\n",
      "Epoch: 14/100.. Training loss: 1259.894\n",
      "Epoch: 14/100.. Training loss: 1096.712\n",
      "Epoch: 14/100.. Training loss: 1107.182\n",
      "Epoch: 14/100.. Training loss: 975.629\n",
      "Epoch: 14/100.. Training loss: 824.244\n",
      "Epoch: 14/100.. Training loss: 1849.573\n",
      "Epoch: 14/100.. Training loss: 3060.407\n",
      "Epoch: 14/100.. Training loss: 2840.301\n",
      "Epoch: 14/100.. Training loss: 1191.605\n",
      "Epoch: 14/100.. Training loss: 2042.982\n",
      "Epoch: 14/100.. Training loss: 3922.837\n",
      "Epoch: 15/100.. Training loss: 1576.769\n",
      "Epoch: 15/100.. Training loss: 1107.673\n",
      "Epoch: 15/100.. Training loss: 1270.727\n",
      "Epoch: 15/100.. Training loss: 1062.814\n",
      "Epoch: 15/100.. Training loss: 1116.606\n",
      "Epoch: 15/100.. Training loss: 972.086\n",
      "Epoch: 15/100.. Training loss: 862.551\n",
      "Epoch: 15/100.. Training loss: 1922.466\n",
      "Epoch: 15/100.. Training loss: 2995.203\n",
      "Epoch: 15/100.. Training loss: 3157.030\n",
      "Epoch: 15/100.. Training loss: 1383.695\n",
      "Epoch: 15/100.. Training loss: 1905.467\n",
      "Epoch: 15/100.. Training loss: 3728.044\n",
      "Epoch: 16/100.. Training loss: 1688.253\n",
      "Epoch: 16/100.. Training loss: 1096.435\n",
      "Epoch: 16/100.. Training loss: 1506.017\n",
      "Epoch: 16/100.. Training loss: 981.891\n",
      "Epoch: 16/100.. Training loss: 1009.054\n",
      "Epoch: 16/100.. Training loss: 1051.986\n",
      "Epoch: 16/100.. Training loss: 803.269\n",
      "Epoch: 16/100.. Training loss: 1954.100\n",
      "Epoch: 16/100.. Training loss: 2989.073\n",
      "Epoch: 16/100.. Training loss: 2925.776\n",
      "Epoch: 16/100.. Training loss: 1488.823\n",
      "Epoch: 16/100.. Training loss: 2140.478\n",
      "Epoch: 16/100.. Training loss: 3383.993\n",
      "Epoch: 17/100.. Training loss: 1899.407\n",
      "Epoch: 17/100.. Training loss: 862.630\n",
      "Epoch: 17/100.. Training loss: 1403.849\n",
      "Epoch: 17/100.. Training loss: 1016.682\n",
      "Epoch: 17/100.. Training loss: 1080.094\n",
      "Epoch: 17/100.. Training loss: 1071.091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/100.. Training loss: 697.724\n",
      "Epoch: 17/100.. Training loss: 2209.738\n",
      "Epoch: 17/100.. Training loss: 4424.556\n",
      "Epoch: 17/100.. Training loss: 1346.405\n",
      "Epoch: 17/100.. Training loss: 1900.212\n",
      "Epoch: 17/100.. Training loss: 2403.243\n",
      "Epoch: 17/100.. Training loss: 2875.736\n",
      "Epoch: 18/100.. Training loss: 1693.934\n",
      "Epoch: 18/100.. Training loss: 880.126\n",
      "Epoch: 18/100.. Training loss: 1496.069\n",
      "Epoch: 18/100.. Training loss: 1033.338\n",
      "Epoch: 18/100.. Training loss: 967.520\n",
      "Epoch: 18/100.. Training loss: 1112.501\n",
      "Epoch: 18/100.. Training loss: 694.805\n",
      "Epoch: 18/100.. Training loss: 2179.571\n",
      "Epoch: 18/100.. Training loss: 4463.037\n",
      "Epoch: 18/100.. Training loss: 1305.730\n",
      "Epoch: 18/100.. Training loss: 1819.365\n",
      "Epoch: 18/100.. Training loss: 2818.057\n",
      "Epoch: 18/100.. Training loss: 2450.047\n",
      "Epoch: 19/100.. Training loss: 1518.818\n",
      "Epoch: 19/100.. Training loss: 863.744\n",
      "Epoch: 19/100.. Training loss: 1531.667\n",
      "Epoch: 19/100.. Training loss: 1099.628\n",
      "Epoch: 19/100.. Training loss: 1016.535\n",
      "Epoch: 19/100.. Training loss: 992.207\n",
      "Epoch: 19/100.. Training loss: 671.651\n",
      "Epoch: 19/100.. Training loss: 2177.663\n",
      "Epoch: 19/100.. Training loss: 4839.906\n",
      "Epoch: 19/100.. Training loss: 1361.161\n",
      "Epoch: 19/100.. Training loss: 1844.381\n",
      "Epoch: 19/100.. Training loss: 2782.194\n",
      "Epoch: 19/100.. Training loss: 2597.972\n",
      "Epoch: 20/100.. Training loss: 1505.441\n",
      "Epoch: 20/100.. Training loss: 883.771\n",
      "Epoch: 20/100.. Training loss: 1322.486\n",
      "Epoch: 20/100.. Training loss: 1248.127\n",
      "Epoch: 20/100.. Training loss: 932.004\n",
      "Epoch: 20/100.. Training loss: 963.743\n",
      "Epoch: 20/100.. Training loss: 1137.353\n",
      "Epoch: 20/100.. Training loss: 2252.109\n",
      "Epoch: 20/100.. Training loss: 4008.003\n",
      "Epoch: 20/100.. Training loss: 1321.070\n",
      "Epoch: 20/100.. Training loss: 2294.060\n",
      "Epoch: 20/100.. Training loss: 2713.483\n",
      "Epoch: 20/100.. Training loss: 2391.590\n",
      "Epoch: 21/100.. Training loss: 1493.685\n",
      "Epoch: 21/100.. Training loss: 1196.143\n",
      "Epoch: 21/100.. Training loss: 1121.252\n",
      "Epoch: 21/100.. Training loss: 1170.601\n",
      "Epoch: 21/100.. Training loss: 974.070\n",
      "Epoch: 21/100.. Training loss: 1010.226\n",
      "Epoch: 21/100.. Training loss: 1299.052\n",
      "Epoch: 21/100.. Training loss: 2065.500\n",
      "Epoch: 21/100.. Training loss: 4022.864\n",
      "Epoch: 21/100.. Training loss: 1390.330\n",
      "Epoch: 21/100.. Training loss: 2147.260\n",
      "Epoch: 21/100.. Training loss: 2752.494\n",
      "Epoch: 22/100.. Training loss: 2334.820\n",
      "Epoch: 22/100.. Training loss: 1195.524\n",
      "Epoch: 22/100.. Training loss: 1426.014\n",
      "Epoch: 22/100.. Training loss: 1143.613\n",
      "Epoch: 22/100.. Training loss: 1105.811\n",
      "Epoch: 22/100.. Training loss: 848.331\n",
      "Epoch: 22/100.. Training loss: 1023.403\n",
      "Epoch: 22/100.. Training loss: 1571.788\n",
      "Epoch: 22/100.. Training loss: 2121.955\n",
      "Epoch: 22/100.. Training loss: 3595.094\n",
      "Epoch: 22/100.. Training loss: 1390.839\n",
      "Epoch: 22/100.. Training loss: 2067.065\n",
      "Epoch: 22/100.. Training loss: 2987.680\n",
      "Epoch: 23/100.. Training loss: 2374.427\n",
      "Epoch: 23/100.. Training loss: 1069.991\n",
      "Epoch: 23/100.. Training loss: 1320.768\n",
      "Epoch: 23/100.. Training loss: 1117.725\n",
      "Epoch: 23/100.. Training loss: 1142.574\n",
      "Epoch: 23/100.. Training loss: 798.764\n",
      "Epoch: 23/100.. Training loss: 936.025\n",
      "Epoch: 23/100.. Training loss: 1795.366\n",
      "Epoch: 23/100.. Training loss: 2081.056\n",
      "Epoch: 23/100.. Training loss: 3623.701\n",
      "Epoch: 23/100.. Training loss: 1369.033\n",
      "Epoch: 23/100.. Training loss: 2001.257\n",
      "Epoch: 23/100.. Training loss: 3054.429\n",
      "Epoch: 24/100.. Training loss: 2459.512\n",
      "Epoch: 24/100.. Training loss: 1102.755\n",
      "Epoch: 24/100.. Training loss: 1303.442\n",
      "Epoch: 24/100.. Training loss: 1115.079\n",
      "Epoch: 24/100.. Training loss: 1147.462\n",
      "Epoch: 24/100.. Training loss: 1043.264\n",
      "Epoch: 24/100.. Training loss: 804.749\n",
      "Epoch: 24/100.. Training loss: 1849.107\n",
      "Epoch: 24/100.. Training loss: 3130.926\n",
      "Epoch: 24/100.. Training loss: 2936.655\n",
      "Epoch: 24/100.. Training loss: 1216.976\n",
      "Epoch: 24/100.. Training loss: 1997.463\n",
      "Epoch: 24/100.. Training loss: 3818.047\n",
      "Epoch: 25/100.. Training loss: 1716.083\n",
      "Epoch: 25/100.. Training loss: 1045.147\n",
      "Epoch: 25/100.. Training loss: 1232.499\n",
      "Epoch: 25/100.. Training loss: 1050.768\n",
      "Epoch: 25/100.. Training loss: 1184.198\n",
      "Epoch: 25/100.. Training loss: 972.496\n",
      "Epoch: 25/100.. Training loss: 776.667\n",
      "Epoch: 25/100.. Training loss: 1940.607\n",
      "Epoch: 25/100.. Training loss: 2998.189\n",
      "Epoch: 25/100.. Training loss: 2941.863\n",
      "Epoch: 25/100.. Training loss: 1397.703\n",
      "Epoch: 25/100.. Training loss: 1856.192\n",
      "Epoch: 25/100.. Training loss: 3543.016\n",
      "Epoch: 26/100.. Training loss: 1752.458\n",
      "Epoch: 26/100.. Training loss: 1057.337\n",
      "Epoch: 26/100.. Training loss: 1518.736\n",
      "Epoch: 26/100.. Training loss: 1012.562\n",
      "Epoch: 26/100.. Training loss: 978.340\n",
      "Epoch: 26/100.. Training loss: 1106.215\n",
      "Epoch: 26/100.. Training loss: 766.670\n",
      "Epoch: 26/100.. Training loss: 1956.352\n",
      "Epoch: 26/100.. Training loss: 3022.280\n",
      "Epoch: 26/100.. Training loss: 3201.863\n",
      "Epoch: 26/100.. Training loss: 1383.268\n",
      "Epoch: 26/100.. Training loss: 2207.543\n",
      "Epoch: 26/100.. Training loss: 3300.044\n",
      "Epoch: 27/100.. Training loss: 1794.568\n",
      "Epoch: 27/100.. Training loss: 901.108\n",
      "Epoch: 27/100.. Training loss: 1479.502\n",
      "Epoch: 27/100.. Training loss: 976.960\n",
      "Epoch: 27/100.. Training loss: 1061.124\n",
      "Epoch: 27/100.. Training loss: 1113.338\n",
      "Epoch: 27/100.. Training loss: 681.526\n",
      "Epoch: 27/100.. Training loss: 2143.915\n",
      "Epoch: 27/100.. Training loss: 4620.593\n",
      "Epoch: 27/100.. Training loss: 1214.588\n",
      "Epoch: 27/100.. Training loss: 1881.813\n",
      "Epoch: 27/100.. Training loss: 2417.062\n",
      "Epoch: 27/100.. Training loss: 2718.869\n",
      "Epoch: 28/100.. Training loss: 1693.964\n",
      "Epoch: 28/100.. Training loss: 876.781\n",
      "Epoch: 28/100.. Training loss: 1464.451\n",
      "Epoch: 28/100.. Training loss: 1042.089\n",
      "Epoch: 28/100.. Training loss: 946.867\n",
      "Epoch: 28/100.. Training loss: 1153.731\n",
      "Epoch: 28/100.. Training loss: 661.828\n",
      "Epoch: 28/100.. Training loss: 2150.627\n",
      "Epoch: 28/100.. Training loss: 4749.584\n",
      "Epoch: 28/100.. Training loss: 1241.395\n",
      "Epoch: 28/100.. Training loss: 1823.431\n",
      "Epoch: 28/100.. Training loss: 2958.208\n",
      "Epoch: 28/100.. Training loss: 2510.695\n",
      "Epoch: 29/100.. Training loss: 1428.865\n",
      "Epoch: 29/100.. Training loss: 862.620\n",
      "Epoch: 29/100.. Training loss: 1541.572\n",
      "Epoch: 29/100.. Training loss: 1164.980\n",
      "Epoch: 29/100.. Training loss: 955.589\n",
      "Epoch: 29/100.. Training loss: 1009.462\n",
      "Epoch: 29/100.. Training loss: 620.408\n",
      "Epoch: 29/100.. Training loss: 2184.953\n",
      "Epoch: 29/100.. Training loss: 4777.202\n",
      "Epoch: 29/100.. Training loss: 1334.206\n",
      "Epoch: 29/100.. Training loss: 1822.881\n",
      "Epoch: 29/100.. Training loss: 2792.452\n",
      "Epoch: 29/100.. Training loss: 2718.361\n",
      "Epoch: 30/100.. Training loss: 1438.479\n",
      "Epoch: 30/100.. Training loss: 861.199\n",
      "Epoch: 30/100.. Training loss: 1422.129\n",
      "Epoch: 30/100.. Training loss: 1251.519\n",
      "Epoch: 30/100.. Training loss: 902.539\n",
      "Epoch: 30/100.. Training loss: 956.069\n",
      "Epoch: 30/100.. Training loss: 1108.199\n",
      "Epoch: 30/100.. Training loss: 2245.875\n",
      "Epoch: 30/100.. Training loss: 4093.608\n",
      "Epoch: 30/100.. Training loss: 1271.914\n",
      "Epoch: 30/100.. Training loss: 2253.521\n",
      "Epoch: 30/100.. Training loss: 2682.892\n",
      "Epoch: 30/100.. Training loss: 2210.524\n",
      "Epoch: 31/100.. Training loss: 1430.949\n",
      "Epoch: 31/100.. Training loss: 1222.261\n",
      "Epoch: 31/100.. Training loss: 1159.036\n",
      "Epoch: 31/100.. Training loss: 1242.089\n",
      "Epoch: 31/100.. Training loss: 919.335\n",
      "Epoch: 31/100.. Training loss: 1014.266\n",
      "Epoch: 31/100.. Training loss: 1273.286\n",
      "Epoch: 31/100.. Training loss: 1996.581\n",
      "Epoch: 31/100.. Training loss: 4113.118\n",
      "Epoch: 31/100.. Training loss: 1364.995\n",
      "Epoch: 31/100.. Training loss: 2114.798\n",
      "Epoch: 31/100.. Training loss: 2782.840\n",
      "Epoch: 32/100.. Training loss: 2426.876\n",
      "Epoch: 32/100.. Training loss: 1156.628\n",
      "Epoch: 32/100.. Training loss: 1444.433\n",
      "Epoch: 32/100.. Training loss: 1139.069\n",
      "Epoch: 32/100.. Training loss: 1125.936\n",
      "Epoch: 32/100.. Training loss: 823.389\n",
      "Epoch: 32/100.. Training loss: 1013.586\n",
      "Epoch: 32/100.. Training loss: 1605.617\n",
      "Epoch: 32/100.. Training loss: 2113.076\n",
      "Epoch: 32/100.. Training loss: 3790.292\n",
      "Epoch: 32/100.. Training loss: 1358.379\n",
      "Epoch: 32/100.. Training loss: 2079.967\n",
      "Epoch: 32/100.. Training loss: 2904.886\n",
      "Epoch: 33/100.. Training loss: 2356.741\n",
      "Epoch: 33/100.. Training loss: 1013.771\n",
      "Epoch: 33/100.. Training loss: 1333.486\n",
      "Epoch: 33/100.. Training loss: 1134.440\n",
      "Epoch: 33/100.. Training loss: 1209.529\n",
      "Epoch: 33/100.. Training loss: 778.253\n",
      "Epoch: 33/100.. Training loss: 985.943\n",
      "Epoch: 33/100.. Training loss: 1759.929\n",
      "Epoch: 33/100.. Training loss: 1966.878\n",
      "Epoch: 33/100.. Training loss: 4043.656\n",
      "Epoch: 33/100.. Training loss: 1301.212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33/100.. Training loss: 2081.169\n",
      "Epoch: 33/100.. Training loss: 3021.081\n",
      "Epoch: 34/100.. Training loss: 2390.568\n",
      "Epoch: 34/100.. Training loss: 1137.470\n",
      "Epoch: 34/100.. Training loss: 1263.419\n",
      "Epoch: 34/100.. Training loss: 1046.116\n",
      "Epoch: 34/100.. Training loss: 1166.257\n",
      "Epoch: 34/100.. Training loss: 1000.552\n",
      "Epoch: 34/100.. Training loss: 786.307\n",
      "Epoch: 34/100.. Training loss: 1896.705\n",
      "Epoch: 34/100.. Training loss: 3030.295\n",
      "Epoch: 34/100.. Training loss: 2858.054\n",
      "Epoch: 34/100.. Training loss: 1176.832\n",
      "Epoch: 34/100.. Training loss: 2072.861\n",
      "Epoch: 34/100.. Training loss: 4026.846\n",
      "Epoch: 35/100.. Training loss: 1597.164\n",
      "Epoch: 35/100.. Training loss: 999.055\n",
      "Epoch: 35/100.. Training loss: 1279.395\n",
      "Epoch: 35/100.. Training loss: 1058.445\n",
      "Epoch: 35/100.. Training loss: 1176.633\n",
      "Epoch: 35/100.. Training loss: 987.451\n",
      "Epoch: 35/100.. Training loss: 849.142\n",
      "Epoch: 35/100.. Training loss: 1891.187\n",
      "Epoch: 35/100.. Training loss: 3008.017\n",
      "Epoch: 35/100.. Training loss: 3058.711\n",
      "Epoch: 35/100.. Training loss: 1393.779\n",
      "Epoch: 35/100.. Training loss: 1890.431\n",
      "Epoch: 35/100.. Training loss: 3688.594\n",
      "Epoch: 36/100.. Training loss: 1799.601\n",
      "Epoch: 36/100.. Training loss: 979.164\n",
      "Epoch: 36/100.. Training loss: 1468.866\n",
      "Epoch: 36/100.. Training loss: 1029.444\n",
      "Epoch: 36/100.. Training loss: 993.256\n",
      "Epoch: 36/100.. Training loss: 1047.363\n",
      "Epoch: 36/100.. Training loss: 804.867\n",
      "Epoch: 36/100.. Training loss: 1917.329\n",
      "Epoch: 36/100.. Training loss: 2983.150\n",
      "Epoch: 36/100.. Training loss: 3054.440\n",
      "Epoch: 36/100.. Training loss: 1412.875\n",
      "Epoch: 36/100.. Training loss: 2203.862\n",
      "Epoch: 36/100.. Training loss: 3429.959\n",
      "Epoch: 37/100.. Training loss: 1773.604\n",
      "Epoch: 37/100.. Training loss: 837.553\n",
      "Epoch: 37/100.. Training loss: 1397.396\n",
      "Epoch: 37/100.. Training loss: 1085.858\n",
      "Epoch: 37/100.. Training loss: 1062.659\n",
      "Epoch: 37/100.. Training loss: 1103.621\n",
      "Epoch: 37/100.. Training loss: 675.942\n",
      "Epoch: 37/100.. Training loss: 2095.352\n",
      "Epoch: 37/100.. Training loss: 4448.968\n",
      "Epoch: 37/100.. Training loss: 1228.662\n",
      "Epoch: 37/100.. Training loss: 1850.122\n",
      "Epoch: 37/100.. Training loss: 2419.825\n",
      "Epoch: 37/100.. Training loss: 2952.398\n",
      "Epoch: 38/100.. Training loss: 1608.355\n",
      "Epoch: 38/100.. Training loss: 876.323\n",
      "Epoch: 38/100.. Training loss: 1451.350\n",
      "Epoch: 38/100.. Training loss: 1139.363\n",
      "Epoch: 38/100.. Training loss: 968.490\n",
      "Epoch: 38/100.. Training loss: 1125.501\n",
      "Epoch: 38/100.. Training loss: 686.550\n",
      "Epoch: 38/100.. Training loss: 2241.028\n",
      "Epoch: 38/100.. Training loss: 4681.162\n",
      "Epoch: 38/100.. Training loss: 1151.887\n",
      "Epoch: 38/100.. Training loss: 1813.688\n",
      "Epoch: 38/100.. Training loss: 2932.515\n",
      "Epoch: 38/100.. Training loss: 2469.445\n",
      "Epoch: 39/100.. Training loss: 1463.625\n",
      "Epoch: 39/100.. Training loss: 852.307\n",
      "Epoch: 39/100.. Training loss: 1563.982\n",
      "Epoch: 39/100.. Training loss: 1170.313\n",
      "Epoch: 39/100.. Training loss: 1041.357\n",
      "Epoch: 39/100.. Training loss: 990.489\n",
      "Epoch: 39/100.. Training loss: 600.110\n",
      "Epoch: 39/100.. Training loss: 2164.499\n",
      "Epoch: 39/100.. Training loss: 4567.209\n",
      "Epoch: 39/100.. Training loss: 1287.976\n",
      "Epoch: 39/100.. Training loss: 1804.717\n",
      "Epoch: 39/100.. Training loss: 2779.420\n",
      "Epoch: 39/100.. Training loss: 2587.286\n",
      "Epoch: 40/100.. Training loss: 1409.420\n",
      "Epoch: 40/100.. Training loss: 911.720\n",
      "Epoch: 40/100.. Training loss: 1276.589\n",
      "Epoch: 40/100.. Training loss: 1308.801\n",
      "Epoch: 40/100.. Training loss: 932.551\n",
      "Epoch: 40/100.. Training loss: 911.336\n",
      "Epoch: 40/100.. Training loss: 1128.996\n",
      "Epoch: 40/100.. Training loss: 2225.114\n",
      "Epoch: 40/100.. Training loss: 4179.708\n",
      "Epoch: 40/100.. Training loss: 1246.086\n",
      "Epoch: 40/100.. Training loss: 2303.334\n",
      "Epoch: 40/100.. Training loss: 2811.650\n",
      "Epoch: 40/100.. Training loss: 2321.291\n",
      "Epoch: 41/100.. Training loss: 1400.322\n",
      "Epoch: 41/100.. Training loss: 1180.403\n",
      "Epoch: 41/100.. Training loss: 1118.123\n",
      "Epoch: 41/100.. Training loss: 1250.776\n",
      "Epoch: 41/100.. Training loss: 883.485\n",
      "Epoch: 41/100.. Training loss: 999.613\n",
      "Epoch: 41/100.. Training loss: 1281.682\n",
      "Epoch: 41/100.. Training loss: 2004.690\n",
      "Epoch: 41/100.. Training loss: 4111.454\n",
      "Epoch: 41/100.. Training loss: 1513.278\n",
      "Epoch: 41/100.. Training loss: 2107.225\n",
      "Epoch: 41/100.. Training loss: 2795.564\n",
      "Epoch: 42/100.. Training loss: 2409.636\n",
      "Epoch: 42/100.. Training loss: 1172.625\n",
      "Epoch: 42/100.. Training loss: 1406.724\n",
      "Epoch: 42/100.. Training loss: 1117.333\n",
      "Epoch: 42/100.. Training loss: 1079.612\n",
      "Epoch: 42/100.. Training loss: 870.399\n",
      "Epoch: 42/100.. Training loss: 1013.942\n",
      "Epoch: 42/100.. Training loss: 1595.563\n",
      "Epoch: 42/100.. Training loss: 2045.442\n",
      "Epoch: 42/100.. Training loss: 3978.621\n",
      "Epoch: 42/100.. Training loss: 1268.751\n",
      "Epoch: 42/100.. Training loss: 2094.508\n",
      "Epoch: 42/100.. Training loss: 3187.789\n",
      "Epoch: 43/100.. Training loss: 2434.497\n",
      "Epoch: 43/100.. Training loss: 1095.907\n",
      "Epoch: 43/100.. Training loss: 1350.461\n",
      "Epoch: 43/100.. Training loss: 1120.494\n",
      "Epoch: 43/100.. Training loss: 1236.783\n",
      "Epoch: 43/100.. Training loss: 777.230\n",
      "Epoch: 43/100.. Training loss: 964.173\n",
      "Epoch: 43/100.. Training loss: 1756.498\n",
      "Epoch: 43/100.. Training loss: 1944.567\n",
      "Epoch: 43/100.. Training loss: 3924.322\n",
      "Epoch: 43/100.. Training loss: 1239.460\n",
      "Epoch: 43/100.. Training loss: 2013.311\n",
      "Epoch: 43/100.. Training loss: 2975.306\n",
      "Epoch: 44/100.. Training loss: 2360.513\n",
      "Epoch: 44/100.. Training loss: 1051.734\n",
      "Epoch: 44/100.. Training loss: 1312.508\n",
      "Epoch: 44/100.. Training loss: 1098.672\n",
      "Epoch: 44/100.. Training loss: 1190.735\n",
      "Epoch: 44/100.. Training loss: 949.747\n",
      "Epoch: 44/100.. Training loss: 817.579\n",
      "Epoch: 44/100.. Training loss: 1832.604\n",
      "Epoch: 44/100.. Training loss: 3045.675\n",
      "Epoch: 44/100.. Training loss: 2932.394\n",
      "Epoch: 44/100.. Training loss: 1140.969\n",
      "Epoch: 44/100.. Training loss: 2023.866\n",
      "Epoch: 44/100.. Training loss: 3857.395\n",
      "Epoch: 45/100.. Training loss: 1672.544\n",
      "Epoch: 45/100.. Training loss: 1004.740\n",
      "Epoch: 45/100.. Training loss: 1260.452\n",
      "Epoch: 45/100.. Training loss: 1069.543\n",
      "Epoch: 45/100.. Training loss: 1228.947\n",
      "Epoch: 45/100.. Training loss: 977.434\n",
      "Epoch: 45/100.. Training loss: 800.616\n",
      "Epoch: 45/100.. Training loss: 1884.611\n",
      "Epoch: 45/100.. Training loss: 2968.772\n",
      "Epoch: 45/100.. Training loss: 3109.560\n",
      "Epoch: 45/100.. Training loss: 1328.092\n",
      "Epoch: 45/100.. Training loss: 1879.536\n",
      "Epoch: 45/100.. Training loss: 3762.000\n",
      "Epoch: 46/100.. Training loss: 1725.662\n",
      "Epoch: 46/100.. Training loss: 1181.583\n",
      "Epoch: 46/100.. Training loss: 1468.699\n",
      "Epoch: 46/100.. Training loss: 1083.812\n",
      "Epoch: 46/100.. Training loss: 998.002\n",
      "Epoch: 46/100.. Training loss: 1000.972\n",
      "Epoch: 46/100.. Training loss: 971.013\n",
      "Epoch: 46/100.. Training loss: 1877.170\n",
      "Epoch: 46/100.. Training loss: 3011.929\n",
      "Epoch: 46/100.. Training loss: 3258.962\n",
      "Epoch: 46/100.. Training loss: 1589.494\n",
      "Epoch: 46/100.. Training loss: 2184.529\n",
      "Epoch: 46/100.. Training loss: 3233.679\n",
      "Epoch: 47/100.. Training loss: 1827.776\n",
      "Epoch: 47/100.. Training loss: 927.960\n",
      "Epoch: 47/100.. Training loss: 1470.353\n",
      "Epoch: 47/100.. Training loss: 1056.208\n",
      "Epoch: 47/100.. Training loss: 1133.938\n",
      "Epoch: 47/100.. Training loss: 1100.471\n",
      "Epoch: 47/100.. Training loss: 710.543\n",
      "Epoch: 47/100.. Training loss: 2199.094\n",
      "Epoch: 47/100.. Training loss: 4360.768\n",
      "Epoch: 47/100.. Training loss: 1192.845\n",
      "Epoch: 47/100.. Training loss: 1886.769\n",
      "Epoch: 47/100.. Training loss: 2426.893\n",
      "Epoch: 47/100.. Training loss: 2832.895\n",
      "Epoch: 48/100.. Training loss: 1723.748\n",
      "Epoch: 48/100.. Training loss: 917.936\n",
      "Epoch: 48/100.. Training loss: 1462.248\n",
      "Epoch: 48/100.. Training loss: 1200.927\n",
      "Epoch: 48/100.. Training loss: 981.995\n",
      "Epoch: 48/100.. Training loss: 1047.031\n",
      "Epoch: 48/100.. Training loss: 858.222\n",
      "Epoch: 48/100.. Training loss: 2105.365\n",
      "Epoch: 48/100.. Training loss: 4454.452\n",
      "Epoch: 48/100.. Training loss: 1376.413\n",
      "Epoch: 48/100.. Training loss: 1982.291\n",
      "Epoch: 48/100.. Training loss: 2829.741\n",
      "Epoch: 48/100.. Training loss: 2651.724\n",
      "Epoch: 49/100.. Training loss: 1614.918\n",
      "Epoch: 49/100.. Training loss: 920.978\n",
      "Epoch: 49/100.. Training loss: 1552.268\n",
      "Epoch: 49/100.. Training loss: 1202.248\n",
      "Epoch: 49/100.. Training loss: 994.807\n",
      "Epoch: 49/100.. Training loss: 1036.532\n",
      "Epoch: 49/100.. Training loss: 775.578\n",
      "Epoch: 49/100.. Training loss: 2141.149\n",
      "Epoch: 49/100.. Training loss: 4628.418\n",
      "Epoch: 49/100.. Training loss: 1334.192\n",
      "Epoch: 49/100.. Training loss: 1912.900\n",
      "Epoch: 49/100.. Training loss: 2816.984\n",
      "Epoch: 49/100.. Training loss: 2798.131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50/100.. Training loss: 1576.593\n",
      "Epoch: 50/100.. Training loss: 887.057\n",
      "Epoch: 50/100.. Training loss: 1431.634\n",
      "Epoch: 50/100.. Training loss: 1362.546\n",
      "Epoch: 50/100.. Training loss: 960.232\n",
      "Epoch: 50/100.. Training loss: 1039.364\n",
      "Epoch: 50/100.. Training loss: 1225.459\n",
      "Epoch: 50/100.. Training loss: 2153.997\n",
      "Epoch: 50/100.. Training loss: 4042.770\n",
      "Epoch: 50/100.. Training loss: 1282.950\n",
      "Epoch: 50/100.. Training loss: 2344.869\n",
      "Epoch: 50/100.. Training loss: 2657.739\n",
      "Epoch: 50/100.. Training loss: 2326.566\n",
      "Epoch: 51/100.. Training loss: 1515.752\n",
      "Epoch: 51/100.. Training loss: 1250.661\n",
      "Epoch: 51/100.. Training loss: 1193.374\n",
      "Epoch: 51/100.. Training loss: 1235.575\n",
      "Epoch: 51/100.. Training loss: 962.050\n",
      "Epoch: 51/100.. Training loss: 1029.015\n",
      "Epoch: 51/100.. Training loss: 1399.776\n",
      "Epoch: 51/100.. Training loss: 1984.376\n",
      "Epoch: 51/100.. Training loss: 4153.730\n",
      "Epoch: 51/100.. Training loss: 1438.888\n",
      "Epoch: 51/100.. Training loss: 2190.382\n",
      "Epoch: 51/100.. Training loss: 2852.643\n",
      "Epoch: 52/100.. Training loss: 2488.569\n",
      "Epoch: 52/100.. Training loss: 1303.154\n",
      "Epoch: 52/100.. Training loss: 1480.029\n",
      "Epoch: 52/100.. Training loss: 1182.411\n",
      "Epoch: 52/100.. Training loss: 1164.206\n",
      "Epoch: 52/100.. Training loss: 934.260\n",
      "Epoch: 52/100.. Training loss: 1077.433\n",
      "Epoch: 52/100.. Training loss: 1626.565\n",
      "Epoch: 52/100.. Training loss: 2190.128\n",
      "Epoch: 52/100.. Training loss: 3845.214\n",
      "Epoch: 52/100.. Training loss: 1416.147\n",
      "Epoch: 52/100.. Training loss: 2092.561\n",
      "Epoch: 52/100.. Training loss: 3198.620\n",
      "Epoch: 53/100.. Training loss: 2580.136\n",
      "Epoch: 53/100.. Training loss: 1049.789\n",
      "Epoch: 53/100.. Training loss: 1363.501\n",
      "Epoch: 53/100.. Training loss: 1099.787\n",
      "Epoch: 53/100.. Training loss: 1132.251\n",
      "Epoch: 53/100.. Training loss: 791.953\n",
      "Epoch: 53/100.. Training loss: 1043.357\n",
      "Epoch: 53/100.. Training loss: 1746.578\n",
      "Epoch: 53/100.. Training loss: 2106.790\n",
      "Epoch: 53/100.. Training loss: 3856.616\n",
      "Epoch: 53/100.. Training loss: 1349.919\n",
      "Epoch: 53/100.. Training loss: 2084.323\n",
      "Epoch: 53/100.. Training loss: 3106.995\n",
      "Epoch: 54/100.. Training loss: 2536.353\n",
      "Epoch: 54/100.. Training loss: 1092.083\n",
      "Epoch: 54/100.. Training loss: 1319.032\n",
      "Epoch: 54/100.. Training loss: 1116.877\n",
      "Epoch: 54/100.. Training loss: 1196.703\n",
      "Epoch: 54/100.. Training loss: 964.966\n",
      "Epoch: 54/100.. Training loss: 833.482\n",
      "Epoch: 54/100.. Training loss: 1835.924\n",
      "Epoch: 54/100.. Training loss: 3156.788\n",
      "Epoch: 54/100.. Training loss: 2954.345\n",
      "Epoch: 54/100.. Training loss: 1185.514\n",
      "Epoch: 54/100.. Training loss: 2007.307\n",
      "Epoch: 54/100.. Training loss: 3849.481\n",
      "Epoch: 55/100.. Training loss: 1743.730\n",
      "Epoch: 55/100.. Training loss: 1058.708\n",
      "Epoch: 55/100.. Training loss: 1247.340\n",
      "Epoch: 55/100.. Training loss: 1088.713\n",
      "Epoch: 55/100.. Training loss: 1167.783\n",
      "Epoch: 55/100.. Training loss: 996.216\n",
      "Epoch: 55/100.. Training loss: 880.934\n",
      "Epoch: 55/100.. Training loss: 1896.444\n",
      "Epoch: 55/100.. Training loss: 2933.844\n",
      "Epoch: 55/100.. Training loss: 2957.944\n",
      "Epoch: 55/100.. Training loss: 1349.099\n",
      "Epoch: 55/100.. Training loss: 1902.732\n",
      "Epoch: 55/100.. Training loss: 3804.990\n",
      "Epoch: 56/100.. Training loss: 1776.218\n",
      "Epoch: 56/100.. Training loss: 1036.671\n",
      "Epoch: 56/100.. Training loss: 1507.339\n",
      "Epoch: 56/100.. Training loss: 1096.849\n",
      "Epoch: 56/100.. Training loss: 1049.982\n",
      "Epoch: 56/100.. Training loss: 1033.183\n",
      "Epoch: 56/100.. Training loss: 798.746\n",
      "Epoch: 56/100.. Training loss: 1956.524\n",
      "Epoch: 56/100.. Training loss: 2889.020\n",
      "Epoch: 56/100.. Training loss: 3022.695\n",
      "Epoch: 56/100.. Training loss: 1431.709\n",
      "Epoch: 56/100.. Training loss: 2179.368\n",
      "Epoch: 56/100.. Training loss: 3381.528\n",
      "Epoch: 57/100.. Training loss: 1831.615\n",
      "Epoch: 57/100.. Training loss: 893.627\n",
      "Epoch: 57/100.. Training loss: 1450.315\n",
      "Epoch: 57/100.. Training loss: 1057.692\n",
      "Epoch: 57/100.. Training loss: 1139.462\n",
      "Epoch: 57/100.. Training loss: 1104.239\n",
      "Epoch: 57/100.. Training loss: 755.196\n",
      "Epoch: 57/100.. Training loss: 2179.006\n",
      "Epoch: 57/100.. Training loss: 4438.546\n",
      "Epoch: 57/100.. Training loss: 1308.581\n",
      "Epoch: 57/100.. Training loss: 1936.279\n",
      "Epoch: 57/100.. Training loss: 2445.799\n",
      "Epoch: 57/100.. Training loss: 2849.030\n",
      "Epoch: 58/100.. Training loss: 1706.484\n",
      "Epoch: 58/100.. Training loss: 925.230\n",
      "Epoch: 58/100.. Training loss: 1443.993\n",
      "Epoch: 58/100.. Training loss: 1126.520\n",
      "Epoch: 58/100.. Training loss: 972.163\n",
      "Epoch: 58/100.. Training loss: 1204.372\n",
      "Epoch: 58/100.. Training loss: 691.118\n",
      "Epoch: 58/100.. Training loss: 2206.953\n",
      "Epoch: 58/100.. Training loss: 4529.024\n",
      "Epoch: 58/100.. Training loss: 1276.390\n",
      "Epoch: 58/100.. Training loss: 1867.938\n",
      "Epoch: 58/100.. Training loss: 2896.400\n",
      "Epoch: 58/100.. Training loss: 2501.903\n",
      "Epoch: 59/100.. Training loss: 1549.025\n",
      "Epoch: 59/100.. Training loss: 866.895\n",
      "Epoch: 59/100.. Training loss: 1441.802\n",
      "Epoch: 59/100.. Training loss: 1177.687\n",
      "Epoch: 59/100.. Training loss: 985.510\n",
      "Epoch: 59/100.. Training loss: 1115.822\n",
      "Epoch: 59/100.. Training loss: 676.424\n",
      "Epoch: 59/100.. Training loss: 2104.685\n",
      "Epoch: 59/100.. Training loss: 4838.629\n",
      "Epoch: 59/100.. Training loss: 1329.352\n",
      "Epoch: 59/100.. Training loss: 1875.125\n",
      "Epoch: 59/100.. Training loss: 2867.404\n",
      "Epoch: 59/100.. Training loss: 2651.630\n",
      "Epoch: 60/100.. Training loss: 1618.570\n",
      "Epoch: 60/100.. Training loss: 930.616\n",
      "Epoch: 60/100.. Training loss: 1317.457\n",
      "Epoch: 60/100.. Training loss: 1356.737\n",
      "Epoch: 60/100.. Training loss: 945.796\n",
      "Epoch: 60/100.. Training loss: 1028.816\n",
      "Epoch: 60/100.. Training loss: 1161.414\n",
      "Epoch: 60/100.. Training loss: 2239.739\n",
      "Epoch: 60/100.. Training loss: 4167.937\n",
      "Epoch: 60/100.. Training loss: 1291.237\n",
      "Epoch: 60/100.. Training loss: 2309.141\n",
      "Epoch: 60/100.. Training loss: 2729.501\n",
      "Epoch: 60/100.. Training loss: 2438.834\n",
      "Epoch: 61/100.. Training loss: 1459.789\n",
      "Epoch: 61/100.. Training loss: 1182.616\n",
      "Epoch: 61/100.. Training loss: 1121.558\n",
      "Epoch: 61/100.. Training loss: 1320.973\n",
      "Epoch: 61/100.. Training loss: 952.225\n",
      "Epoch: 61/100.. Training loss: 1005.064\n",
      "Epoch: 61/100.. Training loss: 1303.470\n",
      "Epoch: 61/100.. Training loss: 2048.563\n",
      "Epoch: 61/100.. Training loss: 4286.167\n",
      "Epoch: 61/100.. Training loss: 1390.965\n",
      "Epoch: 61/100.. Training loss: 2180.657\n",
      "Epoch: 61/100.. Training loss: 2825.020\n",
      "Epoch: 62/100.. Training loss: 2320.632\n",
      "Epoch: 62/100.. Training loss: 1183.324\n",
      "Epoch: 62/100.. Training loss: 1464.432\n",
      "Epoch: 62/100.. Training loss: 1159.977\n",
      "Epoch: 62/100.. Training loss: 1166.725\n",
      "Epoch: 62/100.. Training loss: 882.217\n",
      "Epoch: 62/100.. Training loss: 1007.183\n",
      "Epoch: 62/100.. Training loss: 1629.202\n",
      "Epoch: 62/100.. Training loss: 2050.961\n",
      "Epoch: 62/100.. Training loss: 4001.098\n",
      "Epoch: 62/100.. Training loss: 1307.800\n",
      "Epoch: 62/100.. Training loss: 2136.825\n",
      "Epoch: 62/100.. Training loss: 3108.951\n",
      "Epoch: 63/100.. Training loss: 2418.792\n",
      "Epoch: 63/100.. Training loss: 1015.760\n",
      "Epoch: 63/100.. Training loss: 1427.000\n",
      "Epoch: 63/100.. Training loss: 1113.034\n",
      "Epoch: 63/100.. Training loss: 1226.360\n",
      "Epoch: 63/100.. Training loss: 821.939\n",
      "Epoch: 63/100.. Training loss: 955.864\n",
      "Epoch: 63/100.. Training loss: 1807.341\n",
      "Epoch: 63/100.. Training loss: 2091.118\n",
      "Epoch: 63/100.. Training loss: 3989.352\n",
      "Epoch: 63/100.. Training loss: 1263.437\n",
      "Epoch: 63/100.. Training loss: 2086.445\n",
      "Epoch: 63/100.. Training loss: 2997.246\n",
      "Epoch: 64/100.. Training loss: 2349.544\n",
      "Epoch: 64/100.. Training loss: 1224.935\n",
      "Epoch: 64/100.. Training loss: 1435.906\n",
      "Epoch: 64/100.. Training loss: 1271.918\n",
      "Epoch: 64/100.. Training loss: 1158.845\n",
      "Epoch: 64/100.. Training loss: 952.691\n",
      "Epoch: 64/100.. Training loss: 1013.380\n",
      "Epoch: 64/100.. Training loss: 2009.794\n",
      "Epoch: 64/100.. Training loss: 3143.801\n",
      "Epoch: 64/100.. Training loss: 2881.479\n",
      "Epoch: 64/100.. Training loss: 1292.972\n",
      "Epoch: 64/100.. Training loss: 2206.499\n",
      "Epoch: 64/100.. Training loss: 4251.743\n",
      "Epoch: 65/100.. Training loss: 1664.967\n",
      "Epoch: 65/100.. Training loss: 1185.583\n",
      "Epoch: 65/100.. Training loss: 1297.237\n",
      "Epoch: 65/100.. Training loss: 1152.005\n",
      "Epoch: 65/100.. Training loss: 1162.380\n",
      "Epoch: 65/100.. Training loss: 970.500\n",
      "Epoch: 65/100.. Training loss: 977.200\n",
      "Epoch: 65/100.. Training loss: 2046.658\n",
      "Epoch: 65/100.. Training loss: 2996.983\n",
      "Epoch: 65/100.. Training loss: 2892.512\n",
      "Epoch: 65/100.. Training loss: 1520.495\n",
      "Epoch: 65/100.. Training loss: 1986.688\n",
      "Epoch: 65/100.. Training loss: 3983.272\n",
      "Epoch: 66/100.. Training loss: 1729.664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 66/100.. Training loss: 1150.018\n",
      "Epoch: 66/100.. Training loss: 1483.413\n",
      "Epoch: 66/100.. Training loss: 1001.801\n",
      "Epoch: 66/100.. Training loss: 1029.444\n",
      "Epoch: 66/100.. Training loss: 1064.149\n",
      "Epoch: 66/100.. Training loss: 886.576\n",
      "Epoch: 66/100.. Training loss: 2043.713\n",
      "Epoch: 66/100.. Training loss: 3041.566\n",
      "Epoch: 66/100.. Training loss: 2933.496\n",
      "Epoch: 66/100.. Training loss: 1600.603\n",
      "Epoch: 66/100.. Training loss: 2145.588\n",
      "Epoch: 66/100.. Training loss: 3516.022\n",
      "Epoch: 67/100.. Training loss: 1858.524\n",
      "Epoch: 67/100.. Training loss: 916.650\n",
      "Epoch: 67/100.. Training loss: 1511.727\n",
      "Epoch: 67/100.. Training loss: 1071.753\n",
      "Epoch: 67/100.. Training loss: 1112.083\n",
      "Epoch: 67/100.. Training loss: 1116.396\n",
      "Epoch: 67/100.. Training loss: 768.634\n",
      "Epoch: 67/100.. Training loss: 2228.314\n",
      "Epoch: 67/100.. Training loss: 4557.872\n",
      "Epoch: 67/100.. Training loss: 1281.342\n",
      "Epoch: 67/100.. Training loss: 2031.658\n",
      "Epoch: 67/100.. Training loss: 2422.704\n",
      "Epoch: 67/100.. Training loss: 2992.751\n",
      "Epoch: 68/100.. Training loss: 1703.703\n",
      "Epoch: 68/100.. Training loss: 881.863\n",
      "Epoch: 68/100.. Training loss: 1476.747\n",
      "Epoch: 68/100.. Training loss: 1161.283\n",
      "Epoch: 68/100.. Training loss: 967.442\n",
      "Epoch: 68/100.. Training loss: 1138.937\n",
      "Epoch: 68/100.. Training loss: 758.921\n",
      "Epoch: 68/100.. Training loss: 2198.356\n",
      "Epoch: 68/100.. Training loss: 4686.254\n",
      "Epoch: 68/100.. Training loss: 1299.746\n",
      "Epoch: 68/100.. Training loss: 1979.583\n",
      "Epoch: 68/100.. Training loss: 2834.183\n",
      "Epoch: 68/100.. Training loss: 2567.865\n",
      "Epoch: 69/100.. Training loss: 1597.480\n",
      "Epoch: 69/100.. Training loss: 904.226\n",
      "Epoch: 69/100.. Training loss: 1546.051\n",
      "Epoch: 69/100.. Training loss: 1151.409\n",
      "Epoch: 69/100.. Training loss: 1002.825\n",
      "Epoch: 69/100.. Training loss: 1055.433\n",
      "Epoch: 69/100.. Training loss: 669.752\n",
      "Epoch: 69/100.. Training loss: 2277.835\n",
      "Epoch: 69/100.. Training loss: 4530.709\n",
      "Epoch: 69/100.. Training loss: 1345.589\n",
      "Epoch: 69/100.. Training loss: 1961.040\n",
      "Epoch: 69/100.. Training loss: 2676.613\n",
      "Epoch: 69/100.. Training loss: 2689.948\n",
      "Epoch: 70/100.. Training loss: 1618.012\n",
      "Epoch: 70/100.. Training loss: 927.759\n",
      "Epoch: 70/100.. Training loss: 1415.765\n",
      "Epoch: 70/100.. Training loss: 1295.782\n",
      "Epoch: 70/100.. Training loss: 961.765\n",
      "Epoch: 70/100.. Training loss: 966.499\n",
      "Epoch: 70/100.. Training loss: 1196.036\n",
      "Epoch: 70/100.. Training loss: 2323.648\n",
      "Epoch: 70/100.. Training loss: 4066.666\n",
      "Epoch: 70/100.. Training loss: 1407.204\n",
      "Epoch: 70/100.. Training loss: 2307.992\n",
      "Epoch: 70/100.. Training loss: 2725.386\n",
      "Epoch: 70/100.. Training loss: 2321.513\n",
      "Epoch: 71/100.. Training loss: 1551.760\n",
      "Epoch: 71/100.. Training loss: 1185.335\n",
      "Epoch: 71/100.. Training loss: 1196.406\n",
      "Epoch: 71/100.. Training loss: 1259.313\n",
      "Epoch: 71/100.. Training loss: 950.784\n",
      "Epoch: 71/100.. Training loss: 1018.990\n",
      "Epoch: 71/100.. Training loss: 1360.322\n",
      "Epoch: 71/100.. Training loss: 2023.469\n",
      "Epoch: 71/100.. Training loss: 4235.060\n",
      "Epoch: 71/100.. Training loss: 1424.019\n",
      "Epoch: 71/100.. Training loss: 2204.901\n",
      "Epoch: 71/100.. Training loss: 2792.767\n",
      "Epoch: 72/100.. Training loss: 2436.843\n",
      "Epoch: 72/100.. Training loss: 1307.352\n",
      "Epoch: 72/100.. Training loss: 1470.868\n",
      "Epoch: 72/100.. Training loss: 1198.987\n",
      "Epoch: 72/100.. Training loss: 1136.318\n",
      "Epoch: 72/100.. Training loss: 864.775\n",
      "Epoch: 72/100.. Training loss: 1096.393\n",
      "Epoch: 72/100.. Training loss: 1639.046\n",
      "Epoch: 72/100.. Training loss: 2145.928\n",
      "Epoch: 72/100.. Training loss: 3793.825\n",
      "Epoch: 72/100.. Training loss: 1308.111\n",
      "Epoch: 72/100.. Training loss: 2076.771\n",
      "Epoch: 72/100.. Training loss: 3013.565\n",
      "Epoch: 73/100.. Training loss: 2425.752\n",
      "Epoch: 73/100.. Training loss: 1075.758\n",
      "Epoch: 73/100.. Training loss: 1437.002\n",
      "Epoch: 73/100.. Training loss: 1131.739\n",
      "Epoch: 73/100.. Training loss: 1234.947\n",
      "Epoch: 73/100.. Training loss: 819.058\n",
      "Epoch: 73/100.. Training loss: 992.895\n",
      "Epoch: 73/100.. Training loss: 1854.844\n",
      "Epoch: 73/100.. Training loss: 2164.122\n",
      "Epoch: 73/100.. Training loss: 3824.039\n",
      "Epoch: 73/100.. Training loss: 1351.154\n",
      "Epoch: 73/100.. Training loss: 2053.413\n",
      "Epoch: 73/100.. Training loss: 2994.212\n",
      "Epoch: 74/100.. Training loss: 2418.654\n",
      "Epoch: 74/100.. Training loss: 1091.133\n",
      "Epoch: 74/100.. Training loss: 1394.718\n",
      "Epoch: 74/100.. Training loss: 1144.317\n",
      "Epoch: 74/100.. Training loss: 1203.946\n",
      "Epoch: 74/100.. Training loss: 1044.907\n",
      "Epoch: 74/100.. Training loss: 888.600\n",
      "Epoch: 74/100.. Training loss: 1850.073\n",
      "Epoch: 74/100.. Training loss: 3046.253\n",
      "Epoch: 74/100.. Training loss: 2751.805\n",
      "Epoch: 74/100.. Training loss: 1243.688\n",
      "Epoch: 74/100.. Training loss: 2023.293\n",
      "Epoch: 74/100.. Training loss: 3955.264\n",
      "Epoch: 75/100.. Training loss: 1700.563\n",
      "Epoch: 75/100.. Training loss: 1042.220\n",
      "Epoch: 75/100.. Training loss: 1245.252\n",
      "Epoch: 75/100.. Training loss: 1143.471\n",
      "Epoch: 75/100.. Training loss: 1214.016\n",
      "Epoch: 75/100.. Training loss: 1028.992\n",
      "Epoch: 75/100.. Training loss: 869.279\n",
      "Epoch: 75/100.. Training loss: 1868.360\n",
      "Epoch: 75/100.. Training loss: 3121.256\n",
      "Epoch: 75/100.. Training loss: 3014.635\n",
      "Epoch: 75/100.. Training loss: 1374.348\n",
      "Epoch: 75/100.. Training loss: 1913.119\n",
      "Epoch: 75/100.. Training loss: 3797.236\n",
      "Epoch: 76/100.. Training loss: 1835.031\n",
      "Epoch: 76/100.. Training loss: 1076.209\n",
      "Epoch: 76/100.. Training loss: 1443.809\n",
      "Epoch: 76/100.. Training loss: 1050.584\n",
      "Epoch: 76/100.. Training loss: 1059.137\n",
      "Epoch: 76/100.. Training loss: 1082.332\n",
      "Epoch: 76/100.. Training loss: 833.237\n",
      "Epoch: 76/100.. Training loss: 1895.268\n",
      "Epoch: 76/100.. Training loss: 3114.603\n",
      "Epoch: 76/100.. Training loss: 3001.413\n",
      "Epoch: 76/100.. Training loss: 1457.809\n",
      "Epoch: 76/100.. Training loss: 2199.285\n",
      "Epoch: 76/100.. Training loss: 3482.883\n",
      "Epoch: 77/100.. Training loss: 1817.544\n",
      "Epoch: 77/100.. Training loss: 926.535\n",
      "Epoch: 77/100.. Training loss: 1426.722\n",
      "Epoch: 77/100.. Training loss: 1099.930\n",
      "Epoch: 77/100.. Training loss: 1090.207\n",
      "Epoch: 77/100.. Training loss: 1088.742\n",
      "Epoch: 77/100.. Training loss: 768.038\n",
      "Epoch: 77/100.. Training loss: 2098.268\n",
      "Epoch: 77/100.. Training loss: 4686.034\n",
      "Epoch: 77/100.. Training loss: 1278.474\n",
      "Epoch: 77/100.. Training loss: 1944.242\n",
      "Epoch: 77/100.. Training loss: 2462.032\n",
      "Epoch: 77/100.. Training loss: 2998.733\n",
      "Epoch: 78/100.. Training loss: 1687.424\n",
      "Epoch: 78/100.. Training loss: 891.604\n",
      "Epoch: 78/100.. Training loss: 1490.066\n",
      "Epoch: 78/100.. Training loss: 1173.914\n",
      "Epoch: 78/100.. Training loss: 1005.206\n",
      "Epoch: 78/100.. Training loss: 1151.207\n",
      "Epoch: 78/100.. Training loss: 786.269\n",
      "Epoch: 78/100.. Training loss: 2236.237\n",
      "Epoch: 78/100.. Training loss: 4614.818\n",
      "Epoch: 78/100.. Training loss: 1305.636\n",
      "Epoch: 78/100.. Training loss: 1912.753\n",
      "Epoch: 78/100.. Training loss: 2878.685\n",
      "Epoch: 78/100.. Training loss: 2633.277\n",
      "Epoch: 79/100.. Training loss: 1571.115\n",
      "Epoch: 79/100.. Training loss: 927.365\n",
      "Epoch: 79/100.. Training loss: 1559.893\n",
      "Epoch: 79/100.. Training loss: 1294.584\n",
      "Epoch: 79/100.. Training loss: 1038.553\n",
      "Epoch: 79/100.. Training loss: 1004.649\n",
      "Epoch: 79/100.. Training loss: 659.666\n",
      "Epoch: 79/100.. Training loss: 2162.995\n",
      "Epoch: 79/100.. Training loss: 4790.423\n",
      "Epoch: 79/100.. Training loss: 1240.815\n",
      "Epoch: 79/100.. Training loss: 1827.848\n",
      "Epoch: 79/100.. Training loss: 2869.880\n",
      "Epoch: 79/100.. Training loss: 2709.708\n",
      "Epoch: 80/100.. Training loss: 1467.870\n",
      "Epoch: 80/100.. Training loss: 891.925\n",
      "Epoch: 80/100.. Training loss: 1483.887\n",
      "Epoch: 80/100.. Training loss: 1361.310\n",
      "Epoch: 80/100.. Training loss: 952.984\n",
      "Epoch: 80/100.. Training loss: 979.626\n",
      "Epoch: 80/100.. Training loss: 1188.087\n",
      "Epoch: 80/100.. Training loss: 2317.073\n",
      "Epoch: 80/100.. Training loss: 4073.336\n",
      "Epoch: 80/100.. Training loss: 1296.604\n",
      "Epoch: 80/100.. Training loss: 2327.655\n",
      "Epoch: 80/100.. Training loss: 2673.796\n",
      "Epoch: 80/100.. Training loss: 2289.148\n",
      "Epoch: 81/100.. Training loss: 1507.574\n",
      "Epoch: 81/100.. Training loss: 1294.481\n",
      "Epoch: 81/100.. Training loss: 1190.951\n",
      "Epoch: 81/100.. Training loss: 1282.505\n",
      "Epoch: 81/100.. Training loss: 994.809\n",
      "Epoch: 81/100.. Training loss: 1073.914\n",
      "Epoch: 81/100.. Training loss: 1359.449\n",
      "Epoch: 81/100.. Training loss: 2078.720\n",
      "Epoch: 81/100.. Training loss: 4044.337\n",
      "Epoch: 81/100.. Training loss: 1500.314\n",
      "Epoch: 81/100.. Training loss: 2133.633\n",
      "Epoch: 81/100.. Training loss: 2816.796\n",
      "Epoch: 82/100.. Training loss: 2488.696\n",
      "Epoch: 82/100.. Training loss: 1259.462\n",
      "Epoch: 82/100.. Training loss: 1413.971\n",
      "Epoch: 82/100.. Training loss: 1193.261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 82/100.. Training loss: 1245.826\n",
      "Epoch: 82/100.. Training loss: 879.748\n",
      "Epoch: 82/100.. Training loss: 1103.241\n",
      "Epoch: 82/100.. Training loss: 1622.288\n",
      "Epoch: 82/100.. Training loss: 2134.050\n",
      "Epoch: 82/100.. Training loss: 3799.333\n",
      "Epoch: 82/100.. Training loss: 1371.840\n",
      "Epoch: 82/100.. Training loss: 2169.891\n",
      "Epoch: 82/100.. Training loss: 3167.864\n",
      "Epoch: 83/100.. Training loss: 2343.029\n",
      "Epoch: 83/100.. Training loss: 1053.556\n",
      "Epoch: 83/100.. Training loss: 1402.673\n",
      "Epoch: 83/100.. Training loss: 1179.309\n",
      "Epoch: 83/100.. Training loss: 1258.358\n",
      "Epoch: 83/100.. Training loss: 809.281\n",
      "Epoch: 83/100.. Training loss: 1040.709\n",
      "Epoch: 83/100.. Training loss: 1777.803\n",
      "Epoch: 83/100.. Training loss: 2062.461\n",
      "Epoch: 83/100.. Training loss: 4004.299\n",
      "Epoch: 83/100.. Training loss: 1282.868\n",
      "Epoch: 83/100.. Training loss: 2140.922\n",
      "Epoch: 83/100.. Training loss: 3108.373\n",
      "Epoch: 84/100.. Training loss: 2374.062\n",
      "Epoch: 84/100.. Training loss: 1118.943\n",
      "Epoch: 84/100.. Training loss: 1288.816\n",
      "Epoch: 84/100.. Training loss: 1110.818\n",
      "Epoch: 84/100.. Training loss: 1220.612\n",
      "Epoch: 84/100.. Training loss: 1025.863\n",
      "Epoch: 84/100.. Training loss: 996.372\n",
      "Epoch: 84/100.. Training loss: 1848.336\n",
      "Epoch: 84/100.. Training loss: 3207.687\n",
      "Epoch: 84/100.. Training loss: 2996.058\n",
      "Epoch: 84/100.. Training loss: 1211.470\n",
      "Epoch: 84/100.. Training loss: 2065.165\n",
      "Epoch: 84/100.. Training loss: 3990.616\n",
      "Epoch: 85/100.. Training loss: 1574.144\n",
      "Epoch: 85/100.. Training loss: 1058.749\n",
      "Epoch: 85/100.. Training loss: 1238.001\n",
      "Epoch: 85/100.. Training loss: 1115.484\n",
      "Epoch: 85/100.. Training loss: 1255.131\n",
      "Epoch: 85/100.. Training loss: 1043.673\n",
      "Epoch: 85/100.. Training loss: 855.340\n",
      "Epoch: 85/100.. Training loss: 1914.378\n",
      "Epoch: 85/100.. Training loss: 3086.756\n",
      "Epoch: 85/100.. Training loss: 3055.517\n",
      "Epoch: 85/100.. Training loss: 1329.298\n",
      "Epoch: 85/100.. Training loss: 1916.672\n",
      "Epoch: 85/100.. Training loss: 3826.477\n",
      "Epoch: 86/100.. Training loss: 1683.755\n",
      "Epoch: 86/100.. Training loss: 976.295\n",
      "Epoch: 86/100.. Training loss: 1485.996\n",
      "Epoch: 86/100.. Training loss: 1136.415\n",
      "Epoch: 86/100.. Training loss: 1100.761\n",
      "Epoch: 86/100.. Training loss: 1072.161\n",
      "Epoch: 86/100.. Training loss: 804.804\n",
      "Epoch: 86/100.. Training loss: 2001.902\n",
      "Epoch: 86/100.. Training loss: 3147.281\n",
      "Epoch: 86/100.. Training loss: 3080.394\n",
      "Epoch: 86/100.. Training loss: 1450.762\n",
      "Epoch: 86/100.. Training loss: 2141.494\n",
      "Epoch: 86/100.. Training loss: 3530.525\n",
      "Epoch: 87/100.. Training loss: 1730.320\n",
      "Epoch: 87/100.. Training loss: 909.918\n",
      "Epoch: 87/100.. Training loss: 1499.882\n",
      "Epoch: 87/100.. Training loss: 1105.534\n",
      "Epoch: 87/100.. Training loss: 1091.627\n",
      "Epoch: 87/100.. Training loss: 1106.850\n",
      "Epoch: 87/100.. Training loss: 778.560\n",
      "Epoch: 87/100.. Training loss: 2163.899\n",
      "Epoch: 87/100.. Training loss: 4744.449\n",
      "Epoch: 87/100.. Training loss: 1272.721\n",
      "Epoch: 87/100.. Training loss: 1872.625\n",
      "Epoch: 87/100.. Training loss: 2408.173\n",
      "Epoch: 87/100.. Training loss: 2843.091\n",
      "Epoch: 88/100.. Training loss: 1675.110\n",
      "Epoch: 88/100.. Training loss: 935.553\n",
      "Epoch: 88/100.. Training loss: 1547.016\n",
      "Epoch: 88/100.. Training loss: 1132.988\n",
      "Epoch: 88/100.. Training loss: 986.208\n",
      "Epoch: 88/100.. Training loss: 1166.122\n",
      "Epoch: 88/100.. Training loss: 730.938\n",
      "Epoch: 88/100.. Training loss: 2305.116\n",
      "Epoch: 88/100.. Training loss: 4818.865\n",
      "Epoch: 88/100.. Training loss: 1274.994\n",
      "Epoch: 88/100.. Training loss: 1883.029\n",
      "Epoch: 88/100.. Training loss: 3024.512\n",
      "Epoch: 88/100.. Training loss: 2466.825\n",
      "Epoch: 89/100.. Training loss: 1484.666\n",
      "Epoch: 89/100.. Training loss: 907.273\n",
      "Epoch: 89/100.. Training loss: 1543.650\n",
      "Epoch: 89/100.. Training loss: 1193.754\n",
      "Epoch: 89/100.. Training loss: 1057.869\n",
      "Epoch: 89/100.. Training loss: 1007.292\n",
      "Epoch: 89/100.. Training loss: 640.604\n",
      "Epoch: 89/100.. Training loss: 2199.417\n",
      "Epoch: 89/100.. Training loss: 4839.333\n",
      "Epoch: 89/100.. Training loss: 1259.534\n",
      "Epoch: 89/100.. Training loss: 1843.751\n",
      "Epoch: 89/100.. Training loss: 2793.266\n",
      "Epoch: 89/100.. Training loss: 2707.709\n",
      "Epoch: 90/100.. Training loss: 1481.787\n",
      "Epoch: 90/100.. Training loss: 950.641\n",
      "Epoch: 90/100.. Training loss: 1343.736\n",
      "Epoch: 90/100.. Training loss: 1368.604\n",
      "Epoch: 90/100.. Training loss: 958.608\n",
      "Epoch: 90/100.. Training loss: 1036.292\n",
      "Epoch: 90/100.. Training loss: 1164.407\n",
      "Epoch: 90/100.. Training loss: 2364.989\n",
      "Epoch: 90/100.. Training loss: 4255.815\n",
      "Epoch: 90/100.. Training loss: 1339.219\n",
      "Epoch: 90/100.. Training loss: 2248.978\n",
      "Epoch: 90/100.. Training loss: 2683.466\n",
      "Epoch: 90/100.. Training loss: 2252.085\n",
      "Epoch: 91/100.. Training loss: 1503.960\n",
      "Epoch: 91/100.. Training loss: 1258.645\n",
      "Epoch: 91/100.. Training loss: 1190.469\n",
      "Epoch: 91/100.. Training loss: 1350.902\n",
      "Epoch: 91/100.. Training loss: 982.471\n",
      "Epoch: 91/100.. Training loss: 1052.397\n",
      "Epoch: 91/100.. Training loss: 1408.172\n",
      "Epoch: 91/100.. Training loss: 2081.215\n",
      "Epoch: 91/100.. Training loss: 4368.849\n",
      "Epoch: 91/100.. Training loss: 1369.713\n",
      "Epoch: 91/100.. Training loss: 2191.857\n",
      "Epoch: 91/100.. Training loss: 2801.679\n",
      "Epoch: 92/100.. Training loss: 2554.381\n",
      "Epoch: 92/100.. Training loss: 1249.568\n",
      "Epoch: 92/100.. Training loss: 1436.316\n",
      "Epoch: 92/100.. Training loss: 1186.142\n",
      "Epoch: 92/100.. Training loss: 1296.663\n",
      "Epoch: 92/100.. Training loss: 864.415\n",
      "Epoch: 92/100.. Training loss: 1080.854\n",
      "Epoch: 92/100.. Training loss: 1583.407\n",
      "Epoch: 92/100.. Training loss: 2145.122\n",
      "Epoch: 92/100.. Training loss: 3863.052\n",
      "Epoch: 92/100.. Training loss: 1378.196\n",
      "Epoch: 92/100.. Training loss: 2136.474\n",
      "Epoch: 92/100.. Training loss: 3095.229\n",
      "Epoch: 93/100.. Training loss: 2427.505\n",
      "Epoch: 93/100.. Training loss: 1097.358\n",
      "Epoch: 93/100.. Training loss: 1422.729\n",
      "Epoch: 93/100.. Training loss: 1119.461\n",
      "Epoch: 93/100.. Training loss: 1239.543\n",
      "Epoch: 93/100.. Training loss: 824.095\n",
      "Epoch: 93/100.. Training loss: 1017.311\n",
      "Epoch: 93/100.. Training loss: 1732.368\n",
      "Epoch: 93/100.. Training loss: 2079.068\n",
      "Epoch: 93/100.. Training loss: 4055.655\n",
      "Epoch: 93/100.. Training loss: 1245.989\n",
      "Epoch: 93/100.. Training loss: 2035.733\n",
      "Epoch: 93/100.. Training loss: 3113.603\n",
      "Epoch: 94/100.. Training loss: 2407.905\n",
      "Epoch: 94/100.. Training loss: 1147.537\n",
      "Epoch: 94/100.. Training loss: 1256.656\n",
      "Epoch: 94/100.. Training loss: 1183.190\n",
      "Epoch: 94/100.. Training loss: 1278.051\n",
      "Epoch: 94/100.. Training loss: 1041.405\n",
      "Epoch: 94/100.. Training loss: 847.303\n",
      "Epoch: 94/100.. Training loss: 1817.737\n",
      "Epoch: 94/100.. Training loss: 3182.575\n",
      "Epoch: 94/100.. Training loss: 3003.058\n",
      "Epoch: 94/100.. Training loss: 1160.949\n",
      "Epoch: 94/100.. Training loss: 2011.236\n",
      "Epoch: 94/100.. Training loss: 3778.421\n",
      "Epoch: 95/100.. Training loss: 1669.203\n",
      "Epoch: 95/100.. Training loss: 1025.729\n",
      "Epoch: 95/100.. Training loss: 1247.968\n",
      "Epoch: 95/100.. Training loss: 1143.923\n",
      "Epoch: 95/100.. Training loss: 1282.333\n",
      "Epoch: 95/100.. Training loss: 992.072\n",
      "Epoch: 95/100.. Training loss: 935.889\n",
      "Epoch: 95/100.. Training loss: 1943.643\n",
      "Epoch: 95/100.. Training loss: 3146.291\n",
      "Epoch: 95/100.. Training loss: 3112.209\n",
      "Epoch: 95/100.. Training loss: 1403.248\n",
      "Epoch: 95/100.. Training loss: 1923.312\n",
      "Epoch: 95/100.. Training loss: 3706.540\n",
      "Epoch: 96/100.. Training loss: 1712.948\n",
      "Epoch: 96/100.. Training loss: 1046.644\n",
      "Epoch: 96/100.. Training loss: 1451.143\n",
      "Epoch: 96/100.. Training loss: 1104.206\n",
      "Epoch: 96/100.. Training loss: 1072.921\n",
      "Epoch: 96/100.. Training loss: 1069.776\n",
      "Epoch: 96/100.. Training loss: 832.570\n",
      "Epoch: 96/100.. Training loss: 1988.231\n",
      "Epoch: 96/100.. Training loss: 3103.298\n",
      "Epoch: 96/100.. Training loss: 3000.225\n",
      "Epoch: 96/100.. Training loss: 1429.608\n",
      "Epoch: 96/100.. Training loss: 2182.214\n",
      "Epoch: 96/100.. Training loss: 3424.722\n",
      "Epoch: 97/100.. Training loss: 1840.045\n",
      "Epoch: 97/100.. Training loss: 850.217\n",
      "Epoch: 97/100.. Training loss: 1438.703\n",
      "Epoch: 97/100.. Training loss: 1092.985\n",
      "Epoch: 97/100.. Training loss: 1141.586\n",
      "Epoch: 97/100.. Training loss: 1110.840\n",
      "Epoch: 97/100.. Training loss: 705.920\n",
      "Epoch: 97/100.. Training loss: 2240.766\n",
      "Epoch: 97/100.. Training loss: 4370.508\n",
      "Epoch: 97/100.. Training loss: 1287.857\n",
      "Epoch: 97/100.. Training loss: 2011.098\n",
      "Epoch: 97/100.. Training loss: 2366.814\n",
      "Epoch: 97/100.. Training loss: 2707.341\n",
      "Epoch: 98/100.. Training loss: 1684.385\n",
      "Epoch: 98/100.. Training loss: 890.252\n",
      "Epoch: 98/100.. Training loss: 1447.822\n",
      "Epoch: 98/100.. Training loss: 1200.055\n",
      "Epoch: 98/100.. Training loss: 1021.396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 98/100.. Training loss: 1203.020\n",
      "Epoch: 98/100.. Training loss: 708.075\n",
      "Epoch: 98/100.. Training loss: 2186.924\n",
      "Epoch: 98/100.. Training loss: 4657.388\n",
      "Epoch: 98/100.. Training loss: 1239.507\n",
      "Epoch: 98/100.. Training loss: 1830.240\n",
      "Epoch: 98/100.. Training loss: 2793.375\n",
      "Epoch: 98/100.. Training loss: 2464.966\n",
      "Epoch: 99/100.. Training loss: 1585.563\n",
      "Epoch: 99/100.. Training loss: 898.809\n",
      "Epoch: 99/100.. Training loss: 1558.345\n",
      "Epoch: 99/100.. Training loss: 1247.875\n",
      "Epoch: 99/100.. Training loss: 1034.777\n",
      "Epoch: 99/100.. Training loss: 1007.746\n",
      "Epoch: 99/100.. Training loss: 720.199\n",
      "Epoch: 99/100.. Training loss: 2109.137\n",
      "Epoch: 99/100.. Training loss: 4895.103\n",
      "Epoch: 99/100.. Training loss: 1313.540\n",
      "Epoch: 99/100.. Training loss: 1779.747\n",
      "Epoch: 99/100.. Training loss: 2954.078\n",
      "Epoch: 99/100.. Training loss: 2603.010\n",
      "Epoch: 100/100.. Training loss: 1474.042\n",
      "Epoch: 100/100.. Training loss: 879.280\n",
      "Epoch: 100/100.. Training loss: 1399.279\n",
      "Epoch: 100/100.. Training loss: 1300.541\n",
      "Epoch: 100/100.. Training loss: 975.955\n",
      "Epoch: 100/100.. Training loss: 965.816\n",
      "Epoch: 100/100.. Training loss: 1117.095\n",
      "Epoch: 100/100.. Training loss: 2277.267\n",
      "Epoch: 100/100.. Training loss: 4222.586\n",
      "Epoch: 100/100.. Training loss: 1296.885\n",
      "Epoch: 100/100.. Training loss: 2253.762\n",
      "Epoch: 100/100.. Training loss: 2723.427\n",
      "Epoch: 100/100.. Training loss: 2278.554\n"
     ]
    }
   ],
   "source": [
    "model = NN(hidden_sizes=[30, 15])\n",
    "\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.005)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "training(model, criterion, optimizer, featureloader, labelloader, epochs=100, print_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1508,  0.1116,  0.1067, -0.1962,  0.3717,  0.2708,  0.3213, -1.3317,\n",
      "         -0.1024, -0.3320,  0.4663,  0.6533,  0.1071,  0.8629, -0.8526,  0.3096,\n",
      "          0.4353, -0.0418, -1.6303,  0.7298, -0.1991,  0.1517, -0.2207, -0.4123],\n",
      "        [ 0.2594,  0.0228, -0.1904,  0.0450,  0.2800,  0.5958,  0.5181,  0.8084,\n",
      "         -0.7581, -0.6571,  0.8008,  0.1153,  0.0067,  0.5632, -0.2421,  0.8394,\n",
      "         -0.4086,  0.2528,  0.8038,  0.1209,  0.0834,  0.0516, -0.0973, -0.3190],\n",
      "        [-0.1005, -0.1669,  0.1688, -0.0156,  0.2320, -0.4707,  0.3908,  0.5735,\n",
      "         -0.4592, -0.3241,  0.6081,  1.3575, -0.1406,  0.3307, -0.5829, -0.1431,\n",
      "          2.5006,  0.3831,  0.2938,  0.0314, -0.0466, -0.0469, -0.1101, -0.2619],\n",
      "        [-0.2211, -0.0815, -0.0201, -0.1794, -0.4260, -0.3989, -0.6235, -1.0338,\n",
      "          0.3433,  0.2787, -0.7507, -0.1301, -0.0221, -0.7762,  0.5694, -0.8800,\n",
      "         -0.0156, -0.5715, -1.0808, -0.1436,  0.0518, -0.0729,  0.4691,  0.3457],\n",
      "        [-0.3124,  0.0886, -0.1416, -0.1693,  0.0945,  0.0327, -0.8137,  0.8773,\n",
      "          0.1415,  0.4564, -0.0911, -3.4937, -0.1392, -0.0930,  1.0961, -0.6485,\n",
      "         -1.9397, -1.0400,  0.6999, -0.0407, -0.0676, -0.1180,  0.8716,  0.4440],\n",
      "        [ 0.0516, -0.0385, -0.0853,  0.0932, -0.0677, -0.2843, -0.4740, -0.7722,\n",
      "         -0.0224,  0.4026, -0.4065, -0.0438,  0.0103, -0.4522,  0.2987, -0.9592,\n",
      "          0.0802, -0.4259, -0.6993, -0.0049,  0.1121,  0.1481,  0.2442,  0.6400],\n",
      "        [-0.4770,  0.1271,  0.0930, -0.1550, -0.2891, -0.0155, -0.7883, -0.7708,\n",
      "          0.4335,  0.5656, -0.1010, -0.0217,  0.0883, -0.7357,  0.4239, -0.9518,\n",
      "          0.0206, -0.1801, -0.6596,  0.0940, -0.1436,  0.0591,  0.6098,  0.1232],\n",
      "        [-0.8074,  0.0169, -0.1450, -0.1692,  0.9829,  0.6182,  1.1374, -0.5302,\n",
      "         -0.0904, -0.3641,  0.1083,  3.3635, -0.1870,  1.8148, -0.7214,  0.3019,\n",
      "          2.6431,  0.3613, -0.8945,  0.4743,  0.0360, -0.0530,  0.5559, -0.4869],\n",
      "        [ 0.2722,  0.2971,  0.1219,  0.1130,  0.0561,  0.0443, -0.2201, -0.9117,\n",
      "         -0.1215,  0.1874, -0.3620, -0.3844, -0.0740, -0.3640,  0.4445,  0.4148,\n",
      "         -1.1886, -0.0973, -0.7527, -0.1763, -0.0373,  0.2039, -0.2044,  0.0223],\n",
      "        [-0.1500,  0.3049,  0.1529,  0.1739,  0.0266,  0.3644, -0.5392,  0.1333,\n",
      "         -0.5020,  0.0713,  0.0187, -2.2678,  0.1619, -0.8968,  0.5573, -0.4288,\n",
      "         -1.7546, -0.7060, -0.2269, -1.1510, -0.1369, -0.1152, -0.1423,  0.1123],\n",
      "        [-0.2798, -0.1510, -0.1852,  0.1937,  0.2645,  0.5882,  0.0934, -1.3469,\n",
      "          0.1246, -0.5452,  0.6316,  1.7358,  0.0414,  0.7323, -0.7647,  0.3538,\n",
      "          2.3755,  0.4234, -1.3654,  0.3433, -0.1138,  0.0366,  0.0443, -0.2806],\n",
      "        [ 0.1897, -0.4325, -0.0577, -0.1369,  0.0220, -0.1655,  0.4380, -1.0977,\n",
      "          0.3095, -0.9746,  0.9075,  2.0322, -0.0792,  0.7750, -0.3897,  0.5586,\n",
      "          2.1803, -0.0798, -1.3642,  0.0303,  0.0720, -0.0333, -0.2343, -1.0823],\n",
      "        [-0.3560, -0.3331,  0.1632,  0.0788,  0.5653,  0.7720,  0.7278, -0.2534,\n",
      "         -0.3799,  0.0105,  0.1274,  3.1823, -0.0005, -0.0233, -0.7097,  0.1834,\n",
      "          2.7238,  0.4341, -0.0029, -0.1244,  0.0896, -0.1015, -0.1936, -0.3580],\n",
      "        [ 0.5775,  0.1099, -0.1583,  0.1658, -0.2744,  0.1894, -0.1110, -1.0908,\n",
      "          0.1365,  0.4069, -0.3149, -0.3431, -0.1977, -0.5464, -0.0315,  0.1907,\n",
      "         -0.9955,  0.1187, -0.9859, -0.0993,  0.0761, -0.1287,  0.1146,  0.1706],\n",
      "        [ 0.5984, -0.0187,  0.1460, -0.1189,  0.0512, -0.2663, -0.7419,  0.7825,\n",
      "          0.1281,  0.2768, -0.5603, -1.1498, -0.1044, -1.3128,  0.6207, -0.1240,\n",
      "         -1.6124, -0.0665,  0.9886, -0.1018, -0.1996,  0.1245,  0.1762,  0.1928],\n",
      "        [-0.0157,  0.0443,  0.1009,  0.1946,  0.0955, -0.0400, -0.0708, -0.9280,\n",
      "          0.3972,  0.2081,  0.6048,  0.7591,  0.1174, -0.1561, -0.3658, -0.2581,\n",
      "          2.0136,  0.3527, -0.9197, -0.3071, -0.0239, -0.0650,  0.3128,  0.0323],\n",
      "        [-0.7302, -0.2825,  0.1300,  0.1830,  0.2170,  0.2213, -0.0168, -0.6752,\n",
      "          0.0708, -0.1947,  0.3353, -0.3970, -0.1406,  0.0235, -0.1460,  0.1651,\n",
      "         -0.6484, -0.1284, -0.5778, -0.1581, -0.0850, -0.0211,  0.2915,  0.0405],\n",
      "        [-0.1944, -0.2693,  0.0963, -0.0038, -0.0135, -0.0590,  0.3497,  0.3540,\n",
      "         -0.1901, -0.6637,  0.5997,  1.2415, -0.0851, -0.0622, -0.7646,  0.4697,\n",
      "          2.2118,  0.2831,  0.3302, -0.3448, -0.0177,  0.1247,  0.2532,  0.2401],\n",
      "        [ 0.5926, -0.1236, -0.0076,  0.1253, -0.1627, -0.0853, -0.8560,  0.6502,\n",
      "         -0.4029,  0.2709,  0.1895, -3.3277,  0.0676, -0.3644,  0.8023, -0.2740,\n",
      "         -3.0464, -0.2428,  0.6849, -0.9321,  0.0204, -0.2009,  0.1381,  0.4954],\n",
      "        [ 0.0384, -0.1709,  0.1354,  0.0894, -0.4031, -0.2513,  0.1227, -1.0540,\n",
      "         -0.3519,  0.2659,  0.0159,  0.6342,  0.1563, -0.3165, -0.1427, -0.5204,\n",
      "          1.5647, -0.5048, -0.9257,  0.2208,  0.1792, -0.1799,  0.3404,  0.0446],\n",
      "        [-0.0053, -0.0895, -0.1865,  0.0401, -0.4698, -0.2627, -0.0613, -0.8790,\n",
      "          0.0293,  0.4569, -0.3733,  0.3366,  0.1025, -0.4791,  0.3657, -0.5770,\n",
      "          0.5655, -0.2040, -0.9522, -0.0251,  0.0988,  0.0550, -0.0591,  0.4773],\n",
      "        [ 0.1699, -0.2125,  0.0230, -0.0858,  0.0036,  0.4237,  0.1424, -0.3243,\n",
      "          0.1629,  0.2733,  0.2171, -0.2760, -0.0102, -0.0425,  0.1019,  0.2320,\n",
      "         -0.3906, -0.1663, -0.5195,  0.3208, -0.0961, -0.1579,  0.2964, -0.0474],\n",
      "        [ 0.1421,  0.2674, -0.1677,  0.0377,  0.0498,  0.1345, -0.5688,  0.1197,\n",
      "         -0.1106,  0.6593,  0.0869, -3.6892, -0.1384, -0.0783,  1.1957, -0.6094,\n",
      "         -2.8857, -0.6050,  0.4230, -1.1206,  0.0431, -0.0268,  0.5127, -0.2179],\n",
      "        [-0.0567,  1.2462,  0.0551, -0.0791,  0.7060,  0.5406, -0.1796, -0.4526,\n",
      "          0.1269, -0.0485,  0.7218,  3.0845, -0.1614,  0.4367, -0.2958,  0.3719,\n",
      "          3.0136,  0.0606, -0.4950,  0.5451,  0.1476, -0.0569,  0.0945, -0.8245],\n",
      "        [ 0.1633, -0.0401,  0.1023, -0.1266, -0.2331, -0.1558,  0.7679, -0.8745,\n",
      "         -0.2947, -0.3059,  0.2359,  4.0858,  0.1056,  0.6255, -1.6613,  0.9598,\n",
      "          2.7201,  0.5548, -0.6546,  0.4579, -0.0156,  0.0585, -0.7224, -0.0956],\n",
      "        [ 0.1630,  0.2010,  0.1158,  0.1247, -0.6058,  0.0840, -0.2156, -0.2836,\n",
      "          0.2887,  0.7796, -0.6116, -1.7688,  0.2038, -0.2836,  0.5650, -0.2051,\n",
      "         -1.9905, -0.1185, -0.2752,  0.1121, -0.0865,  0.0532,  0.0488, -0.1257],\n",
      "        [-0.0385,  0.1933, -0.0191, -0.0914,  0.1484,  0.3380, -0.2309,  0.8794,\n",
      "         -0.6793, -0.4459,  0.1286, -0.9740, -0.1637,  0.5914,  0.1084,  0.4002,\n",
      "         -1.5391,  0.0311,  0.8991,  0.1825,  0.0431,  0.1206,  0.0810,  0.2795],\n",
      "        [ 0.0102, -0.2242,  0.0694, -0.0227, -0.5053, -0.0350, -0.2512,  1.4255,\n",
      "         -0.1636,  0.5531, -0.5897, -2.5042,  0.0384, -1.3395,  0.3955, -0.6018,\n",
      "         -2.2282, -0.3329,  1.4532, -0.4391, -0.1958, -0.0901, -0.1686,  0.6883],\n",
      "        [ 0.2243,  0.1015,  0.0889,  0.1195, -0.3144, -0.9611, -0.6124,  0.7772,\n",
      "         -0.0161,  0.5270, -0.1167, -1.0129, -0.1234, -1.0721,  0.9040, -0.1910,\n",
      "         -1.1881, -0.0590,  0.4514,  0.0465, -0.0036,  0.0751, -0.0507,  0.5567],\n",
      "        [ 0.5035,  0.1059,  0.0311,  0.1352, -0.7718, -0.6996, -1.1195, -0.0890,\n",
      "          0.3986,  0.4070, -0.4552, -2.5900, -0.1504, -0.9942,  0.5627, -0.3278,\n",
      "         -2.3803,  0.3631, -0.1510, -0.6930,  0.1704, -0.1730,  0.2871,  0.4628]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 1.5771, -0.9274, -0.3939,  1.1537, -0.8630,  0.9035,  0.9301,  0.5487,\n",
      "         0.9029,  0.0692,  1.2882,  1.2849,  0.1503,  1.0969, -0.6581,  0.8403,\n",
      "         0.5547, -0.1115, -0.6745,  1.2087,  1.0449,  0.4476, -0.2258,  0.7068,\n",
      "         0.9576,  0.5358, -1.0708, -1.2645, -0.6781,  0.1398],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0675,  1.3888, -0.2648, -1.4282, -0.0693, -1.6112, -1.3597, -0.0505,\n",
      "         -0.2258, -0.2782, -0.1781, -0.1145, -0.2376, -0.1910, -0.0676, -1.8500,\n",
      "          0.3070, -0.2874,  0.1920, -6.7032, -2.7476, -0.4001,  0.0671, -0.0934,\n",
      "         -0.1514,  0.3428,  6.1606,  0.1247, -0.2652, -0.0418],\n",
      "        [-0.5222, -0.0159, -0.4776,  0.0475,  4.1955, -0.2851,  0.4372, -1.4343,\n",
      "         -0.0406,  0.2780, -0.5157, -0.8050, -0.3463, -0.0766,  0.4095, -0.0340,\n",
      "          0.5696,  0.0546,  0.6784, -0.0341,  0.1189, -0.0319,  0.6685, -0.7606,\n",
      "         -5.8317,  0.1532,  0.1893,  1.2803,  0.9955,  0.1407],\n",
      "        [ 0.1530,  0.1450,  0.1091, -0.1897, -0.5568, -0.1130,  0.4238,  3.3121,\n",
      "         -0.5213, -1.1610,  0.6502,  0.4095,  1.8986, -0.5325, -1.0714,  0.8008,\n",
      "          0.3459,  0.3629, -2.5639,  0.1436, -0.0378,  0.3165, -0.9232,  3.2666,\n",
      "          1.7962, -0.1998, -0.0009, -0.8561, -1.3113, -0.9324],\n",
      "        [-0.0596, -6.3415, -0.3049,  5.2610,  0.2156,  4.3317,  5.6961, -0.2243,\n",
      "          0.4077,  0.1100,  0.1470, -0.0287, -0.0654,  0.4986,  0.4997,  0.5476,\n",
      "          0.3725,  0.1404, -0.2591,  1.0222,  2.1337, -0.2663,  0.2229, -0.0553,\n",
      "         -0.0258, -0.0347, -0.8659,  0.0101,  0.5153,  0.2547],\n",
      "        [ 0.2868, -0.0244, -0.4774, -0.0666,  0.0467, -0.0680,  0.0479, -0.0233,\n",
      "          4.2688,  0.1374, -0.1501,  0.0484, -0.1477,  4.1266,  0.0298, -0.1485,\n",
      "          2.5959, -0.3229,  0.0669, -0.0567,  0.1292,  2.3344, -0.1082, -0.0291,\n",
      "          0.0258,  0.4180,  0.2220,  0.1275, -0.0990,  0.1311],\n",
      "        [-9.2682,  0.2363,  0.6336, -0.2200,  0.3336,  0.2300,  0.0272, -0.3042,\n",
      "         -0.3413,  0.2231, -3.1250, -0.7303,  0.2272, -0.6510,  1.5810, -0.9934,\n",
      "         -0.7507,  0.3513,  0.1671,  0.3601,  0.2348, -0.9802,  0.0848, -0.0605,\n",
      "         -0.3645, -0.7151,  0.0995,  0.9526,  1.0109,  0.1431],\n",
      "        [-0.0326,  0.2651, -0.2673, -0.1706, -0.0365, -0.2845, -0.1343, -0.0148,\n",
      "         -0.0424, -0.0770, -0.0938,  0.1073, -0.1508, -0.5007,  0.0204, -2.6279,\n",
      "         -0.1296, -0.1688,  0.0597, -0.4801, -0.2205, -0.1207, -0.0096, -0.0662,\n",
      "         -0.0085,  0.1144,  0.7673,  0.0033, -0.0747,  0.1528],\n",
      "        [ 0.0438,  0.0924, -0.0540, -0.0486, -0.0549,  0.1364, -0.1733,  0.0965,\n",
      "          0.4033,  0.0243, -0.0453,  0.1303, -0.0649,  0.1927, -0.0706,  0.4553,\n",
      "          0.2836,  0.0620,  0.0410,  0.3485,  0.2065,  0.4823, -0.1175, -0.0396,\n",
      "          0.1281,  0.0785, -0.1629, -0.0713, -0.0207, -0.0356],\n",
      "        [-0.1405,  0.2457,  0.4573, -0.0783, -1.6670, -0.2981, -0.3234,  0.8408,\n",
      "         -0.3009, -2.1306,  0.0239,  0.2646,  1.0447, -0.2369, -0.0765, -0.2231,\n",
      "         -0.1684,  1.0556, -1.8034, -0.1599, -0.0823,  0.0334, -2.1887,  0.8993,\n",
      "          1.3025, -0.9314, -0.2349, -0.2886, -0.4081, -1.6937],\n",
      "        [ 0.0648, -0.0596,  2.1960, -0.1583, -0.0646,  0.0620, -0.3539,  0.0883,\n",
      "         -0.2454, -0.3517,  0.2198,  0.1485,  0.2765, -0.1462,  0.1130,  0.0798,\n",
      "         -0.1932,  1.4760, -0.2325, -0.0336,  0.0521, -0.3694, -0.1417,  0.2004,\n",
      "          0.0570, -2.0537, -0.2267, -0.0840, -0.1010, -0.5369],\n",
      "        [ 0.2983, -0.2890, -1.1217,  0.2124,  2.0090,  0.3180,  0.3217, -0.7893,\n",
      "          0.8529,  2.5933,  0.0758, -0.0513, -1.5496,  0.6042,  0.1623,  0.4940,\n",
      "          0.5030, -1.2749,  1.7219,  0.3958,  0.1426, -0.1770,  2.3809, -0.8320,\n",
      "         -1.1369,  1.3269,  0.4061,  0.3113,  0.1805,  1.4933],\n",
      "        [ 1.2182,  0.0082,  0.0257, -0.0801, -0.6003, -0.0391,  0.1756,  0.3074,\n",
      "         -0.0725, -0.2019,  4.1573,  4.7969,  0.2581, -0.2119, -2.0528,  0.7974,\n",
      "          0.4567,  0.1607, -0.1350,  0.3838,  0.3773,  0.2795,  0.0255, -0.0544,\n",
      "          0.5870, -0.1651, -0.3785, -4.1780, -0.3194, -0.2476],\n",
      "        [ 0.1770,  0.0094, -0.0932,  0.1046,  0.1183,  0.1285,  0.0158, -0.0203,\n",
      "          0.2152, -0.0838,  0.1225, -0.0207, -0.0225,  0.1721, -0.2369,  0.0493,\n",
      "          0.2306, -0.1555, -0.0797,  0.4483,  0.1711,  0.3173, -0.0128,  0.0191,\n",
      "          0.0679,  0.0849, -0.2140, -0.0973, -0.0193, -0.1897],\n",
      "        [-0.1015, -0.0377, -0.3494, -0.2675,  0.7448,  0.0180, -0.4343, -2.0404,\n",
      "          0.3749,  0.7083, -0.2375, -0.2058, -1.4823,  0.3267,  0.5415, -0.7175,\n",
      "          0.0272,  0.0210,  2.2841, -0.2270,  0.0699, -0.2963,  0.9007, -2.7261,\n",
      "         -0.7754,  0.1070,  0.1869,  0.7766,  1.0299,  0.9366],\n",
      "        [ 0.1328, -0.0978, -0.0813,  0.0022,  0.0774, -0.1473, -0.0787,  0.0356,\n",
      "          0.5507,  0.0822,  0.0496,  0.1704, -0.0356, -0.1509, -0.1499,  0.5276,\n",
      "          0.6223,  0.1679, -0.1017,  0.3952,  0.2244,  0.2833,  0.0952, -0.1938,\n",
      "          0.0846,  0.2001, -0.2784, -0.2147, -0.1025, -0.0983]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-3.6029, -0.4202,  0.8855,  2.8397, 10.3478, -4.5757, -9.6872, 12.1420,\n",
      "        -0.7472, -5.1968, -5.9622,  2.6708, 12.0150, -7.6268, 11.5561],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-13.7144,  11.7372, -13.3850,  12.7418,  14.2131,   8.8078, -14.9387,\n",
      "          16.8160, -14.3966, -13.5338,  -5.4230,  -8.4949,  16.2346,  -9.4051,\n",
      "          17.0120]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([16.9731], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer = torch.from_numpy(scaled_raw[:, 2:])\n",
    "outputs = model(inf.type(torch.DoubleTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(823.3343, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "tensor([162.3062], grad_fn=<ThAddBackward>) tensor([[191.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "inf = torch.from_numpy(scaled_raw[0,2:])\n",
    "target = torch.from_numpy(raw.target[0].reshape(1,1))\n",
    "output = model(inf.type(torch.FloatTensor))\n",
    "print(criterion(target.type(torch.DoubleTensor), output.type(torch.DoubleTensor)))\n",
    "print(output, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
