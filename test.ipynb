{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from src.visualization import cycle_plot, save_plot\n",
    "from src.data import LoadData, list_dataset, get_json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000e+00,  1.00000e+00, -7.00000e-04, -4.00000e-04,\n",
       "         1.00000e+02,  5.18670e+02,  6.41820e+02,  1.58970e+03,\n",
       "         1.40060e+03,  1.46200e+01,  2.16100e+01,  5.54360e+02,\n",
       "         2.38806e+03,  9.04619e+03,  1.30000e+00,  4.74700e+01,\n",
       "         5.21660e+02,  2.38802e+03,  8.13862e+03,  8.41950e+00,\n",
       "         3.00000e-02,  3.92000e+02,  2.38800e+03,  1.00000e+02,\n",
       "         3.90600e+01,  2.34190e+01],\n",
       "       [ 1.00000e+00,  2.00000e+00,  1.90000e-03, -3.00000e-04,\n",
       "         1.00000e+02,  5.18670e+02,  6.42150e+02,  1.59182e+03,\n",
       "         1.40314e+03,  1.46200e+01,  2.16100e+01,  5.53750e+02,\n",
       "         2.38804e+03,  9.04407e+03,  1.30000e+00,  4.74900e+01,\n",
       "         5.22280e+02,  2.38807e+03,  8.13149e+03,  8.43180e+00,\n",
       "         3.00000e-02,  3.92000e+02,  2.38800e+03,  1.00000e+02,\n",
       "         3.90000e+01,  2.34236e+01],\n",
       "       [ 1.00000e+00,  3.00000e+00, -4.30000e-03,  3.00000e-04,\n",
       "         1.00000e+02,  5.18670e+02,  6.42350e+02,  1.58799e+03,\n",
       "         1.40420e+03,  1.46200e+01,  2.16100e+01,  5.54260e+02,\n",
       "         2.38808e+03,  9.05294e+03,  1.30000e+00,  4.72700e+01,\n",
       "         5.22420e+02,  2.38803e+03,  8.13323e+03,  8.41780e+00,\n",
       "         3.00000e-02,  3.90000e+02,  2.38800e+03,  1.00000e+02,\n",
       "         3.89500e+01,  2.33442e+01],\n",
       "       [ 1.00000e+00,  4.00000e+00,  7.00000e-04,  0.00000e+00,\n",
       "         1.00000e+02,  5.18670e+02,  6.42350e+02,  1.58279e+03,\n",
       "         1.40187e+03,  1.46200e+01,  2.16100e+01,  5.54450e+02,\n",
       "         2.38811e+03,  9.04948e+03,  1.30000e+00,  4.71300e+01,\n",
       "         5.22860e+02,  2.38808e+03,  8.13383e+03,  8.36820e+00,\n",
       "         3.00000e-02,  3.92000e+02,  2.38800e+03,  1.00000e+02,\n",
       "         3.88800e+01,  2.33739e+01],\n",
       "       [ 1.00000e+00,  5.00000e+00, -1.90000e-03, -2.00000e-04,\n",
       "         1.00000e+02,  5.18670e+02,  6.42370e+02,  1.58285e+03,\n",
       "         1.40622e+03,  1.46200e+01,  2.16100e+01,  5.54000e+02,\n",
       "         2.38806e+03,  9.05515e+03,  1.30000e+00,  4.72800e+01,\n",
       "         5.22190e+02,  2.38804e+03,  8.13380e+03,  8.42940e+00,\n",
       "         3.00000e-02,  3.93000e+02,  2.38800e+03,  1.00000e+02,\n",
       "         3.89000e+01,  2.34044e+01]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featurenames = get_json('references/col_to_feat.json')\n",
    "train = LoadData('train_FD001.txt', sep='\\s+', names=featurenames)\n",
    "train.features[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train, scaler = train.standardize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    \"\"\"Neural Network Generator.\"\"\"\n",
    "    def __init__(self, input_size=24, hidden_sizes=[20,15], output_size=1, drop_p=0.4):\n",
    "        \"\"\"Generate fully-connected neural network.\n",
    "\n",
    "        parameters\n",
    "        ----------\n",
    "        input_size (int): size of the input\n",
    "        hidden_sizes (list of int): size of the hidden layers\n",
    "        output_layer (int): size of the output layer\n",
    "        drop_p (float): dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Linear(input_size, hidden_sizes[0])\n",
    "        ])\n",
    "        layers = zip(hidden_sizes[:-1], hidden_sizes[1:])\n",
    "        self.hidden_layers.extend([nn.Linear(h1,h2) for h1,h2 in layers])\n",
    "        self.output = nn.Linear(hidden_sizes[-1], output_size)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        for index, linear in enumerate(self.hidden_layers):\n",
    "            if index % 2 == 0:\n",
    "                X = linear(X)\n",
    "                X = self.dropout(X)\n",
    "            elif index % 2 != 0:\n",
    "                X = torch.tanh(linear(X))\n",
    "        \n",
    "        X = self.output(X)\n",
    "\n",
    "        return F.relu(X)\n",
    "#         return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    featureloader,\n",
    "    labelloader,\n",
    "    epochs=5,\n",
    "    print_every=40\n",
    "):\n",
    "    epoch_loss = 0\n",
    "    steps = 0\n",
    "    1\n",
    "    for epoch in range(epochs):\n",
    "        for features, labels in zip(featureloader, labelloader):\n",
    "            steps += 1\n",
    "            features, labels = features.type(torch.FloatTensor), labels.type(torch.FloatTensor)\n",
    "            labels.resize_(labels.shape[0], 1)\n",
    "            \n",
    "            output = model.forward(features)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            if steps % print_every == 0:\n",
    "                print('Epoch: {}/{}..'.format(epoch+1, epochs),\n",
    "                      'Training loss: {:.3f}'.format(epoch_loss/print_every))\n",
    "\n",
    "                epoch_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureset = torch.from_numpy(scaled_train[:,2:])\n",
    "labelset = torch.from_numpy(train.target)\n",
    "\n",
    "featureloader = torch.utils.data.DataLoader(featureset, batch_size=32)\n",
    "labelloader = torch.utils.data.DataLoader(labelset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/150.. Training loss: 15531.182\n",
      "Epoch: 1/150.. Training loss: 12517.763\n",
      "Epoch: 1/150.. Training loss: 8934.156\n",
      "Epoch: 1/150.. Training loss: 8416.421\n",
      "Epoch: 1/150.. Training loss: 10267.745\n",
      "Epoch: 1/150.. Training loss: 9529.734\n",
      "Epoch: 1/150.. Training loss: 10970.965\n",
      "Epoch: 1/150.. Training loss: 11785.343\n",
      "Epoch: 1/150.. Training loss: 10046.226\n",
      "Epoch: 1/150.. Training loss: 10937.389\n",
      "Epoch: 2/150.. Training loss: 10990.589\n",
      "Epoch: 2/150.. Training loss: 7333.386\n",
      "Epoch: 2/150.. Training loss: 5868.357\n",
      "Epoch: 2/150.. Training loss: 4328.906\n",
      "Epoch: 2/150.. Training loss: 3697.618\n",
      "Epoch: 2/150.. Training loss: 5199.400\n",
      "Epoch: 2/150.. Training loss: 5354.514\n",
      "Epoch: 2/150.. Training loss: 9662.640\n",
      "Epoch: 2/150.. Training loss: 3717.498\n",
      "Epoch: 2/150.. Training loss: 6203.951\n",
      "Epoch: 2/150.. Training loss: 9046.713\n",
      "Epoch: 3/150.. Training loss: 4285.361\n",
      "Epoch: 3/150.. Training loss: 2885.899\n",
      "Epoch: 3/150.. Training loss: 3267.372\n",
      "Epoch: 3/150.. Training loss: 2388.896\n",
      "Epoch: 3/150.. Training loss: 1715.088\n",
      "Epoch: 3/150.. Training loss: 3145.484\n",
      "Epoch: 3/150.. Training loss: 2689.725\n",
      "Epoch: 3/150.. Training loss: 6812.796\n",
      "Epoch: 3/150.. Training loss: 2195.580\n",
      "Epoch: 3/150.. Training loss: 3856.910\n",
      "Epoch: 3/150.. Training loss: 6207.468\n",
      "Epoch: 4/150.. Training loss: 3004.581\n",
      "Epoch: 4/150.. Training loss: 2042.804\n",
      "Epoch: 4/150.. Training loss: 1541.807\n",
      "Epoch: 4/150.. Training loss: 1202.790\n",
      "Epoch: 4/150.. Training loss: 1727.296\n",
      "Epoch: 4/150.. Training loss: 2097.240\n",
      "Epoch: 4/150.. Training loss: 2316.893\n",
      "Epoch: 4/150.. Training loss: 4514.373\n",
      "Epoch: 4/150.. Training loss: 2271.739\n",
      "Epoch: 4/150.. Training loss: 3717.578\n",
      "Epoch: 4/150.. Training loss: 3274.126\n",
      "Epoch: 5/150.. Training loss: 2140.089\n",
      "Epoch: 5/150.. Training loss: 1709.662\n",
      "Epoch: 5/150.. Training loss: 1112.971\n",
      "Epoch: 5/150.. Training loss: 1173.675\n",
      "Epoch: 5/150.. Training loss: 1405.153\n",
      "Epoch: 5/150.. Training loss: 1889.055\n",
      "Epoch: 5/150.. Training loss: 2700.891\n",
      "Epoch: 5/150.. Training loss: 3112.916\n",
      "Epoch: 5/150.. Training loss: 2166.383\n",
      "Epoch: 5/150.. Training loss: 3145.867\n",
      "Epoch: 6/150.. Training loss: 3093.239\n",
      "Epoch: 6/150.. Training loss: 1371.451\n",
      "Epoch: 6/150.. Training loss: 1578.239\n",
      "Epoch: 6/150.. Training loss: 1096.219\n",
      "Epoch: 6/150.. Training loss: 960.223\n",
      "Epoch: 6/150.. Training loss: 1179.554\n",
      "Epoch: 6/150.. Training loss: 1924.927\n",
      "Epoch: 6/150.. Training loss: 4048.268\n",
      "Epoch: 6/150.. Training loss: 1387.023\n",
      "Epoch: 6/150.. Training loss: 2222.787\n",
      "Epoch: 6/150.. Training loss: 3971.645\n",
      "Epoch: 7/150.. Training loss: 1750.959\n",
      "Epoch: 7/150.. Training loss: 953.380\n",
      "Epoch: 7/150.. Training loss: 1562.769\n",
      "Epoch: 7/150.. Training loss: 1148.246\n",
      "Epoch: 7/150.. Training loss: 894.216\n",
      "Epoch: 7/150.. Training loss: 1067.534\n",
      "Epoch: 7/150.. Training loss: 1933.463\n",
      "Epoch: 7/150.. Training loss: 3816.107\n",
      "Epoch: 7/150.. Training loss: 1409.739\n",
      "Epoch: 7/150.. Training loss: 2059.590\n",
      "Epoch: 7/150.. Training loss: 3697.608\n",
      "Epoch: 8/150.. Training loss: 1718.185\n",
      "Epoch: 8/150.. Training loss: 1308.370\n",
      "Epoch: 8/150.. Training loss: 1041.116\n",
      "Epoch: 8/150.. Training loss: 941.256\n",
      "Epoch: 8/150.. Training loss: 1175.378\n",
      "Epoch: 8/150.. Training loss: 1187.661\n",
      "Epoch: 8/150.. Training loss: 2236.193\n",
      "Epoch: 8/150.. Training loss: 3570.959\n",
      "Epoch: 8/150.. Training loss: 1541.513\n",
      "Epoch: 8/150.. Training loss: 2892.480\n",
      "Epoch: 8/150.. Training loss: 2436.385\n",
      "Epoch: 9/150.. Training loss: 1511.932\n",
      "Epoch: 9/150.. Training loss: 1297.154\n",
      "Epoch: 9/150.. Training loss: 1086.753\n",
      "Epoch: 9/150.. Training loss: 1086.289\n",
      "Epoch: 9/150.. Training loss: 1011.556\n",
      "Epoch: 9/150.. Training loss: 1610.874\n",
      "Epoch: 9/150.. Training loss: 2587.963\n",
      "Epoch: 9/150.. Training loss: 2554.320\n",
      "Epoch: 9/150.. Training loss: 1917.197\n",
      "Epoch: 9/150.. Training loss: 2731.393\n",
      "Epoch: 10/150.. Training loss: 2555.069\n",
      "Epoch: 10/150.. Training loss: 1073.413\n",
      "Epoch: 10/150.. Training loss: 1373.968\n",
      "Epoch: 10/150.. Training loss: 1126.064\n",
      "Epoch: 10/150.. Training loss: 878.699\n",
      "Epoch: 10/150.. Training loss: 972.771\n",
      "Epoch: 10/150.. Training loss: 1719.639\n",
      "Epoch: 10/150.. Training loss: 3879.039\n",
      "Epoch: 10/150.. Training loss: 1346.169\n",
      "Epoch: 10/150.. Training loss: 2053.780\n",
      "Epoch: 10/150.. Training loss: 3392.073\n",
      "Epoch: 11/150.. Training loss: 1673.345\n",
      "Epoch: 11/150.. Training loss: 812.261\n",
      "Epoch: 11/150.. Training loss: 1476.873\n",
      "Epoch: 11/150.. Training loss: 1149.829\n",
      "Epoch: 11/150.. Training loss: 876.713\n",
      "Epoch: 11/150.. Training loss: 871.782\n",
      "Epoch: 11/150.. Training loss: 1942.997\n",
      "Epoch: 11/150.. Training loss: 3840.579\n",
      "Epoch: 11/150.. Training loss: 1465.367\n",
      "Epoch: 11/150.. Training loss: 1935.900\n",
      "Epoch: 11/150.. Training loss: 3352.527\n",
      "Epoch: 12/150.. Training loss: 1700.237\n",
      "Epoch: 12/150.. Training loss: 1230.264\n",
      "Epoch: 12/150.. Training loss: 1013.278\n",
      "Epoch: 12/150.. Training loss: 965.159\n",
      "Epoch: 12/150.. Training loss: 1124.895\n",
      "Epoch: 12/150.. Training loss: 1125.032\n",
      "Epoch: 12/150.. Training loss: 2292.171\n",
      "Epoch: 12/150.. Training loss: 3441.253\n",
      "Epoch: 12/150.. Training loss: 1452.050\n",
      "Epoch: 12/150.. Training loss: 2885.913\n",
      "Epoch: 12/150.. Training loss: 2333.138\n",
      "Epoch: 13/150.. Training loss: 1431.375\n",
      "Epoch: 13/150.. Training loss: 1235.654\n",
      "Epoch: 13/150.. Training loss: 1112.057\n",
      "Epoch: 13/150.. Training loss: 1115.459\n",
      "Epoch: 13/150.. Training loss: 1000.840\n",
      "Epoch: 13/150.. Training loss: 1595.287\n",
      "Epoch: 13/150.. Training loss: 2704.255\n",
      "Epoch: 13/150.. Training loss: 2518.411\n",
      "Epoch: 13/150.. Training loss: 1876.601\n",
      "Epoch: 13/150.. Training loss: 2715.769\n",
      "Epoch: 14/150.. Training loss: 2382.480\n",
      "Epoch: 14/150.. Training loss: 997.467\n",
      "Epoch: 14/150.. Training loss: 1312.037\n",
      "Epoch: 14/150.. Training loss: 1144.223\n",
      "Epoch: 14/150.. Training loss: 905.264\n",
      "Epoch: 14/150.. Training loss: 914.854\n",
      "Epoch: 14/150.. Training loss: 1715.071\n",
      "Epoch: 14/150.. Training loss: 3877.469\n",
      "Epoch: 14/150.. Training loss: 1401.438\n",
      "Epoch: 14/150.. Training loss: 2029.157\n",
      "Epoch: 14/150.. Training loss: 3286.431\n",
      "Epoch: 15/150.. Training loss: 1700.993\n",
      "Epoch: 15/150.. Training loss: 817.414\n",
      "Epoch: 15/150.. Training loss: 1480.355\n",
      "Epoch: 15/150.. Training loss: 1147.771\n",
      "Epoch: 15/150.. Training loss: 870.882\n",
      "Epoch: 15/150.. Training loss: 820.689\n",
      "Epoch: 15/150.. Training loss: 1955.683\n",
      "Epoch: 15/150.. Training loss: 3837.105\n",
      "Epoch: 15/150.. Training loss: 1450.430\n",
      "Epoch: 15/150.. Training loss: 1920.086\n",
      "Epoch: 15/150.. Training loss: 3352.124\n",
      "Epoch: 16/150.. Training loss: 1669.768\n",
      "Epoch: 16/150.. Training loss: 1230.677\n",
      "Epoch: 16/150.. Training loss: 1017.350\n",
      "Epoch: 16/150.. Training loss: 993.619\n",
      "Epoch: 16/150.. Training loss: 1064.626\n",
      "Epoch: 16/150.. Training loss: 1068.917\n",
      "Epoch: 16/150.. Training loss: 2257.093\n",
      "Epoch: 16/150.. Training loss: 3352.728\n",
      "Epoch: 16/150.. Training loss: 1439.310\n",
      "Epoch: 16/150.. Training loss: 2883.291\n",
      "Epoch: 16/150.. Training loss: 2309.869\n",
      "Epoch: 17/150.. Training loss: 1401.844\n",
      "Epoch: 17/150.. Training loss: 1252.731\n",
      "Epoch: 17/150.. Training loss: 1089.494\n",
      "Epoch: 17/150.. Training loss: 1055.667\n",
      "Epoch: 17/150.. Training loss: 1039.559\n",
      "Epoch: 17/150.. Training loss: 1599.908\n",
      "Epoch: 17/150.. Training loss: 2748.985\n",
      "Epoch: 17/150.. Training loss: 2531.740\n",
      "Epoch: 17/150.. Training loss: 1785.560\n",
      "Epoch: 17/150.. Training loss: 2717.126\n",
      "Epoch: 18/150.. Training loss: 2292.763\n",
      "Epoch: 18/150.. Training loss: 1007.877\n",
      "Epoch: 18/150.. Training loss: 1353.605\n",
      "Epoch: 18/150.. Training loss: 1159.224\n",
      "Epoch: 18/150.. Training loss: 882.792\n",
      "Epoch: 18/150.. Training loss: 916.717\n",
      "Epoch: 18/150.. Training loss: 1695.397\n",
      "Epoch: 18/150.. Training loss: 4062.644\n",
      "Epoch: 18/150.. Training loss: 1363.737\n",
      "Epoch: 18/150.. Training loss: 1998.264\n",
      "Epoch: 18/150.. Training loss: 3340.565\n",
      "Epoch: 19/150.. Training loss: 1689.306\n",
      "Epoch: 19/150.. Training loss: 822.079\n",
      "Epoch: 19/150.. Training loss: 1455.524\n",
      "Epoch: 19/150.. Training loss: 1122.864\n",
      "Epoch: 19/150.. Training loss: 901.866\n",
      "Epoch: 19/150.. Training loss: 827.928\n",
      "Epoch: 19/150.. Training loss: 1923.261\n",
      "Epoch: 19/150.. Training loss: 4023.453\n",
      "Epoch: 19/150.. Training loss: 1528.126\n",
      "Epoch: 19/150.. Training loss: 1929.990\n",
      "Epoch: 19/150.. Training loss: 3289.155\n",
      "Epoch: 20/150.. Training loss: 1713.595\n",
      "Epoch: 20/150.. Training loss: 1216.904\n",
      "Epoch: 20/150.. Training loss: 1000.936\n",
      "Epoch: 20/150.. Training loss: 964.057\n",
      "Epoch: 20/150.. Training loss: 1136.489\n",
      "Epoch: 20/150.. Training loss: 1081.521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/150.. Training loss: 2303.453\n",
      "Epoch: 20/150.. Training loss: 3430.868\n",
      "Epoch: 20/150.. Training loss: 1410.082\n",
      "Epoch: 20/150.. Training loss: 2956.633\n",
      "Epoch: 20/150.. Training loss: 2254.077\n",
      "Epoch: 21/150.. Training loss: 1426.261\n",
      "Epoch: 21/150.. Training loss: 1222.212\n",
      "Epoch: 21/150.. Training loss: 1110.459\n",
      "Epoch: 21/150.. Training loss: 1086.684\n",
      "Epoch: 21/150.. Training loss: 1022.481\n",
      "Epoch: 21/150.. Training loss: 1553.831\n",
      "Epoch: 21/150.. Training loss: 2688.191\n",
      "Epoch: 21/150.. Training loss: 2662.355\n",
      "Epoch: 21/150.. Training loss: 1788.234\n",
      "Epoch: 21/150.. Training loss: 2820.229\n",
      "Epoch: 22/150.. Training loss: 2315.658\n",
      "Epoch: 22/150.. Training loss: 1034.271\n",
      "Epoch: 22/150.. Training loss: 1319.971\n",
      "Epoch: 22/150.. Training loss: 1190.979\n",
      "Epoch: 22/150.. Training loss: 841.845\n",
      "Epoch: 22/150.. Training loss: 926.436\n",
      "Epoch: 22/150.. Training loss: 1671.119\n",
      "Epoch: 22/150.. Training loss: 4139.507\n",
      "Epoch: 22/150.. Training loss: 1279.267\n",
      "Epoch: 22/150.. Training loss: 2001.446\n",
      "Epoch: 22/150.. Training loss: 3395.108\n",
      "Epoch: 23/150.. Training loss: 1652.033\n",
      "Epoch: 23/150.. Training loss: 792.043\n",
      "Epoch: 23/150.. Training loss: 1408.433\n",
      "Epoch: 23/150.. Training loss: 1223.585\n",
      "Epoch: 23/150.. Training loss: 852.719\n",
      "Epoch: 23/150.. Training loss: 854.013\n",
      "Epoch: 23/150.. Training loss: 1845.979\n",
      "Epoch: 23/150.. Training loss: 3974.523\n",
      "Epoch: 23/150.. Training loss: 1374.484\n",
      "Epoch: 23/150.. Training loss: 1952.767\n",
      "Epoch: 23/150.. Training loss: 3315.525\n",
      "Epoch: 24/150.. Training loss: 1560.729\n",
      "Epoch: 24/150.. Training loss: 1225.973\n",
      "Epoch: 24/150.. Training loss: 1046.449\n",
      "Epoch: 24/150.. Training loss: 992.960\n",
      "Epoch: 24/150.. Training loss: 1088.095\n",
      "Epoch: 24/150.. Training loss: 1052.619\n",
      "Epoch: 24/150.. Training loss: 2358.310\n",
      "Epoch: 24/150.. Training loss: 3375.668\n",
      "Epoch: 24/150.. Training loss: 1411.553\n",
      "Epoch: 24/150.. Training loss: 2806.382\n",
      "Epoch: 24/150.. Training loss: 2278.873\n",
      "Epoch: 25/150.. Training loss: 1388.278\n",
      "Epoch: 25/150.. Training loss: 1192.417\n",
      "Epoch: 25/150.. Training loss: 1074.629\n",
      "Epoch: 25/150.. Training loss: 1087.597\n",
      "Epoch: 25/150.. Training loss: 919.230\n",
      "Epoch: 25/150.. Training loss: 1593.474\n",
      "Epoch: 25/150.. Training loss: 2681.223\n",
      "Epoch: 25/150.. Training loss: 2521.213\n",
      "Epoch: 25/150.. Training loss: 1803.779\n",
      "Epoch: 25/150.. Training loss: 2705.414\n",
      "Epoch: 26/150.. Training loss: 2290.500\n",
      "Epoch: 26/150.. Training loss: 1036.358\n",
      "Epoch: 26/150.. Training loss: 1406.417\n",
      "Epoch: 26/150.. Training loss: 1138.120\n",
      "Epoch: 26/150.. Training loss: 882.973\n",
      "Epoch: 26/150.. Training loss: 898.470\n",
      "Epoch: 26/150.. Training loss: 1710.927\n",
      "Epoch: 26/150.. Training loss: 4267.976\n",
      "Epoch: 26/150.. Training loss: 1283.864\n",
      "Epoch: 26/150.. Training loss: 2010.403\n",
      "Epoch: 26/150.. Training loss: 3213.673\n",
      "Epoch: 27/150.. Training loss: 1699.418\n",
      "Epoch: 27/150.. Training loss: 767.373\n",
      "Epoch: 27/150.. Training loss: 1439.985\n",
      "Epoch: 27/150.. Training loss: 1134.337\n",
      "Epoch: 27/150.. Training loss: 888.905\n",
      "Epoch: 27/150.. Training loss: 829.474\n",
      "Epoch: 27/150.. Training loss: 1879.928\n",
      "Epoch: 27/150.. Training loss: 3994.274\n",
      "Epoch: 27/150.. Training loss: 1404.496\n",
      "Epoch: 27/150.. Training loss: 1926.734\n",
      "Epoch: 27/150.. Training loss: 3334.274\n",
      "Epoch: 28/150.. Training loss: 1525.374\n",
      "Epoch: 28/150.. Training loss: 1226.553\n",
      "Epoch: 28/150.. Training loss: 1003.124\n",
      "Epoch: 28/150.. Training loss: 995.646\n",
      "Epoch: 28/150.. Training loss: 1070.734\n",
      "Epoch: 28/150.. Training loss: 1096.266\n",
      "Epoch: 28/150.. Training loss: 2248.956\n",
      "Epoch: 28/150.. Training loss: 3555.573\n",
      "Epoch: 28/150.. Training loss: 1434.171\n",
      "Epoch: 28/150.. Training loss: 2876.900\n",
      "Epoch: 28/150.. Training loss: 2317.322\n",
      "Epoch: 29/150.. Training loss: 1351.812\n",
      "Epoch: 29/150.. Training loss: 1222.248\n",
      "Epoch: 29/150.. Training loss: 1110.832\n",
      "Epoch: 29/150.. Training loss: 1069.071\n",
      "Epoch: 29/150.. Training loss: 968.325\n",
      "Epoch: 29/150.. Training loss: 1568.121\n",
      "Epoch: 29/150.. Training loss: 2708.491\n",
      "Epoch: 29/150.. Training loss: 2607.187\n",
      "Epoch: 29/150.. Training loss: 1788.128\n",
      "Epoch: 29/150.. Training loss: 2826.584\n",
      "Epoch: 30/150.. Training loss: 2436.934\n",
      "Epoch: 30/150.. Training loss: 1034.888\n",
      "Epoch: 30/150.. Training loss: 1340.197\n",
      "Epoch: 30/150.. Training loss: 1165.509\n",
      "Epoch: 30/150.. Training loss: 864.287\n",
      "Epoch: 30/150.. Training loss: 879.682\n",
      "Epoch: 30/150.. Training loss: 1699.814\n",
      "Epoch: 30/150.. Training loss: 4145.547\n",
      "Epoch: 30/150.. Training loss: 1328.746\n",
      "Epoch: 30/150.. Training loss: 1972.531\n",
      "Epoch: 30/150.. Training loss: 3247.499\n",
      "Epoch: 31/150.. Training loss: 1627.612\n",
      "Epoch: 31/150.. Training loss: 802.088\n",
      "Epoch: 31/150.. Training loss: 1449.418\n",
      "Epoch: 31/150.. Training loss: 1172.637\n",
      "Epoch: 31/150.. Training loss: 873.380\n",
      "Epoch: 31/150.. Training loss: 770.584\n",
      "Epoch: 31/150.. Training loss: 1913.144\n",
      "Epoch: 31/150.. Training loss: 3977.852\n",
      "Epoch: 31/150.. Training loss: 1427.373\n",
      "Epoch: 31/150.. Training loss: 1945.468\n",
      "Epoch: 31/150.. Training loss: 3206.421\n",
      "Epoch: 32/150.. Training loss: 1491.268\n",
      "Epoch: 32/150.. Training loss: 1221.441\n",
      "Epoch: 32/150.. Training loss: 1014.151\n",
      "Epoch: 32/150.. Training loss: 1038.606\n",
      "Epoch: 32/150.. Training loss: 1088.677\n",
      "Epoch: 32/150.. Training loss: 1062.853\n",
      "Epoch: 32/150.. Training loss: 2287.923\n",
      "Epoch: 32/150.. Training loss: 3387.519\n",
      "Epoch: 32/150.. Training loss: 1379.987\n",
      "Epoch: 32/150.. Training loss: 2863.808\n",
      "Epoch: 32/150.. Training loss: 2213.543\n",
      "Epoch: 33/150.. Training loss: 1320.875\n",
      "Epoch: 33/150.. Training loss: 1252.091\n",
      "Epoch: 33/150.. Training loss: 1099.324\n",
      "Epoch: 33/150.. Training loss: 1070.029\n",
      "Epoch: 33/150.. Training loss: 939.341\n",
      "Epoch: 33/150.. Training loss: 1571.610\n",
      "Epoch: 33/150.. Training loss: 2798.576\n",
      "Epoch: 33/150.. Training loss: 2603.260\n",
      "Epoch: 33/150.. Training loss: 1745.744\n",
      "Epoch: 33/150.. Training loss: 2811.415\n",
      "Epoch: 34/150.. Training loss: 2232.696\n",
      "Epoch: 34/150.. Training loss: 977.137\n",
      "Epoch: 34/150.. Training loss: 1340.084\n",
      "Epoch: 34/150.. Training loss: 1147.977\n",
      "Epoch: 34/150.. Training loss: 916.626\n",
      "Epoch: 34/150.. Training loss: 890.125\n",
      "Epoch: 34/150.. Training loss: 1739.307\n",
      "Epoch: 34/150.. Training loss: 4109.444\n",
      "Epoch: 34/150.. Training loss: 1324.531\n",
      "Epoch: 34/150.. Training loss: 1969.092\n",
      "Epoch: 34/150.. Training loss: 3215.529\n",
      "Epoch: 35/150.. Training loss: 1646.886\n",
      "Epoch: 35/150.. Training loss: 805.545\n",
      "Epoch: 35/150.. Training loss: 1444.676\n",
      "Epoch: 35/150.. Training loss: 1100.926\n",
      "Epoch: 35/150.. Training loss: 898.640\n",
      "Epoch: 35/150.. Training loss: 790.873\n",
      "Epoch: 35/150.. Training loss: 1936.780\n",
      "Epoch: 35/150.. Training loss: 3946.213\n",
      "Epoch: 35/150.. Training loss: 1451.496\n",
      "Epoch: 35/150.. Training loss: 1869.166\n",
      "Epoch: 35/150.. Training loss: 3300.835\n",
      "Epoch: 36/150.. Training loss: 1557.138\n",
      "Epoch: 36/150.. Training loss: 1210.830\n",
      "Epoch: 36/150.. Training loss: 996.178\n",
      "Epoch: 36/150.. Training loss: 975.522\n",
      "Epoch: 36/150.. Training loss: 1094.511\n",
      "Epoch: 36/150.. Training loss: 1036.225\n",
      "Epoch: 36/150.. Training loss: 2279.533\n",
      "Epoch: 36/150.. Training loss: 3443.949\n",
      "Epoch: 36/150.. Training loss: 1389.202\n",
      "Epoch: 36/150.. Training loss: 2843.332\n",
      "Epoch: 36/150.. Training loss: 2288.508\n",
      "Epoch: 37/150.. Training loss: 1337.213\n",
      "Epoch: 37/150.. Training loss: 1212.935\n",
      "Epoch: 37/150.. Training loss: 1083.394\n",
      "Epoch: 37/150.. Training loss: 1059.308\n",
      "Epoch: 37/150.. Training loss: 962.645\n",
      "Epoch: 37/150.. Training loss: 1591.642\n",
      "Epoch: 37/150.. Training loss: 2726.503\n",
      "Epoch: 37/150.. Training loss: 2614.906\n",
      "Epoch: 37/150.. Training loss: 1741.974\n",
      "Epoch: 37/150.. Training loss: 2768.169\n",
      "Epoch: 38/150.. Training loss: 2248.184\n",
      "Epoch: 38/150.. Training loss: 1016.255\n",
      "Epoch: 38/150.. Training loss: 1314.334\n",
      "Epoch: 38/150.. Training loss: 1115.742\n",
      "Epoch: 38/150.. Training loss: 880.004\n",
      "Epoch: 38/150.. Training loss: 895.295\n",
      "Epoch: 38/150.. Training loss: 1687.896\n",
      "Epoch: 38/150.. Training loss: 4140.423\n",
      "Epoch: 38/150.. Training loss: 1270.933\n",
      "Epoch: 38/150.. Training loss: 1956.003\n",
      "Epoch: 38/150.. Training loss: 3189.145\n",
      "Epoch: 39/150.. Training loss: 1616.569\n",
      "Epoch: 39/150.. Training loss: 798.068\n",
      "Epoch: 39/150.. Training loss: 1437.092\n",
      "Epoch: 39/150.. Training loss: 1177.238\n",
      "Epoch: 39/150.. Training loss: 879.855\n",
      "Epoch: 39/150.. Training loss: 796.377\n",
      "Epoch: 39/150.. Training loss: 1822.902\n",
      "Epoch: 39/150.. Training loss: 4011.943\n",
      "Epoch: 39/150.. Training loss: 1430.871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39/150.. Training loss: 1905.247\n",
      "Epoch: 39/150.. Training loss: 3220.176\n",
      "Epoch: 40/150.. Training loss: 1619.721\n",
      "Epoch: 40/150.. Training loss: 1274.412\n",
      "Epoch: 40/150.. Training loss: 1051.775\n",
      "Epoch: 40/150.. Training loss: 1080.373\n",
      "Epoch: 40/150.. Training loss: 1066.366\n",
      "Epoch: 40/150.. Training loss: 1045.725\n",
      "Epoch: 40/150.. Training loss: 2309.104\n",
      "Epoch: 40/150.. Training loss: 3548.723\n",
      "Epoch: 40/150.. Training loss: 1378.170\n",
      "Epoch: 40/150.. Training loss: 2916.951\n",
      "Epoch: 40/150.. Training loss: 2211.051\n",
      "Epoch: 41/150.. Training loss: 1353.567\n",
      "Epoch: 41/150.. Training loss: 1209.752\n",
      "Epoch: 41/150.. Training loss: 1100.046\n",
      "Epoch: 41/150.. Training loss: 1072.185\n",
      "Epoch: 41/150.. Training loss: 949.391\n",
      "Epoch: 41/150.. Training loss: 1582.376\n",
      "Epoch: 41/150.. Training loss: 2779.319\n",
      "Epoch: 41/150.. Training loss: 2550.607\n",
      "Epoch: 41/150.. Training loss: 1758.841\n",
      "Epoch: 41/150.. Training loss: 2737.826\n",
      "Epoch: 42/150.. Training loss: 2289.548\n",
      "Epoch: 42/150.. Training loss: 1007.101\n",
      "Epoch: 42/150.. Training loss: 1307.326\n",
      "Epoch: 42/150.. Training loss: 1117.132\n",
      "Epoch: 42/150.. Training loss: 874.983\n",
      "Epoch: 42/150.. Training loss: 912.853\n",
      "Epoch: 42/150.. Training loss: 1655.763\n",
      "Epoch: 42/150.. Training loss: 4064.839\n",
      "Epoch: 42/150.. Training loss: 1253.083\n",
      "Epoch: 42/150.. Training loss: 1966.520\n",
      "Epoch: 42/150.. Training loss: 3207.795\n",
      "Epoch: 43/150.. Training loss: 1687.238\n",
      "Epoch: 43/150.. Training loss: 774.980\n",
      "Epoch: 43/150.. Training loss: 1415.237\n",
      "Epoch: 43/150.. Training loss: 1212.752\n",
      "Epoch: 43/150.. Training loss: 865.008\n",
      "Epoch: 43/150.. Training loss: 768.758\n",
      "Epoch: 43/150.. Training loss: 1897.207\n",
      "Epoch: 43/150.. Training loss: 3992.865\n",
      "Epoch: 43/150.. Training loss: 1376.645\n",
      "Epoch: 43/150.. Training loss: 1874.266\n",
      "Epoch: 43/150.. Training loss: 3370.217\n",
      "Epoch: 44/150.. Training loss: 1557.068\n",
      "Epoch: 44/150.. Training loss: 1297.631\n",
      "Epoch: 44/150.. Training loss: 1039.910\n",
      "Epoch: 44/150.. Training loss: 1057.399\n",
      "Epoch: 44/150.. Training loss: 1088.485\n",
      "Epoch: 44/150.. Training loss: 1022.686\n",
      "Epoch: 44/150.. Training loss: 2339.437\n",
      "Epoch: 44/150.. Training loss: 3572.216\n",
      "Epoch: 44/150.. Training loss: 1369.778\n",
      "Epoch: 44/150.. Training loss: 2947.991\n",
      "Epoch: 44/150.. Training loss: 2232.829\n",
      "Epoch: 45/150.. Training loss: 1312.304\n",
      "Epoch: 45/150.. Training loss: 1253.850\n",
      "Epoch: 45/150.. Training loss: 1104.780\n",
      "Epoch: 45/150.. Training loss: 1085.860\n",
      "Epoch: 45/150.. Training loss: 923.209\n",
      "Epoch: 45/150.. Training loss: 1552.535\n",
      "Epoch: 45/150.. Training loss: 2777.441\n",
      "Epoch: 45/150.. Training loss: 2525.665\n",
      "Epoch: 45/150.. Training loss: 1781.460\n",
      "Epoch: 45/150.. Training loss: 2807.847\n",
      "Epoch: 46/150.. Training loss: 2231.116\n",
      "Epoch: 46/150.. Training loss: 978.398\n",
      "Epoch: 46/150.. Training loss: 1362.068\n",
      "Epoch: 46/150.. Training loss: 1104.727\n",
      "Epoch: 46/150.. Training loss: 868.984\n",
      "Epoch: 46/150.. Training loss: 871.261\n",
      "Epoch: 46/150.. Training loss: 1673.689\n",
      "Epoch: 46/150.. Training loss: 4212.115\n",
      "Epoch: 46/150.. Training loss: 1236.076\n",
      "Epoch: 46/150.. Training loss: 1987.984\n",
      "Epoch: 46/150.. Training loss: 3189.470\n",
      "Epoch: 47/150.. Training loss: 1630.110\n",
      "Epoch: 47/150.. Training loss: 768.602\n",
      "Epoch: 47/150.. Training loss: 1396.165\n",
      "Epoch: 47/150.. Training loss: 1153.563\n",
      "Epoch: 47/150.. Training loss: 883.836\n",
      "Epoch: 47/150.. Training loss: 813.857\n",
      "Epoch: 47/150.. Training loss: 1872.002\n",
      "Epoch: 47/150.. Training loss: 3943.212\n",
      "Epoch: 47/150.. Training loss: 1434.505\n",
      "Epoch: 47/150.. Training loss: 1872.333\n",
      "Epoch: 47/150.. Training loss: 3330.728\n",
      "Epoch: 48/150.. Training loss: 1566.891\n",
      "Epoch: 48/150.. Training loss: 1261.453\n",
      "Epoch: 48/150.. Training loss: 1036.410\n",
      "Epoch: 48/150.. Training loss: 1053.821\n",
      "Epoch: 48/150.. Training loss: 1084.846\n",
      "Epoch: 48/150.. Training loss: 1033.859\n",
      "Epoch: 48/150.. Training loss: 2342.382\n",
      "Epoch: 48/150.. Training loss: 3498.218\n",
      "Epoch: 48/150.. Training loss: 1384.511\n",
      "Epoch: 48/150.. Training loss: 2964.387\n",
      "Epoch: 48/150.. Training loss: 2355.616\n",
      "Epoch: 49/150.. Training loss: 1304.600\n",
      "Epoch: 49/150.. Training loss: 1198.484\n",
      "Epoch: 49/150.. Training loss: 1111.056\n",
      "Epoch: 49/150.. Training loss: 1042.872\n",
      "Epoch: 49/150.. Training loss: 979.722\n",
      "Epoch: 49/150.. Training loss: 1560.833\n",
      "Epoch: 49/150.. Training loss: 2724.332\n",
      "Epoch: 49/150.. Training loss: 2532.609\n",
      "Epoch: 49/150.. Training loss: 1699.461\n",
      "Epoch: 49/150.. Training loss: 2874.313\n",
      "Epoch: 50/150.. Training loss: 2374.144\n",
      "Epoch: 50/150.. Training loss: 1026.378\n",
      "Epoch: 50/150.. Training loss: 1280.687\n",
      "Epoch: 50/150.. Training loss: 1227.642\n",
      "Epoch: 50/150.. Training loss: 831.285\n",
      "Epoch: 50/150.. Training loss: 941.795\n",
      "Epoch: 50/150.. Training loss: 1674.867\n",
      "Epoch: 50/150.. Training loss: 4268.382\n",
      "Epoch: 50/150.. Training loss: 1221.537\n",
      "Epoch: 50/150.. Training loss: 1977.389\n",
      "Epoch: 50/150.. Training loss: 3330.503\n",
      "Epoch: 51/150.. Training loss: 1628.505\n",
      "Epoch: 51/150.. Training loss: 828.344\n",
      "Epoch: 51/150.. Training loss: 1405.782\n",
      "Epoch: 51/150.. Training loss: 1168.255\n",
      "Epoch: 51/150.. Training loss: 888.576\n",
      "Epoch: 51/150.. Training loss: 788.893\n",
      "Epoch: 51/150.. Training loss: 1823.235\n",
      "Epoch: 51/150.. Training loss: 4067.044\n",
      "Epoch: 51/150.. Training loss: 1378.448\n",
      "Epoch: 51/150.. Training loss: 1867.981\n",
      "Epoch: 51/150.. Training loss: 3573.933\n",
      "Epoch: 52/150.. Training loss: 1502.820\n",
      "Epoch: 52/150.. Training loss: 1263.186\n",
      "Epoch: 52/150.. Training loss: 1008.328\n",
      "Epoch: 52/150.. Training loss: 1062.621\n",
      "Epoch: 52/150.. Training loss: 1093.187\n",
      "Epoch: 52/150.. Training loss: 1059.518\n",
      "Epoch: 52/150.. Training loss: 2258.088\n",
      "Epoch: 52/150.. Training loss: 3596.103\n",
      "Epoch: 52/150.. Training loss: 1352.291\n",
      "Epoch: 52/150.. Training loss: 2961.606\n",
      "Epoch: 52/150.. Training loss: 2301.213\n",
      "Epoch: 53/150.. Training loss: 1279.485\n",
      "Epoch: 53/150.. Training loss: 1185.666\n",
      "Epoch: 53/150.. Training loss: 1161.238\n",
      "Epoch: 53/150.. Training loss: 1105.667\n",
      "Epoch: 53/150.. Training loss: 922.023\n",
      "Epoch: 53/150.. Training loss: 1562.298\n",
      "Epoch: 53/150.. Training loss: 2766.935\n",
      "Epoch: 53/150.. Training loss: 2618.949\n",
      "Epoch: 53/150.. Training loss: 1751.772\n",
      "Epoch: 53/150.. Training loss: 2848.389\n",
      "Epoch: 54/150.. Training loss: 2298.283\n",
      "Epoch: 54/150.. Training loss: 1027.984\n",
      "Epoch: 54/150.. Training loss: 1262.217\n",
      "Epoch: 54/150.. Training loss: 1223.826\n",
      "Epoch: 54/150.. Training loss: 845.957\n",
      "Epoch: 54/150.. Training loss: 891.920\n",
      "Epoch: 54/150.. Training loss: 1643.932\n",
      "Epoch: 54/150.. Training loss: 4391.874\n",
      "Epoch: 54/150.. Training loss: 1200.713\n",
      "Epoch: 54/150.. Training loss: 2002.154\n",
      "Epoch: 54/150.. Training loss: 3442.411\n",
      "Epoch: 55/150.. Training loss: 1559.388\n",
      "Epoch: 55/150.. Training loss: 749.292\n",
      "Epoch: 55/150.. Training loss: 1405.509\n",
      "Epoch: 55/150.. Training loss: 1155.562\n",
      "Epoch: 55/150.. Training loss: 867.099\n",
      "Epoch: 55/150.. Training loss: 793.142\n",
      "Epoch: 55/150.. Training loss: 1886.301\n",
      "Epoch: 55/150.. Training loss: 4087.854\n",
      "Epoch: 55/150.. Training loss: 1367.453\n",
      "Epoch: 55/150.. Training loss: 1883.397\n",
      "Epoch: 55/150.. Training loss: 3402.390\n",
      "Epoch: 56/150.. Training loss: 1454.790\n",
      "Epoch: 56/150.. Training loss: 1249.601\n",
      "Epoch: 56/150.. Training loss: 1040.848\n",
      "Epoch: 56/150.. Training loss: 1023.540\n",
      "Epoch: 56/150.. Training loss: 1062.689\n",
      "Epoch: 56/150.. Training loss: 1077.894\n",
      "Epoch: 56/150.. Training loss: 2283.717\n",
      "Epoch: 56/150.. Training loss: 3619.883\n",
      "Epoch: 56/150.. Training loss: 1377.086\n",
      "Epoch: 56/150.. Training loss: 2917.361\n",
      "Epoch: 56/150.. Training loss: 2304.482\n",
      "Epoch: 57/150.. Training loss: 1304.092\n",
      "Epoch: 57/150.. Training loss: 1211.198\n",
      "Epoch: 57/150.. Training loss: 1159.654\n",
      "Epoch: 57/150.. Training loss: 1084.853\n",
      "Epoch: 57/150.. Training loss: 923.628\n",
      "Epoch: 57/150.. Training loss: 1484.025\n",
      "Epoch: 57/150.. Training loss: 2767.793\n",
      "Epoch: 57/150.. Training loss: 2711.364\n",
      "Epoch: 57/150.. Training loss: 1731.175\n",
      "Epoch: 57/150.. Training loss: 2891.879\n",
      "Epoch: 58/150.. Training loss: 2289.275\n",
      "Epoch: 58/150.. Training loss: 961.695\n",
      "Epoch: 58/150.. Training loss: 1286.080\n",
      "Epoch: 58/150.. Training loss: 1215.273\n",
      "Epoch: 58/150.. Training loss: 867.985\n",
      "Epoch: 58/150.. Training loss: 863.014\n",
      "Epoch: 58/150.. Training loss: 1667.462\n",
      "Epoch: 58/150.. Training loss: 4402.798\n",
      "Epoch: 58/150.. Training loss: 1240.546\n",
      "Epoch: 58/150.. Training loss: 1968.860\n",
      "Epoch: 58/150.. Training loss: 3279.852\n",
      "Epoch: 59/150.. Training loss: 1675.832\n",
      "Epoch: 59/150.. Training loss: 776.243\n",
      "Epoch: 59/150.. Training loss: 1376.196\n",
      "Epoch: 59/150.. Training loss: 1245.160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 59/150.. Training loss: 910.552\n",
      "Epoch: 59/150.. Training loss: 830.039\n",
      "Epoch: 59/150.. Training loss: 1807.681\n",
      "Epoch: 59/150.. Training loss: 4156.527\n",
      "Epoch: 59/150.. Training loss: 1319.995\n",
      "Epoch: 59/150.. Training loss: 1874.428\n",
      "Epoch: 59/150.. Training loss: 3426.690\n",
      "Epoch: 60/150.. Training loss: 1503.995\n",
      "Epoch: 60/150.. Training loss: 1198.726\n",
      "Epoch: 60/150.. Training loss: 1026.531\n",
      "Epoch: 60/150.. Training loss: 1045.217\n",
      "Epoch: 60/150.. Training loss: 1084.536\n",
      "Epoch: 60/150.. Training loss: 1079.986\n",
      "Epoch: 60/150.. Training loss: 2255.477\n",
      "Epoch: 60/150.. Training loss: 3436.178\n",
      "Epoch: 60/150.. Training loss: 1361.006\n",
      "Epoch: 60/150.. Training loss: 2907.835\n",
      "Epoch: 60/150.. Training loss: 2318.873\n",
      "Epoch: 61/150.. Training loss: 1280.724\n",
      "Epoch: 61/150.. Training loss: 1187.730\n",
      "Epoch: 61/150.. Training loss: 1166.860\n",
      "Epoch: 61/150.. Training loss: 1093.734\n",
      "Epoch: 61/150.. Training loss: 940.063\n",
      "Epoch: 61/150.. Training loss: 1496.820\n",
      "Epoch: 61/150.. Training loss: 2680.625\n",
      "Epoch: 61/150.. Training loss: 2612.596\n",
      "Epoch: 61/150.. Training loss: 1740.467\n",
      "Epoch: 61/150.. Training loss: 2797.946\n",
      "Epoch: 62/150.. Training loss: 2355.464\n",
      "Epoch: 62/150.. Training loss: 1035.825\n",
      "Epoch: 62/150.. Training loss: 1300.792\n",
      "Epoch: 62/150.. Training loss: 1214.307\n",
      "Epoch: 62/150.. Training loss: 850.082\n",
      "Epoch: 62/150.. Training loss: 878.100\n",
      "Epoch: 62/150.. Training loss: 1649.315\n",
      "Epoch: 62/150.. Training loss: 4112.486\n",
      "Epoch: 62/150.. Training loss: 1318.572\n",
      "Epoch: 62/150.. Training loss: 1922.828\n",
      "Epoch: 62/150.. Training loss: 3295.259\n",
      "Epoch: 63/150.. Training loss: 1597.005\n",
      "Epoch: 63/150.. Training loss: 795.275\n",
      "Epoch: 63/150.. Training loss: 1442.050\n",
      "Epoch: 63/150.. Training loss: 1226.716\n",
      "Epoch: 63/150.. Training loss: 867.250\n",
      "Epoch: 63/150.. Training loss: 826.718\n",
      "Epoch: 63/150.. Training loss: 1809.780\n",
      "Epoch: 63/150.. Training loss: 3981.612\n",
      "Epoch: 63/150.. Training loss: 1399.304\n",
      "Epoch: 63/150.. Training loss: 1842.110\n",
      "Epoch: 63/150.. Training loss: 3422.325\n",
      "Epoch: 64/150.. Training loss: 1541.667\n",
      "Epoch: 64/150.. Training loss: 1225.823\n",
      "Epoch: 64/150.. Training loss: 1032.859\n",
      "Epoch: 64/150.. Training loss: 1042.493\n",
      "Epoch: 64/150.. Training loss: 1095.745\n",
      "Epoch: 64/150.. Training loss: 1006.456\n",
      "Epoch: 64/150.. Training loss: 2261.208\n",
      "Epoch: 64/150.. Training loss: 3529.285\n",
      "Epoch: 64/150.. Training loss: 1318.593\n",
      "Epoch: 64/150.. Training loss: 2945.982\n",
      "Epoch: 64/150.. Training loss: 2389.933\n",
      "Epoch: 65/150.. Training loss: 1290.161\n",
      "Epoch: 65/150.. Training loss: 1205.386\n",
      "Epoch: 65/150.. Training loss: 1149.799\n",
      "Epoch: 65/150.. Training loss: 1081.037\n",
      "Epoch: 65/150.. Training loss: 939.977\n",
      "Epoch: 65/150.. Training loss: 1541.906\n",
      "Epoch: 65/150.. Training loss: 2764.483\n",
      "Epoch: 65/150.. Training loss: 2654.241\n",
      "Epoch: 65/150.. Training loss: 1685.488\n",
      "Epoch: 65/150.. Training loss: 2831.163\n",
      "Epoch: 66/150.. Training loss: 2285.861\n",
      "Epoch: 66/150.. Training loss: 968.830\n",
      "Epoch: 66/150.. Training loss: 1279.368\n",
      "Epoch: 66/150.. Training loss: 1229.554\n",
      "Epoch: 66/150.. Training loss: 852.217\n",
      "Epoch: 66/150.. Training loss: 946.476\n",
      "Epoch: 66/150.. Training loss: 1643.144\n",
      "Epoch: 66/150.. Training loss: 4370.530\n",
      "Epoch: 66/150.. Training loss: 1248.501\n",
      "Epoch: 66/150.. Training loss: 1986.585\n",
      "Epoch: 66/150.. Training loss: 3250.129\n",
      "Epoch: 67/150.. Training loss: 1624.415\n",
      "Epoch: 67/150.. Training loss: 770.925\n",
      "Epoch: 67/150.. Training loss: 1346.224\n",
      "Epoch: 67/150.. Training loss: 1255.825\n",
      "Epoch: 67/150.. Training loss: 874.583\n",
      "Epoch: 67/150.. Training loss: 828.886\n",
      "Epoch: 67/150.. Training loss: 1811.315\n",
      "Epoch: 67/150.. Training loss: 4055.344\n",
      "Epoch: 67/150.. Training loss: 1381.831\n",
      "Epoch: 67/150.. Training loss: 1869.529\n",
      "Epoch: 67/150.. Training loss: 3431.251\n",
      "Epoch: 68/150.. Training loss: 1544.965\n",
      "Epoch: 68/150.. Training loss: 1212.763\n",
      "Epoch: 68/150.. Training loss: 1011.905\n",
      "Epoch: 68/150.. Training loss: 1058.400\n",
      "Epoch: 68/150.. Training loss: 1077.730\n",
      "Epoch: 68/150.. Training loss: 1100.843\n",
      "Epoch: 68/150.. Training loss: 2259.502\n",
      "Epoch: 68/150.. Training loss: 3552.894\n",
      "Epoch: 68/150.. Training loss: 1347.987\n",
      "Epoch: 68/150.. Training loss: 2890.323\n",
      "Epoch: 68/150.. Training loss: 2244.384\n",
      "Epoch: 69/150.. Training loss: 1309.233\n",
      "Epoch: 69/150.. Training loss: 1230.694\n",
      "Epoch: 69/150.. Training loss: 1145.278\n",
      "Epoch: 69/150.. Training loss: 1065.507\n",
      "Epoch: 69/150.. Training loss: 917.748\n",
      "Epoch: 69/150.. Training loss: 1555.901\n",
      "Epoch: 69/150.. Training loss: 2751.710\n",
      "Epoch: 69/150.. Training loss: 2672.470\n",
      "Epoch: 69/150.. Training loss: 1709.921\n",
      "Epoch: 69/150.. Training loss: 2869.989\n",
      "Epoch: 70/150.. Training loss: 2337.988\n",
      "Epoch: 70/150.. Training loss: 998.729\n",
      "Epoch: 70/150.. Training loss: 1262.676\n",
      "Epoch: 70/150.. Training loss: 1280.179\n",
      "Epoch: 70/150.. Training loss: 847.911\n",
      "Epoch: 70/150.. Training loss: 911.767\n",
      "Epoch: 70/150.. Training loss: 1650.710\n",
      "Epoch: 70/150.. Training loss: 4354.432\n",
      "Epoch: 70/150.. Training loss: 1178.030\n",
      "Epoch: 70/150.. Training loss: 1962.398\n",
      "Epoch: 70/150.. Training loss: 3298.614\n",
      "Epoch: 71/150.. Training loss: 1570.113\n",
      "Epoch: 71/150.. Training loss: 808.903\n",
      "Epoch: 71/150.. Training loss: 1422.774\n",
      "Epoch: 71/150.. Training loss: 1255.980\n",
      "Epoch: 71/150.. Training loss: 843.585\n",
      "Epoch: 71/150.. Training loss: 792.816\n",
      "Epoch: 71/150.. Training loss: 1870.215\n",
      "Epoch: 71/150.. Training loss: 4120.873\n",
      "Epoch: 71/150.. Training loss: 1341.593\n",
      "Epoch: 71/150.. Training loss: 1873.920\n",
      "Epoch: 71/150.. Training loss: 3318.664\n",
      "Epoch: 72/150.. Training loss: 1509.345\n",
      "Epoch: 72/150.. Training loss: 1287.422\n",
      "Epoch: 72/150.. Training loss: 1037.773\n",
      "Epoch: 72/150.. Training loss: 1071.622\n",
      "Epoch: 72/150.. Training loss: 1056.454\n",
      "Epoch: 72/150.. Training loss: 1058.531\n",
      "Epoch: 72/150.. Training loss: 2296.597\n",
      "Epoch: 72/150.. Training loss: 3583.416\n",
      "Epoch: 72/150.. Training loss: 1310.206\n",
      "Epoch: 72/150.. Training loss: 2967.711\n",
      "Epoch: 72/150.. Training loss: 2343.250\n",
      "Epoch: 73/150.. Training loss: 1285.248\n",
      "Epoch: 73/150.. Training loss: 1230.297\n",
      "Epoch: 73/150.. Training loss: 1168.462\n",
      "Epoch: 73/150.. Training loss: 1103.376\n",
      "Epoch: 73/150.. Training loss: 1004.147\n",
      "Epoch: 73/150.. Training loss: 1517.815\n",
      "Epoch: 73/150.. Training loss: 2734.647\n",
      "Epoch: 73/150.. Training loss: 2609.527\n",
      "Epoch: 73/150.. Training loss: 1707.743\n",
      "Epoch: 73/150.. Training loss: 2860.839\n",
      "Epoch: 74/150.. Training loss: 2292.692\n",
      "Epoch: 74/150.. Training loss: 1003.310\n",
      "Epoch: 74/150.. Training loss: 1250.774\n",
      "Epoch: 74/150.. Training loss: 1284.722\n",
      "Epoch: 74/150.. Training loss: 850.064\n",
      "Epoch: 74/150.. Training loss: 894.987\n",
      "Epoch: 74/150.. Training loss: 1633.875\n",
      "Epoch: 74/150.. Training loss: 4217.596\n",
      "Epoch: 74/150.. Training loss: 1224.323\n",
      "Epoch: 74/150.. Training loss: 1979.992\n",
      "Epoch: 74/150.. Training loss: 3375.289\n",
      "Epoch: 75/150.. Training loss: 1558.137\n",
      "Epoch: 75/150.. Training loss: 803.673\n",
      "Epoch: 75/150.. Training loss: 1451.652\n",
      "Epoch: 75/150.. Training loss: 1292.763\n",
      "Epoch: 75/150.. Training loss: 886.998\n",
      "Epoch: 75/150.. Training loss: 788.287\n",
      "Epoch: 75/150.. Training loss: 1815.077\n",
      "Epoch: 75/150.. Training loss: 4049.901\n",
      "Epoch: 75/150.. Training loss: 1361.261\n",
      "Epoch: 75/150.. Training loss: 1857.109\n",
      "Epoch: 75/150.. Training loss: 3434.963\n",
      "Epoch: 76/150.. Training loss: 1513.408\n",
      "Epoch: 76/150.. Training loss: 1264.414\n",
      "Epoch: 76/150.. Training loss: 1011.101\n",
      "Epoch: 76/150.. Training loss: 1085.509\n",
      "Epoch: 76/150.. Training loss: 1078.964\n",
      "Epoch: 76/150.. Training loss: 1065.575\n",
      "Epoch: 76/150.. Training loss: 2234.367\n",
      "Epoch: 76/150.. Training loss: 3613.889\n",
      "Epoch: 76/150.. Training loss: 1318.049\n",
      "Epoch: 76/150.. Training loss: 3003.353\n",
      "Epoch: 76/150.. Training loss: 2324.614\n",
      "Epoch: 77/150.. Training loss: 1297.518\n",
      "Epoch: 77/150.. Training loss: 1236.199\n",
      "Epoch: 77/150.. Training loss: 1196.582\n",
      "Epoch: 77/150.. Training loss: 1073.365\n",
      "Epoch: 77/150.. Training loss: 951.248\n",
      "Epoch: 77/150.. Training loss: 1515.909\n",
      "Epoch: 77/150.. Training loss: 2759.405\n",
      "Epoch: 77/150.. Training loss: 2667.327\n",
      "Epoch: 77/150.. Training loss: 1700.081\n",
      "Epoch: 77/150.. Training loss: 2855.549\n",
      "Epoch: 78/150.. Training loss: 2243.244\n",
      "Epoch: 78/150.. Training loss: 1006.930\n",
      "Epoch: 78/150.. Training loss: 1276.940\n",
      "Epoch: 78/150.. Training loss: 1318.714\n",
      "Epoch: 78/150.. Training loss: 861.549\n",
      "Epoch: 78/150.. Training loss: 960.613\n",
      "Epoch: 78/150.. Training loss: 1648.946\n",
      "Epoch: 78/150.. Training loss: 4301.260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 78/150.. Training loss: 1183.250\n",
      "Epoch: 78/150.. Training loss: 1975.461\n",
      "Epoch: 78/150.. Training loss: 3387.584\n",
      "Epoch: 79/150.. Training loss: 1556.070\n",
      "Epoch: 79/150.. Training loss: 832.085\n",
      "Epoch: 79/150.. Training loss: 1399.469\n",
      "Epoch: 79/150.. Training loss: 1280.519\n",
      "Epoch: 79/150.. Training loss: 898.548\n",
      "Epoch: 79/150.. Training loss: 805.462\n",
      "Epoch: 79/150.. Training loss: 1837.702\n",
      "Epoch: 79/150.. Training loss: 4003.835\n",
      "Epoch: 79/150.. Training loss: 1390.606\n",
      "Epoch: 79/150.. Training loss: 1858.004\n",
      "Epoch: 79/150.. Training loss: 3426.615\n",
      "Epoch: 80/150.. Training loss: 1501.271\n",
      "Epoch: 80/150.. Training loss: 1217.342\n",
      "Epoch: 80/150.. Training loss: 1005.207\n",
      "Epoch: 80/150.. Training loss: 1099.895\n",
      "Epoch: 80/150.. Training loss: 1063.741\n",
      "Epoch: 80/150.. Training loss: 1074.366\n",
      "Epoch: 80/150.. Training loss: 2250.571\n",
      "Epoch: 80/150.. Training loss: 3574.721\n",
      "Epoch: 80/150.. Training loss: 1363.828\n",
      "Epoch: 80/150.. Training loss: 2908.978\n",
      "Epoch: 80/150.. Training loss: 2286.684\n",
      "Epoch: 81/150.. Training loss: 1303.859\n",
      "Epoch: 81/150.. Training loss: 1206.751\n",
      "Epoch: 81/150.. Training loss: 1179.908\n",
      "Epoch: 81/150.. Training loss: 1124.096\n",
      "Epoch: 81/150.. Training loss: 919.957\n",
      "Epoch: 81/150.. Training loss: 1533.146\n",
      "Epoch: 81/150.. Training loss: 2847.661\n",
      "Epoch: 81/150.. Training loss: 2661.851\n",
      "Epoch: 81/150.. Training loss: 1723.846\n",
      "Epoch: 81/150.. Training loss: 2835.953\n",
      "Epoch: 82/150.. Training loss: 2325.812\n",
      "Epoch: 82/150.. Training loss: 1033.040\n",
      "Epoch: 82/150.. Training loss: 1327.682\n",
      "Epoch: 82/150.. Training loss: 1263.651\n",
      "Epoch: 82/150.. Training loss: 883.016\n",
      "Epoch: 82/150.. Training loss: 916.655\n",
      "Epoch: 82/150.. Training loss: 1656.489\n",
      "Epoch: 82/150.. Training loss: 4346.298\n",
      "Epoch: 82/150.. Training loss: 1232.882\n",
      "Epoch: 82/150.. Training loss: 1969.969\n",
      "Epoch: 82/150.. Training loss: 3284.898\n",
      "Epoch: 83/150.. Training loss: 1603.244\n",
      "Epoch: 83/150.. Training loss: 844.526\n",
      "Epoch: 83/150.. Training loss: 1414.305\n",
      "Epoch: 83/150.. Training loss: 1229.457\n",
      "Epoch: 83/150.. Training loss: 841.998\n",
      "Epoch: 83/150.. Training loss: 826.892\n",
      "Epoch: 83/150.. Training loss: 1830.331\n",
      "Epoch: 83/150.. Training loss: 4231.594\n",
      "Epoch: 83/150.. Training loss: 1322.837\n",
      "Epoch: 83/150.. Training loss: 1933.078\n",
      "Epoch: 83/150.. Training loss: 3503.062\n",
      "Epoch: 84/150.. Training loss: 1488.606\n",
      "Epoch: 84/150.. Training loss: 1238.053\n",
      "Epoch: 84/150.. Training loss: 1016.856\n",
      "Epoch: 84/150.. Training loss: 1047.285\n",
      "Epoch: 84/150.. Training loss: 1038.056\n",
      "Epoch: 84/150.. Training loss: 1076.700\n",
      "Epoch: 84/150.. Training loss: 2287.107\n",
      "Epoch: 84/150.. Training loss: 3654.495\n",
      "Epoch: 84/150.. Training loss: 1325.024\n",
      "Epoch: 84/150.. Training loss: 3007.659\n",
      "Epoch: 84/150.. Training loss: 2336.354\n",
      "Epoch: 85/150.. Training loss: 1281.770\n",
      "Epoch: 85/150.. Training loss: 1215.563\n",
      "Epoch: 85/150.. Training loss: 1253.976\n",
      "Epoch: 85/150.. Training loss: 1107.415\n",
      "Epoch: 85/150.. Training loss: 928.030\n",
      "Epoch: 85/150.. Training loss: 1499.706\n",
      "Epoch: 85/150.. Training loss: 2720.968\n",
      "Epoch: 85/150.. Training loss: 2786.287\n",
      "Epoch: 85/150.. Training loss: 1717.475\n",
      "Epoch: 85/150.. Training loss: 2908.637\n",
      "Epoch: 86/150.. Training loss: 2315.966\n",
      "Epoch: 86/150.. Training loss: 991.914\n",
      "Epoch: 86/150.. Training loss: 1249.101\n",
      "Epoch: 86/150.. Training loss: 1319.437\n",
      "Epoch: 86/150.. Training loss: 851.029\n",
      "Epoch: 86/150.. Training loss: 933.185\n",
      "Epoch: 86/150.. Training loss: 1665.780\n",
      "Epoch: 86/150.. Training loss: 4290.845\n",
      "Epoch: 86/150.. Training loss: 1252.992\n",
      "Epoch: 86/150.. Training loss: 1990.006\n",
      "Epoch: 86/150.. Training loss: 3240.212\n",
      "Epoch: 87/150.. Training loss: 1564.103\n",
      "Epoch: 87/150.. Training loss: 805.231\n",
      "Epoch: 87/150.. Training loss: 1462.739\n",
      "Epoch: 87/150.. Training loss: 1288.186\n",
      "Epoch: 87/150.. Training loss: 906.042\n",
      "Epoch: 87/150.. Training loss: 880.335\n",
      "Epoch: 87/150.. Training loss: 1833.589\n",
      "Epoch: 87/150.. Training loss: 4057.401\n",
      "Epoch: 87/150.. Training loss: 1332.976\n",
      "Epoch: 87/150.. Training loss: 1933.261\n",
      "Epoch: 87/150.. Training loss: 3404.522\n",
      "Epoch: 88/150.. Training loss: 1535.438\n",
      "Epoch: 88/150.. Training loss: 1171.950\n",
      "Epoch: 88/150.. Training loss: 1055.314\n",
      "Epoch: 88/150.. Training loss: 1101.904\n",
      "Epoch: 88/150.. Training loss: 1106.454\n",
      "Epoch: 88/150.. Training loss: 1095.832\n",
      "Epoch: 88/150.. Training loss: 2295.008\n",
      "Epoch: 88/150.. Training loss: 3541.553\n",
      "Epoch: 88/150.. Training loss: 1312.744\n",
      "Epoch: 88/150.. Training loss: 2946.003\n",
      "Epoch: 88/150.. Training loss: 2264.179\n",
      "Epoch: 89/150.. Training loss: 1262.834\n",
      "Epoch: 89/150.. Training loss: 1229.467\n",
      "Epoch: 89/150.. Training loss: 1121.073\n",
      "Epoch: 89/150.. Training loss: 1090.780\n",
      "Epoch: 89/150.. Training loss: 921.206\n",
      "Epoch: 89/150.. Training loss: 1602.926\n",
      "Epoch: 89/150.. Training loss: 2832.058\n",
      "Epoch: 89/150.. Training loss: 2674.060\n",
      "Epoch: 89/150.. Training loss: 1714.585\n",
      "Epoch: 89/150.. Training loss: 2843.596\n",
      "Epoch: 90/150.. Training loss: 2288.095\n",
      "Epoch: 90/150.. Training loss: 982.455\n",
      "Epoch: 90/150.. Training loss: 1335.773\n",
      "Epoch: 90/150.. Training loss: 1217.714\n",
      "Epoch: 90/150.. Training loss: 865.341\n",
      "Epoch: 90/150.. Training loss: 891.613\n",
      "Epoch: 90/150.. Training loss: 1749.100\n",
      "Epoch: 90/150.. Training loss: 4331.429\n",
      "Epoch: 90/150.. Training loss: 1198.571\n",
      "Epoch: 90/150.. Training loss: 1955.671\n",
      "Epoch: 90/150.. Training loss: 3406.025\n",
      "Epoch: 91/150.. Training loss: 1534.560\n",
      "Epoch: 91/150.. Training loss: 795.070\n",
      "Epoch: 91/150.. Training loss: 1412.035\n",
      "Epoch: 91/150.. Training loss: 1237.209\n",
      "Epoch: 91/150.. Training loss: 873.620\n",
      "Epoch: 91/150.. Training loss: 798.356\n",
      "Epoch: 91/150.. Training loss: 1932.400\n",
      "Epoch: 91/150.. Training loss: 4029.084\n",
      "Epoch: 91/150.. Training loss: 1354.125\n",
      "Epoch: 91/150.. Training loss: 1881.839\n",
      "Epoch: 91/150.. Training loss: 3291.685\n",
      "Epoch: 92/150.. Training loss: 1498.370\n",
      "Epoch: 92/150.. Training loss: 1200.354\n",
      "Epoch: 92/150.. Training loss: 1033.528\n",
      "Epoch: 92/150.. Training loss: 1056.389\n",
      "Epoch: 92/150.. Training loss: 1102.030\n",
      "Epoch: 92/150.. Training loss: 1100.193\n",
      "Epoch: 92/150.. Training loss: 2307.606\n",
      "Epoch: 92/150.. Training loss: 3660.045\n",
      "Epoch: 92/150.. Training loss: 1344.906\n",
      "Epoch: 92/150.. Training loss: 2900.509\n",
      "Epoch: 92/150.. Training loss: 2305.669\n",
      "Epoch: 93/150.. Training loss: 1312.811\n",
      "Epoch: 93/150.. Training loss: 1210.001\n",
      "Epoch: 93/150.. Training loss: 1149.103\n",
      "Epoch: 93/150.. Training loss: 1073.600\n",
      "Epoch: 93/150.. Training loss: 1009.512\n",
      "Epoch: 93/150.. Training loss: 1497.290\n",
      "Epoch: 93/150.. Training loss: 2788.403\n",
      "Epoch: 93/150.. Training loss: 2639.592\n",
      "Epoch: 93/150.. Training loss: 1757.821\n",
      "Epoch: 93/150.. Training loss: 2828.532\n",
      "Epoch: 94/150.. Training loss: 2397.359\n",
      "Epoch: 94/150.. Training loss: 1025.893\n",
      "Epoch: 94/150.. Training loss: 1299.338\n",
      "Epoch: 94/150.. Training loss: 1270.284\n",
      "Epoch: 94/150.. Training loss: 845.924\n",
      "Epoch: 94/150.. Training loss: 893.751\n",
      "Epoch: 94/150.. Training loss: 1695.965\n",
      "Epoch: 94/150.. Training loss: 4305.128\n",
      "Epoch: 94/150.. Training loss: 1227.152\n",
      "Epoch: 94/150.. Training loss: 1969.753\n",
      "Epoch: 94/150.. Training loss: 3325.560\n",
      "Epoch: 95/150.. Training loss: 1612.418\n",
      "Epoch: 95/150.. Training loss: 774.831\n",
      "Epoch: 95/150.. Training loss: 1442.300\n",
      "Epoch: 95/150.. Training loss: 1218.710\n",
      "Epoch: 95/150.. Training loss: 856.350\n",
      "Epoch: 95/150.. Training loss: 853.348\n",
      "Epoch: 95/150.. Training loss: 1879.377\n",
      "Epoch: 95/150.. Training loss: 3966.119\n",
      "Epoch: 95/150.. Training loss: 1395.650\n",
      "Epoch: 95/150.. Training loss: 1883.430\n",
      "Epoch: 95/150.. Training loss: 3356.380\n",
      "Epoch: 96/150.. Training loss: 1587.627\n",
      "Epoch: 96/150.. Training loss: 1283.448\n",
      "Epoch: 96/150.. Training loss: 1017.501\n",
      "Epoch: 96/150.. Training loss: 1097.986\n",
      "Epoch: 96/150.. Training loss: 1088.721\n",
      "Epoch: 96/150.. Training loss: 1061.730\n",
      "Epoch: 96/150.. Training loss: 2295.389\n",
      "Epoch: 96/150.. Training loss: 3607.357\n",
      "Epoch: 96/150.. Training loss: 1298.839\n",
      "Epoch: 96/150.. Training loss: 2935.191\n",
      "Epoch: 96/150.. Training loss: 2295.889\n",
      "Epoch: 97/150.. Training loss: 1279.823\n",
      "Epoch: 97/150.. Training loss: 1237.496\n",
      "Epoch: 97/150.. Training loss: 1131.528\n",
      "Epoch: 97/150.. Training loss: 1085.662\n",
      "Epoch: 97/150.. Training loss: 924.957\n",
      "Epoch: 97/150.. Training loss: 1566.359\n",
      "Epoch: 97/150.. Training loss: 2856.690\n",
      "Epoch: 97/150.. Training loss: 2708.792\n",
      "Epoch: 97/150.. Training loss: 1677.718\n",
      "Epoch: 97/150.. Training loss: 2849.763\n",
      "Epoch: 98/150.. Training loss: 2265.902\n",
      "Epoch: 98/150.. Training loss: 1017.419\n",
      "Epoch: 98/150.. Training loss: 1327.842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 98/150.. Training loss: 1231.547\n",
      "Epoch: 98/150.. Training loss: 864.198\n",
      "Epoch: 98/150.. Training loss: 911.133\n",
      "Epoch: 98/150.. Training loss: 1665.216\n",
      "Epoch: 98/150.. Training loss: 4432.884\n",
      "Epoch: 98/150.. Training loss: 1239.943\n",
      "Epoch: 98/150.. Training loss: 1961.975\n",
      "Epoch: 98/150.. Training loss: 3403.016\n",
      "Epoch: 99/150.. Training loss: 1557.008\n",
      "Epoch: 99/150.. Training loss: 784.685\n",
      "Epoch: 99/150.. Training loss: 1420.332\n",
      "Epoch: 99/150.. Training loss: 1244.540\n",
      "Epoch: 99/150.. Training loss: 853.352\n",
      "Epoch: 99/150.. Training loss: 828.383\n",
      "Epoch: 99/150.. Training loss: 1837.207\n",
      "Epoch: 99/150.. Training loss: 4235.252\n",
      "Epoch: 99/150.. Training loss: 1356.104\n",
      "Epoch: 99/150.. Training loss: 1908.385\n",
      "Epoch: 99/150.. Training loss: 3499.021\n",
      "Epoch: 100/150.. Training loss: 1545.565\n",
      "Epoch: 100/150.. Training loss: 1201.619\n",
      "Epoch: 100/150.. Training loss: 1020.630\n",
      "Epoch: 100/150.. Training loss: 995.920\n",
      "Epoch: 100/150.. Training loss: 1124.124\n",
      "Epoch: 100/150.. Training loss: 1073.451\n",
      "Epoch: 100/150.. Training loss: 2376.276\n",
      "Epoch: 100/150.. Training loss: 3545.990\n",
      "Epoch: 100/150.. Training loss: 1360.184\n",
      "Epoch: 100/150.. Training loss: 2928.140\n",
      "Epoch: 100/150.. Training loss: 2266.498\n",
      "Epoch: 101/150.. Training loss: 1352.427\n",
      "Epoch: 101/150.. Training loss: 1292.438\n",
      "Epoch: 101/150.. Training loss: 1129.775\n",
      "Epoch: 101/150.. Training loss: 1154.348\n",
      "Epoch: 101/150.. Training loss: 954.689\n",
      "Epoch: 101/150.. Training loss: 1644.761\n",
      "Epoch: 101/150.. Training loss: 2885.527\n",
      "Epoch: 101/150.. Training loss: 2585.433\n",
      "Epoch: 101/150.. Training loss: 1709.230\n",
      "Epoch: 101/150.. Training loss: 2819.469\n",
      "Epoch: 102/150.. Training loss: 2251.618\n",
      "Epoch: 102/150.. Training loss: 1019.767\n",
      "Epoch: 102/150.. Training loss: 1278.555\n",
      "Epoch: 102/150.. Training loss: 1319.929\n",
      "Epoch: 102/150.. Training loss: 815.763\n",
      "Epoch: 102/150.. Training loss: 948.599\n",
      "Epoch: 102/150.. Training loss: 1702.356\n",
      "Epoch: 102/150.. Training loss: 4286.112\n",
      "Epoch: 102/150.. Training loss: 1300.799\n",
      "Epoch: 102/150.. Training loss: 2045.399\n",
      "Epoch: 102/150.. Training loss: 3330.491\n",
      "Epoch: 103/150.. Training loss: 1585.295\n",
      "Epoch: 103/150.. Training loss: 835.996\n",
      "Epoch: 103/150.. Training loss: 1407.370\n",
      "Epoch: 103/150.. Training loss: 1243.358\n",
      "Epoch: 103/150.. Training loss: 899.957\n",
      "Epoch: 103/150.. Training loss: 841.285\n",
      "Epoch: 103/150.. Training loss: 1845.460\n",
      "Epoch: 103/150.. Training loss: 4165.011\n",
      "Epoch: 103/150.. Training loss: 1299.444\n",
      "Epoch: 103/150.. Training loss: 1885.415\n",
      "Epoch: 103/150.. Training loss: 3433.682\n",
      "Epoch: 104/150.. Training loss: 1510.689\n",
      "Epoch: 104/150.. Training loss: 1191.254\n",
      "Epoch: 104/150.. Training loss: 992.479\n",
      "Epoch: 104/150.. Training loss: 1013.260\n",
      "Epoch: 104/150.. Training loss: 1081.297\n",
      "Epoch: 104/150.. Training loss: 1069.839\n",
      "Epoch: 104/150.. Training loss: 2291.369\n",
      "Epoch: 104/150.. Training loss: 3634.339\n",
      "Epoch: 104/150.. Training loss: 1319.420\n",
      "Epoch: 104/150.. Training loss: 2955.080\n",
      "Epoch: 104/150.. Training loss: 2273.704\n",
      "Epoch: 105/150.. Training loss: 1311.093\n",
      "Epoch: 105/150.. Training loss: 1261.109\n",
      "Epoch: 105/150.. Training loss: 1146.810\n",
      "Epoch: 105/150.. Training loss: 1095.665\n",
      "Epoch: 105/150.. Training loss: 944.366\n",
      "Epoch: 105/150.. Training loss: 1573.408\n",
      "Epoch: 105/150.. Training loss: 2761.527\n",
      "Epoch: 105/150.. Training loss: 2653.112\n",
      "Epoch: 105/150.. Training loss: 1721.692\n",
      "Epoch: 105/150.. Training loss: 2835.144\n",
      "Epoch: 106/150.. Training loss: 2229.275\n",
      "Epoch: 106/150.. Training loss: 994.611\n",
      "Epoch: 106/150.. Training loss: 1290.293\n",
      "Epoch: 106/150.. Training loss: 1298.075\n",
      "Epoch: 106/150.. Training loss: 853.740\n",
      "Epoch: 106/150.. Training loss: 936.114\n",
      "Epoch: 106/150.. Training loss: 1726.785\n",
      "Epoch: 106/150.. Training loss: 4281.029\n",
      "Epoch: 106/150.. Training loss: 1248.531\n",
      "Epoch: 106/150.. Training loss: 1928.144\n",
      "Epoch: 106/150.. Training loss: 3446.391\n",
      "Epoch: 107/150.. Training loss: 1556.002\n",
      "Epoch: 107/150.. Training loss: 793.365\n",
      "Epoch: 107/150.. Training loss: 1449.950\n",
      "Epoch: 107/150.. Training loss: 1322.037\n",
      "Epoch: 107/150.. Training loss: 856.779\n",
      "Epoch: 107/150.. Training loss: 837.466\n",
      "Epoch: 107/150.. Training loss: 1838.762\n",
      "Epoch: 107/150.. Training loss: 4127.993\n",
      "Epoch: 107/150.. Training loss: 1381.449\n",
      "Epoch: 107/150.. Training loss: 1846.755\n",
      "Epoch: 107/150.. Training loss: 3384.741\n",
      "Epoch: 108/150.. Training loss: 1480.297\n",
      "Epoch: 108/150.. Training loss: 1260.547\n",
      "Epoch: 108/150.. Training loss: 1029.218\n",
      "Epoch: 108/150.. Training loss: 1071.383\n",
      "Epoch: 108/150.. Training loss: 1078.473\n",
      "Epoch: 108/150.. Training loss: 1121.173\n",
      "Epoch: 108/150.. Training loss: 2326.404\n",
      "Epoch: 108/150.. Training loss: 3504.336\n",
      "Epoch: 108/150.. Training loss: 1380.446\n",
      "Epoch: 108/150.. Training loss: 2924.519\n",
      "Epoch: 108/150.. Training loss: 2268.712\n",
      "Epoch: 109/150.. Training loss: 1308.104\n",
      "Epoch: 109/150.. Training loss: 1212.027\n",
      "Epoch: 109/150.. Training loss: 1199.300\n",
      "Epoch: 109/150.. Training loss: 1154.984\n",
      "Epoch: 109/150.. Training loss: 927.822\n",
      "Epoch: 109/150.. Training loss: 1526.400\n",
      "Epoch: 109/150.. Training loss: 2867.634\n",
      "Epoch: 109/150.. Training loss: 2633.950\n",
      "Epoch: 109/150.. Training loss: 1708.573\n",
      "Epoch: 109/150.. Training loss: 2895.852\n",
      "Epoch: 110/150.. Training loss: 2277.061\n",
      "Epoch: 110/150.. Training loss: 1019.898\n",
      "Epoch: 110/150.. Training loss: 1296.889\n",
      "Epoch: 110/150.. Training loss: 1261.920\n",
      "Epoch: 110/150.. Training loss: 865.725\n",
      "Epoch: 110/150.. Training loss: 890.903\n",
      "Epoch: 110/150.. Training loss: 1694.793\n",
      "Epoch: 110/150.. Training loss: 4372.470\n",
      "Epoch: 110/150.. Training loss: 1182.851\n",
      "Epoch: 110/150.. Training loss: 1998.339\n",
      "Epoch: 110/150.. Training loss: 3239.032\n",
      "Epoch: 111/150.. Training loss: 1599.005\n",
      "Epoch: 111/150.. Training loss: 770.052\n",
      "Epoch: 111/150.. Training loss: 1426.274\n",
      "Epoch: 111/150.. Training loss: 1231.711\n",
      "Epoch: 111/150.. Training loss: 838.130\n",
      "Epoch: 111/150.. Training loss: 822.424\n",
      "Epoch: 111/150.. Training loss: 1852.304\n",
      "Epoch: 111/150.. Training loss: 4015.534\n",
      "Epoch: 111/150.. Training loss: 1354.599\n",
      "Epoch: 111/150.. Training loss: 1860.856\n",
      "Epoch: 111/150.. Training loss: 3296.479\n",
      "Epoch: 112/150.. Training loss: 1535.003\n",
      "Epoch: 112/150.. Training loss: 1245.275\n",
      "Epoch: 112/150.. Training loss: 1003.665\n",
      "Epoch: 112/150.. Training loss: 1067.032\n",
      "Epoch: 112/150.. Training loss: 1026.608\n",
      "Epoch: 112/150.. Training loss: 1071.285\n",
      "Epoch: 112/150.. Training loss: 2364.499\n",
      "Epoch: 112/150.. Training loss: 3463.409\n",
      "Epoch: 112/150.. Training loss: 1386.513\n",
      "Epoch: 112/150.. Training loss: 2883.695\n",
      "Epoch: 112/150.. Training loss: 2324.291\n",
      "Epoch: 113/150.. Training loss: 1300.222\n",
      "Epoch: 113/150.. Training loss: 1203.182\n",
      "Epoch: 113/150.. Training loss: 1171.055\n",
      "Epoch: 113/150.. Training loss: 1117.948\n",
      "Epoch: 113/150.. Training loss: 924.095\n",
      "Epoch: 113/150.. Training loss: 1571.432\n",
      "Epoch: 113/150.. Training loss: 2777.054\n",
      "Epoch: 113/150.. Training loss: 2566.891\n",
      "Epoch: 113/150.. Training loss: 1701.815\n",
      "Epoch: 113/150.. Training loss: 2811.373\n",
      "Epoch: 114/150.. Training loss: 2316.876\n",
      "Epoch: 114/150.. Training loss: 1037.232\n",
      "Epoch: 114/150.. Training loss: 1322.451\n",
      "Epoch: 114/150.. Training loss: 1283.158\n",
      "Epoch: 114/150.. Training loss: 853.264\n",
      "Epoch: 114/150.. Training loss: 939.385\n",
      "Epoch: 114/150.. Training loss: 1665.782\n",
      "Epoch: 114/150.. Training loss: 4208.601\n",
      "Epoch: 114/150.. Training loss: 1189.692\n",
      "Epoch: 114/150.. Training loss: 2000.649\n",
      "Epoch: 114/150.. Training loss: 3166.893\n",
      "Epoch: 115/150.. Training loss: 1612.978\n",
      "Epoch: 115/150.. Training loss: 807.803\n",
      "Epoch: 115/150.. Training loss: 1408.498\n",
      "Epoch: 115/150.. Training loss: 1253.431\n",
      "Epoch: 115/150.. Training loss: 911.258\n",
      "Epoch: 115/150.. Training loss: 826.899\n",
      "Epoch: 115/150.. Training loss: 1859.770\n",
      "Epoch: 115/150.. Training loss: 4202.985\n",
      "Epoch: 115/150.. Training loss: 1308.441\n",
      "Epoch: 115/150.. Training loss: 1878.914\n",
      "Epoch: 115/150.. Training loss: 3453.853\n",
      "Epoch: 116/150.. Training loss: 1477.475\n",
      "Epoch: 116/150.. Training loss: 1236.164\n",
      "Epoch: 116/150.. Training loss: 1023.971\n",
      "Epoch: 116/150.. Training loss: 1102.952\n",
      "Epoch: 116/150.. Training loss: 1102.134\n",
      "Epoch: 116/150.. Training loss: 1114.882\n",
      "Epoch: 116/150.. Training loss: 2321.932\n",
      "Epoch: 116/150.. Training loss: 3583.217\n",
      "Epoch: 116/150.. Training loss: 1435.932\n",
      "Epoch: 116/150.. Training loss: 2893.484\n",
      "Epoch: 116/150.. Training loss: 2263.260\n",
      "Epoch: 117/150.. Training loss: 1271.265\n",
      "Epoch: 117/150.. Training loss: 1205.777\n",
      "Epoch: 117/150.. Training loss: 1197.904\n",
      "Epoch: 117/150.. Training loss: 1152.428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 117/150.. Training loss: 941.703\n",
      "Epoch: 117/150.. Training loss: 1504.837\n",
      "Epoch: 117/150.. Training loss: 2892.248\n",
      "Epoch: 117/150.. Training loss: 2623.312\n",
      "Epoch: 117/150.. Training loss: 1735.530\n",
      "Epoch: 117/150.. Training loss: 2866.989\n",
      "Epoch: 118/150.. Training loss: 2270.177\n",
      "Epoch: 118/150.. Training loss: 986.060\n",
      "Epoch: 118/150.. Training loss: 1333.390\n",
      "Epoch: 118/150.. Training loss: 1288.184\n",
      "Epoch: 118/150.. Training loss: 861.349\n",
      "Epoch: 118/150.. Training loss: 884.081\n",
      "Epoch: 118/150.. Training loss: 1676.646\n",
      "Epoch: 118/150.. Training loss: 4219.047\n",
      "Epoch: 118/150.. Training loss: 1173.664\n",
      "Epoch: 118/150.. Training loss: 2026.583\n",
      "Epoch: 118/150.. Training loss: 3222.055\n",
      "Epoch: 119/150.. Training loss: 1604.796\n",
      "Epoch: 119/150.. Training loss: 801.979\n",
      "Epoch: 119/150.. Training loss: 1462.086\n",
      "Epoch: 119/150.. Training loss: 1236.756\n",
      "Epoch: 119/150.. Training loss: 855.121\n",
      "Epoch: 119/150.. Training loss: 866.910\n",
      "Epoch: 119/150.. Training loss: 1826.056\n",
      "Epoch: 119/150.. Training loss: 4143.926\n",
      "Epoch: 119/150.. Training loss: 1342.175\n",
      "Epoch: 119/150.. Training loss: 1857.187\n",
      "Epoch: 119/150.. Training loss: 3498.174\n",
      "Epoch: 120/150.. Training loss: 1512.987\n",
      "Epoch: 120/150.. Training loss: 1227.305\n",
      "Epoch: 120/150.. Training loss: 1064.576\n",
      "Epoch: 120/150.. Training loss: 1108.854\n",
      "Epoch: 120/150.. Training loss: 1056.211\n",
      "Epoch: 120/150.. Training loss: 1057.950\n",
      "Epoch: 120/150.. Training loss: 2266.787\n",
      "Epoch: 120/150.. Training loss: 3498.013\n",
      "Epoch: 120/150.. Training loss: 1337.920\n",
      "Epoch: 120/150.. Training loss: 2919.155\n",
      "Epoch: 120/150.. Training loss: 2264.608\n",
      "Epoch: 121/150.. Training loss: 1258.359\n",
      "Epoch: 121/150.. Training loss: 1262.449\n",
      "Epoch: 121/150.. Training loss: 1167.420\n",
      "Epoch: 121/150.. Training loss: 1106.489\n",
      "Epoch: 121/150.. Training loss: 920.244\n",
      "Epoch: 121/150.. Training loss: 1540.536\n",
      "Epoch: 121/150.. Training loss: 2717.013\n",
      "Epoch: 121/150.. Training loss: 2595.626\n",
      "Epoch: 121/150.. Training loss: 1708.528\n",
      "Epoch: 121/150.. Training loss: 2807.186\n",
      "Epoch: 122/150.. Training loss: 2265.149\n",
      "Epoch: 122/150.. Training loss: 1049.962\n",
      "Epoch: 122/150.. Training loss: 1289.767\n",
      "Epoch: 122/150.. Training loss: 1293.989\n",
      "Epoch: 122/150.. Training loss: 861.737\n",
      "Epoch: 122/150.. Training loss: 927.529\n",
      "Epoch: 122/150.. Training loss: 1644.873\n",
      "Epoch: 122/150.. Training loss: 4206.433\n",
      "Epoch: 122/150.. Training loss: 1251.541\n",
      "Epoch: 122/150.. Training loss: 1917.659\n",
      "Epoch: 122/150.. Training loss: 3307.648\n",
      "Epoch: 123/150.. Training loss: 1670.421\n",
      "Epoch: 123/150.. Training loss: 814.069\n",
      "Epoch: 123/150.. Training loss: 1419.236\n",
      "Epoch: 123/150.. Training loss: 1291.258\n",
      "Epoch: 123/150.. Training loss: 904.044\n",
      "Epoch: 123/150.. Training loss: 831.012\n",
      "Epoch: 123/150.. Training loss: 1790.717\n",
      "Epoch: 123/150.. Training loss: 4267.249\n",
      "Epoch: 123/150.. Training loss: 1350.133\n",
      "Epoch: 123/150.. Training loss: 1877.616\n",
      "Epoch: 123/150.. Training loss: 3443.471\n",
      "Epoch: 124/150.. Training loss: 1503.629\n",
      "Epoch: 124/150.. Training loss: 1208.343\n",
      "Epoch: 124/150.. Training loss: 1048.070\n",
      "Epoch: 124/150.. Training loss: 1137.645\n",
      "Epoch: 124/150.. Training loss: 1070.929\n",
      "Epoch: 124/150.. Training loss: 1090.006\n",
      "Epoch: 124/150.. Training loss: 2295.445\n",
      "Epoch: 124/150.. Training loss: 3568.155\n",
      "Epoch: 124/150.. Training loss: 1343.342\n",
      "Epoch: 124/150.. Training loss: 2938.288\n",
      "Epoch: 124/150.. Training loss: 2269.915\n",
      "Epoch: 125/150.. Training loss: 1309.564\n",
      "Epoch: 125/150.. Training loss: 1217.758\n",
      "Epoch: 125/150.. Training loss: 1157.390\n",
      "Epoch: 125/150.. Training loss: 1126.949\n",
      "Epoch: 125/150.. Training loss: 938.413\n",
      "Epoch: 125/150.. Training loss: 1501.033\n",
      "Epoch: 125/150.. Training loss: 2807.068\n",
      "Epoch: 125/150.. Training loss: 2591.818\n",
      "Epoch: 125/150.. Training loss: 1710.508\n",
      "Epoch: 125/150.. Training loss: 2863.047\n",
      "Epoch: 126/150.. Training loss: 2297.686\n",
      "Epoch: 126/150.. Training loss: 1005.450\n",
      "Epoch: 126/150.. Training loss: 1305.011\n",
      "Epoch: 126/150.. Training loss: 1276.713\n",
      "Epoch: 126/150.. Training loss: 839.882\n",
      "Epoch: 126/150.. Training loss: 908.321\n",
      "Epoch: 126/150.. Training loss: 1634.672\n",
      "Epoch: 126/150.. Training loss: 4509.399\n",
      "Epoch: 126/150.. Training loss: 1160.337\n",
      "Epoch: 126/150.. Training loss: 2003.419\n",
      "Epoch: 126/150.. Training loss: 3257.024\n",
      "Epoch: 127/150.. Training loss: 1540.264\n",
      "Epoch: 127/150.. Training loss: 811.600\n",
      "Epoch: 127/150.. Training loss: 1465.823\n",
      "Epoch: 127/150.. Training loss: 1299.436\n",
      "Epoch: 127/150.. Training loss: 860.966\n",
      "Epoch: 127/150.. Training loss: 840.287\n",
      "Epoch: 127/150.. Training loss: 1856.721\n",
      "Epoch: 127/150.. Training loss: 4035.381\n",
      "Epoch: 127/150.. Training loss: 1300.959\n",
      "Epoch: 127/150.. Training loss: 1868.277\n",
      "Epoch: 127/150.. Training loss: 3378.990\n",
      "Epoch: 128/150.. Training loss: 1543.653\n",
      "Epoch: 128/150.. Training loss: 1244.814\n",
      "Epoch: 128/150.. Training loss: 1045.401\n",
      "Epoch: 128/150.. Training loss: 1167.642\n",
      "Epoch: 128/150.. Training loss: 1063.637\n",
      "Epoch: 128/150.. Training loss: 1055.969\n",
      "Epoch: 128/150.. Training loss: 2368.669\n",
      "Epoch: 128/150.. Training loss: 3535.327\n",
      "Epoch: 128/150.. Training loss: 1339.947\n",
      "Epoch: 128/150.. Training loss: 2938.362\n",
      "Epoch: 128/150.. Training loss: 2272.951\n",
      "Epoch: 129/150.. Training loss: 1271.363\n",
      "Epoch: 129/150.. Training loss: 1227.528\n",
      "Epoch: 129/150.. Training loss: 1173.953\n",
      "Epoch: 129/150.. Training loss: 1082.081\n",
      "Epoch: 129/150.. Training loss: 930.828\n",
      "Epoch: 129/150.. Training loss: 1602.498\n",
      "Epoch: 129/150.. Training loss: 2801.503\n",
      "Epoch: 129/150.. Training loss: 2575.361\n",
      "Epoch: 129/150.. Training loss: 1770.752\n",
      "Epoch: 129/150.. Training loss: 2856.351\n",
      "Epoch: 130/150.. Training loss: 2281.577\n",
      "Epoch: 130/150.. Training loss: 1014.651\n",
      "Epoch: 130/150.. Training loss: 1316.025\n",
      "Epoch: 130/150.. Training loss: 1289.961\n",
      "Epoch: 130/150.. Training loss: 864.996\n",
      "Epoch: 130/150.. Training loss: 910.316\n",
      "Epoch: 130/150.. Training loss: 1655.701\n",
      "Epoch: 130/150.. Training loss: 4317.610\n",
      "Epoch: 130/150.. Training loss: 1146.716\n",
      "Epoch: 130/150.. Training loss: 1974.938\n",
      "Epoch: 130/150.. Training loss: 3492.977\n",
      "Epoch: 131/150.. Training loss: 1530.075\n",
      "Epoch: 131/150.. Training loss: 799.508\n",
      "Epoch: 131/150.. Training loss: 1422.576\n",
      "Epoch: 131/150.. Training loss: 1338.877\n",
      "Epoch: 131/150.. Training loss: 862.874\n",
      "Epoch: 131/150.. Training loss: 832.237\n",
      "Epoch: 131/150.. Training loss: 1839.102\n",
      "Epoch: 131/150.. Training loss: 4246.784\n",
      "Epoch: 131/150.. Training loss: 1296.773\n",
      "Epoch: 131/150.. Training loss: 1849.997\n",
      "Epoch: 131/150.. Training loss: 3444.523\n",
      "Epoch: 132/150.. Training loss: 1479.710\n",
      "Epoch: 132/150.. Training loss: 1224.525\n",
      "Epoch: 132/150.. Training loss: 1021.139\n",
      "Epoch: 132/150.. Training loss: 1075.822\n",
      "Epoch: 132/150.. Training loss: 1114.427\n",
      "Epoch: 132/150.. Training loss: 1149.695\n",
      "Epoch: 132/150.. Training loss: 2354.014\n",
      "Epoch: 132/150.. Training loss: 3529.105\n",
      "Epoch: 132/150.. Training loss: 1325.600\n",
      "Epoch: 132/150.. Training loss: 2968.994\n",
      "Epoch: 132/150.. Training loss: 2303.555\n",
      "Epoch: 133/150.. Training loss: 1309.315\n",
      "Epoch: 133/150.. Training loss: 1204.256\n",
      "Epoch: 133/150.. Training loss: 1183.158\n",
      "Epoch: 133/150.. Training loss: 1123.736\n",
      "Epoch: 133/150.. Training loss: 942.317\n",
      "Epoch: 133/150.. Training loss: 1551.743\n",
      "Epoch: 133/150.. Training loss: 2854.306\n",
      "Epoch: 133/150.. Training loss: 2531.545\n",
      "Epoch: 133/150.. Training loss: 1804.167\n",
      "Epoch: 133/150.. Training loss: 2795.879\n",
      "Epoch: 134/150.. Training loss: 2233.758\n",
      "Epoch: 134/150.. Training loss: 1000.363\n",
      "Epoch: 134/150.. Training loss: 1303.884\n",
      "Epoch: 134/150.. Training loss: 1265.074\n",
      "Epoch: 134/150.. Training loss: 876.490\n",
      "Epoch: 134/150.. Training loss: 919.669\n",
      "Epoch: 134/150.. Training loss: 1716.649\n",
      "Epoch: 134/150.. Training loss: 4321.911\n",
      "Epoch: 134/150.. Training loss: 1197.240\n",
      "Epoch: 134/150.. Training loss: 1983.024\n",
      "Epoch: 134/150.. Training loss: 3516.163\n",
      "Epoch: 135/150.. Training loss: 1568.793\n",
      "Epoch: 135/150.. Training loss: 811.136\n",
      "Epoch: 135/150.. Training loss: 1452.018\n",
      "Epoch: 135/150.. Training loss: 1259.537\n",
      "Epoch: 135/150.. Training loss: 888.996\n",
      "Epoch: 135/150.. Training loss: 869.117\n",
      "Epoch: 135/150.. Training loss: 1853.210\n",
      "Epoch: 135/150.. Training loss: 4114.894\n",
      "Epoch: 135/150.. Training loss: 1354.380\n",
      "Epoch: 135/150.. Training loss: 1847.808\n",
      "Epoch: 135/150.. Training loss: 3328.237\n",
      "Epoch: 136/150.. Training loss: 1569.912\n",
      "Epoch: 136/150.. Training loss: 1196.369\n",
      "Epoch: 136/150.. Training loss: 992.008\n",
      "Epoch: 136/150.. Training loss: 1092.304\n",
      "Epoch: 136/150.. Training loss: 1100.722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 136/150.. Training loss: 1136.510\n",
      "Epoch: 136/150.. Training loss: 2282.495\n",
      "Epoch: 136/150.. Training loss: 3733.637\n",
      "Epoch: 136/150.. Training loss: 1321.876\n",
      "Epoch: 136/150.. Training loss: 3032.708\n",
      "Epoch: 136/150.. Training loss: 2300.346\n",
      "Epoch: 137/150.. Training loss: 1301.163\n",
      "Epoch: 137/150.. Training loss: 1230.784\n",
      "Epoch: 137/150.. Training loss: 1184.752\n",
      "Epoch: 137/150.. Training loss: 1147.119\n",
      "Epoch: 137/150.. Training loss: 964.931\n",
      "Epoch: 137/150.. Training loss: 1533.767\n",
      "Epoch: 137/150.. Training loss: 2806.376\n",
      "Epoch: 137/150.. Training loss: 2533.216\n",
      "Epoch: 137/150.. Training loss: 1763.363\n",
      "Epoch: 137/150.. Training loss: 2813.672\n",
      "Epoch: 138/150.. Training loss: 2308.671\n",
      "Epoch: 138/150.. Training loss: 1016.552\n",
      "Epoch: 138/150.. Training loss: 1279.378\n",
      "Epoch: 138/150.. Training loss: 1279.340\n",
      "Epoch: 138/150.. Training loss: 879.474\n",
      "Epoch: 138/150.. Training loss: 943.586\n",
      "Epoch: 138/150.. Training loss: 1654.846\n",
      "Epoch: 138/150.. Training loss: 4365.064\n",
      "Epoch: 138/150.. Training loss: 1203.839\n",
      "Epoch: 138/150.. Training loss: 2051.988\n",
      "Epoch: 138/150.. Training loss: 3286.319\n",
      "Epoch: 139/150.. Training loss: 1520.776\n",
      "Epoch: 139/150.. Training loss: 792.011\n",
      "Epoch: 139/150.. Training loss: 1426.902\n",
      "Epoch: 139/150.. Training loss: 1221.364\n",
      "Epoch: 139/150.. Training loss: 861.024\n",
      "Epoch: 139/150.. Training loss: 884.896\n",
      "Epoch: 139/150.. Training loss: 1839.138\n",
      "Epoch: 139/150.. Training loss: 4236.969\n",
      "Epoch: 139/150.. Training loss: 1351.443\n",
      "Epoch: 139/150.. Training loss: 1912.107\n",
      "Epoch: 139/150.. Training loss: 3435.690\n",
      "Epoch: 140/150.. Training loss: 1533.399\n",
      "Epoch: 140/150.. Training loss: 1235.439\n",
      "Epoch: 140/150.. Training loss: 1036.566\n",
      "Epoch: 140/150.. Training loss: 1187.185\n",
      "Epoch: 140/150.. Training loss: 1052.519\n",
      "Epoch: 140/150.. Training loss: 1061.645\n",
      "Epoch: 140/150.. Training loss: 2330.044\n",
      "Epoch: 140/150.. Training loss: 3687.875\n",
      "Epoch: 140/150.. Training loss: 1385.134\n",
      "Epoch: 140/150.. Training loss: 3020.339\n",
      "Epoch: 140/150.. Training loss: 2293.344\n",
      "Epoch: 141/150.. Training loss: 1288.204\n",
      "Epoch: 141/150.. Training loss: 1228.372\n",
      "Epoch: 141/150.. Training loss: 1176.579\n",
      "Epoch: 141/150.. Training loss: 1120.677\n",
      "Epoch: 141/150.. Training loss: 952.514\n",
      "Epoch: 141/150.. Training loss: 1550.690\n",
      "Epoch: 141/150.. Training loss: 2873.577\n",
      "Epoch: 141/150.. Training loss: 2535.756\n",
      "Epoch: 141/150.. Training loss: 1773.935\n",
      "Epoch: 141/150.. Training loss: 2858.561\n",
      "Epoch: 142/150.. Training loss: 2229.540\n",
      "Epoch: 142/150.. Training loss: 1029.029\n",
      "Epoch: 142/150.. Training loss: 1326.365\n",
      "Epoch: 142/150.. Training loss: 1378.288\n",
      "Epoch: 142/150.. Training loss: 842.368\n",
      "Epoch: 142/150.. Training loss: 946.483\n",
      "Epoch: 142/150.. Training loss: 1679.299\n",
      "Epoch: 142/150.. Training loss: 4327.724\n",
      "Epoch: 142/150.. Training loss: 1178.518\n",
      "Epoch: 142/150.. Training loss: 1998.596\n",
      "Epoch: 142/150.. Training loss: 3397.517\n",
      "Epoch: 143/150.. Training loss: 1575.377\n",
      "Epoch: 143/150.. Training loss: 796.061\n",
      "Epoch: 143/150.. Training loss: 1394.089\n",
      "Epoch: 143/150.. Training loss: 1318.079\n",
      "Epoch: 143/150.. Training loss: 880.197\n",
      "Epoch: 143/150.. Training loss: 858.944\n",
      "Epoch: 143/150.. Training loss: 1859.726\n",
      "Epoch: 143/150.. Training loss: 4055.770\n",
      "Epoch: 143/150.. Training loss: 1328.960\n",
      "Epoch: 143/150.. Training loss: 1905.416\n",
      "Epoch: 143/150.. Training loss: 3297.399\n",
      "Epoch: 144/150.. Training loss: 1584.150\n",
      "Epoch: 144/150.. Training loss: 1279.369\n",
      "Epoch: 144/150.. Training loss: 1035.759\n",
      "Epoch: 144/150.. Training loss: 1175.339\n",
      "Epoch: 144/150.. Training loss: 1129.096\n",
      "Epoch: 144/150.. Training loss: 1091.097\n",
      "Epoch: 144/150.. Training loss: 2265.842\n",
      "Epoch: 144/150.. Training loss: 3654.752\n",
      "Epoch: 144/150.. Training loss: 1311.580\n",
      "Epoch: 144/150.. Training loss: 2944.465\n",
      "Epoch: 144/150.. Training loss: 2263.288\n",
      "Epoch: 145/150.. Training loss: 1291.442\n",
      "Epoch: 145/150.. Training loss: 1217.049\n",
      "Epoch: 145/150.. Training loss: 1152.767\n",
      "Epoch: 145/150.. Training loss: 1140.232\n",
      "Epoch: 145/150.. Training loss: 929.407\n",
      "Epoch: 145/150.. Training loss: 1519.863\n",
      "Epoch: 145/150.. Training loss: 2793.278\n",
      "Epoch: 145/150.. Training loss: 2525.227\n",
      "Epoch: 145/150.. Training loss: 1754.832\n",
      "Epoch: 145/150.. Training loss: 2793.904\n",
      "Epoch: 146/150.. Training loss: 2262.392\n",
      "Epoch: 146/150.. Training loss: 1052.362\n",
      "Epoch: 146/150.. Training loss: 1281.827\n",
      "Epoch: 146/150.. Training loss: 1305.096\n",
      "Epoch: 146/150.. Training loss: 860.281\n",
      "Epoch: 146/150.. Training loss: 910.103\n",
      "Epoch: 146/150.. Training loss: 1641.900\n",
      "Epoch: 146/150.. Training loss: 4408.093\n",
      "Epoch: 146/150.. Training loss: 1165.815\n",
      "Epoch: 146/150.. Training loss: 2001.898\n",
      "Epoch: 146/150.. Training loss: 3459.414\n",
      "Epoch: 147/150.. Training loss: 1490.430\n",
      "Epoch: 147/150.. Training loss: 767.802\n",
      "Epoch: 147/150.. Training loss: 1387.265\n",
      "Epoch: 147/150.. Training loss: 1328.113\n",
      "Epoch: 147/150.. Training loss: 872.720\n",
      "Epoch: 147/150.. Training loss: 928.116\n",
      "Epoch: 147/150.. Training loss: 1862.683\n",
      "Epoch: 147/150.. Training loss: 4115.971\n",
      "Epoch: 147/150.. Training loss: 1319.556\n",
      "Epoch: 147/150.. Training loss: 1861.148\n",
      "Epoch: 147/150.. Training loss: 3396.601\n",
      "Epoch: 148/150.. Training loss: 1501.644\n",
      "Epoch: 148/150.. Training loss: 1243.725\n",
      "Epoch: 148/150.. Training loss: 1038.468\n",
      "Epoch: 148/150.. Training loss: 1169.173\n",
      "Epoch: 148/150.. Training loss: 1106.024\n",
      "Epoch: 148/150.. Training loss: 1156.747\n",
      "Epoch: 148/150.. Training loss: 2315.407\n",
      "Epoch: 148/150.. Training loss: 3578.900\n",
      "Epoch: 148/150.. Training loss: 1323.970\n",
      "Epoch: 148/150.. Training loss: 2927.807\n",
      "Epoch: 148/150.. Training loss: 2344.652\n",
      "Epoch: 149/150.. Training loss: 1277.698\n",
      "Epoch: 149/150.. Training loss: 1234.793\n",
      "Epoch: 149/150.. Training loss: 1169.969\n",
      "Epoch: 149/150.. Training loss: 1174.170\n",
      "Epoch: 149/150.. Training loss: 955.492\n",
      "Epoch: 149/150.. Training loss: 1579.631\n",
      "Epoch: 149/150.. Training loss: 2860.919\n",
      "Epoch: 149/150.. Training loss: 2648.073\n",
      "Epoch: 149/150.. Training loss: 1734.779\n",
      "Epoch: 149/150.. Training loss: 2818.489\n",
      "Epoch: 150/150.. Training loss: 2296.745\n",
      "Epoch: 150/150.. Training loss: 1032.578\n",
      "Epoch: 150/150.. Training loss: 1319.386\n",
      "Epoch: 150/150.. Training loss: 1331.781\n",
      "Epoch: 150/150.. Training loss: 877.997\n",
      "Epoch: 150/150.. Training loss: 925.584\n",
      "Epoch: 150/150.. Training loss: 1721.217\n",
      "Epoch: 150/150.. Training loss: 4438.750\n",
      "Epoch: 150/150.. Training loss: 1190.573\n",
      "Epoch: 150/150.. Training loss: 1998.845\n",
      "Epoch: 150/150.. Training loss: 3424.874\n"
     ]
    }
   ],
   "source": [
    "model = NN(hidden_sizes=[30, 15])\n",
    "\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.005)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "training(model, criterion, optimizer, featureloader, labelloader, epochs=150, print_every=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] model is saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/syahrulhamdani/anaconda3/envs/100DaysOfMLCode/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type NN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "def save_model(model, epochs, optimizer, filename='checkpoint.pth', save_dir='reports'):\n",
    "    checkpoint = {\n",
    "        'model': model,\n",
    "        'model_state': model.state_dict(),\n",
    "        'epochs': epochs,\n",
    "        'optim': optimizer,\n",
    "        'optim_state': optimizer.state_dict()\n",
    "    }\n",
    "    torch.save(checkpoint, os.path.join(save_dir, filename))\n",
    "    print('[INFO] model is saved!')\n",
    "\n",
    "# save_model(model, 100, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/syahrulhamdani/anaconda3/envs/100DaysOfMLCode/lib/python3.6/site-packages/sklearn/base.py:251: UserWarning: Trying to unpickle estimator StandardScaler from version 0.19.2 when using version 0.20.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "checkpoint = torch.load('reports/checkpoint.pth')\n",
    "model = checkpoint['model']\n",
    "model.load_state_dict(checkpoint['model_state'])\n",
    "scaler = checkpoint['scaler']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EngineID</th>\n",
       "      <th>Cycle</th>\n",
       "      <th>OpSetting1</th>\n",
       "      <th>OpSetting2</th>\n",
       "      <th>OpSetting3</th>\n",
       "      <th>T2</th>\n",
       "      <th>T24</th>\n",
       "      <th>T30</th>\n",
       "      <th>T50</th>\n",
       "      <th>P2</th>\n",
       "      <th>...</th>\n",
       "      <th>phi</th>\n",
       "      <th>NRf</th>\n",
       "      <th>NRc</th>\n",
       "      <th>BPR</th>\n",
       "      <th>farB</th>\n",
       "      <th>htBleed</th>\n",
       "      <th>Nf_dmd</th>\n",
       "      <th>PCNfR_dmd</th>\n",
       "      <th>W31</th>\n",
       "      <th>W32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.02</td>\n",
       "      <td>1585.29</td>\n",
       "      <td>1398.21</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>521.72</td>\n",
       "      <td>2388.03</td>\n",
       "      <td>8125.55</td>\n",
       "      <td>8.4052</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.86</td>\n",
       "      <td>23.3735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.0027</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.71</td>\n",
       "      <td>1588.45</td>\n",
       "      <td>1395.42</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.16</td>\n",
       "      <td>2388.06</td>\n",
       "      <td>8139.62</td>\n",
       "      <td>8.3803</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.02</td>\n",
       "      <td>23.3916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.46</td>\n",
       "      <td>1586.94</td>\n",
       "      <td>1401.34</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>521.97</td>\n",
       "      <td>2388.03</td>\n",
       "      <td>8130.10</td>\n",
       "      <td>8.4441</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.08</td>\n",
       "      <td>23.4166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.44</td>\n",
       "      <td>1584.12</td>\n",
       "      <td>1406.42</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>521.38</td>\n",
       "      <td>2388.05</td>\n",
       "      <td>8132.90</td>\n",
       "      <td>8.3917</td>\n",
       "      <td>0.03</td>\n",
       "      <td>391</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.00</td>\n",
       "      <td>23.3737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.51</td>\n",
       "      <td>1587.19</td>\n",
       "      <td>1401.92</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.15</td>\n",
       "      <td>2388.03</td>\n",
       "      <td>8129.54</td>\n",
       "      <td>8.4031</td>\n",
       "      <td>0.03</td>\n",
       "      <td>390</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.99</td>\n",
       "      <td>23.4130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.11</td>\n",
       "      <td>1579.12</td>\n",
       "      <td>1395.13</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>521.92</td>\n",
       "      <td>2388.08</td>\n",
       "      <td>8127.46</td>\n",
       "      <td>8.4238</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.91</td>\n",
       "      <td>23.3467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.11</td>\n",
       "      <td>1583.34</td>\n",
       "      <td>1404.84</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.01</td>\n",
       "      <td>2388.06</td>\n",
       "      <td>8134.97</td>\n",
       "      <td>8.3914</td>\n",
       "      <td>0.03</td>\n",
       "      <td>391</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.85</td>\n",
       "      <td>23.3952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.54</td>\n",
       "      <td>1580.89</td>\n",
       "      <td>1400.89</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.09</td>\n",
       "      <td>2388.06</td>\n",
       "      <td>8125.93</td>\n",
       "      <td>8.4213</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.05</td>\n",
       "      <td>23.3224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>-0.0036</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.88</td>\n",
       "      <td>1593.29</td>\n",
       "      <td>1412.28</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.03</td>\n",
       "      <td>2388.05</td>\n",
       "      <td>8134.15</td>\n",
       "      <td>8.4353</td>\n",
       "      <td>0.03</td>\n",
       "      <td>391</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.10</td>\n",
       "      <td>23.4521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.0025</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.07</td>\n",
       "      <td>1585.25</td>\n",
       "      <td>1398.64</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.00</td>\n",
       "      <td>2388.06</td>\n",
       "      <td>8134.08</td>\n",
       "      <td>8.4093</td>\n",
       "      <td>0.03</td>\n",
       "      <td>391</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.87</td>\n",
       "      <td>23.3820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.04</td>\n",
       "      <td>1581.03</td>\n",
       "      <td>1403.83</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>521.95</td>\n",
       "      <td>2388.06</td>\n",
       "      <td>8132.38</td>\n",
       "      <td>8.3919</td>\n",
       "      <td>0.03</td>\n",
       "      <td>391</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.06</td>\n",
       "      <td>23.3609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.54</td>\n",
       "      <td>1587.43</td>\n",
       "      <td>1397.82</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.01</td>\n",
       "      <td>2388.06</td>\n",
       "      <td>8132.33</td>\n",
       "      <td>8.3984</td>\n",
       "      <td>0.03</td>\n",
       "      <td>391</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.11</td>\n",
       "      <td>23.3845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>-0.0056</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.94</td>\n",
       "      <td>1589.09</td>\n",
       "      <td>1403.94</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.37</td>\n",
       "      <td>2388.03</td>\n",
       "      <td>8131.12</td>\n",
       "      <td>8.4166</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.08</td>\n",
       "      <td>23.3677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.23</td>\n",
       "      <td>1583.16</td>\n",
       "      <td>1402.88</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>521.95</td>\n",
       "      <td>2388.06</td>\n",
       "      <td>8130.30</td>\n",
       "      <td>8.4293</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.03</td>\n",
       "      <td>23.4572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.50</td>\n",
       "      <td>1584.81</td>\n",
       "      <td>1398.79</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>521.38</td>\n",
       "      <td>2388.00</td>\n",
       "      <td>8133.62</td>\n",
       "      <td>8.4163</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.04</td>\n",
       "      <td>23.3672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.0018</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.32</td>\n",
       "      <td>1584.51</td>\n",
       "      <td>1407.76</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.16</td>\n",
       "      <td>2388.10</td>\n",
       "      <td>8133.83</td>\n",
       "      <td>8.4300</td>\n",
       "      <td>0.03</td>\n",
       "      <td>390</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.87</td>\n",
       "      <td>23.3484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.19</td>\n",
       "      <td>1582.70</td>\n",
       "      <td>1404.12</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.09</td>\n",
       "      <td>2388.02</td>\n",
       "      <td>8126.78</td>\n",
       "      <td>8.4577</td>\n",
       "      <td>0.03</td>\n",
       "      <td>391</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.09</td>\n",
       "      <td>23.3409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.59</td>\n",
       "      <td>1586.53</td>\n",
       "      <td>1403.69</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.14</td>\n",
       "      <td>2388.06</td>\n",
       "      <td>8133.22</td>\n",
       "      <td>8.4323</td>\n",
       "      <td>0.03</td>\n",
       "      <td>391</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.96</td>\n",
       "      <td>23.4481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.43</td>\n",
       "      <td>1585.58</td>\n",
       "      <td>1402.30</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.06</td>\n",
       "      <td>2388.01</td>\n",
       "      <td>8129.31</td>\n",
       "      <td>8.3892</td>\n",
       "      <td>0.03</td>\n",
       "      <td>391</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.06</td>\n",
       "      <td>23.3809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.61</td>\n",
       "      <td>1587.78</td>\n",
       "      <td>1400.70</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.28</td>\n",
       "      <td>2388.05</td>\n",
       "      <td>8128.59</td>\n",
       "      <td>8.4099</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.00</td>\n",
       "      <td>23.3325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.70</td>\n",
       "      <td>1583.30</td>\n",
       "      <td>1399.20</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.05</td>\n",
       "      <td>2388.11</td>\n",
       "      <td>8126.86</td>\n",
       "      <td>8.4174</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.96</td>\n",
       "      <td>23.4025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.45</td>\n",
       "      <td>1582.78</td>\n",
       "      <td>1404.06</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>521.41</td>\n",
       "      <td>2388.04</td>\n",
       "      <td>8128.89</td>\n",
       "      <td>8.4557</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.94</td>\n",
       "      <td>23.3770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.12</td>\n",
       "      <td>1587.51</td>\n",
       "      <td>1395.09</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.00</td>\n",
       "      <td>2388.06</td>\n",
       "      <td>8130.97</td>\n",
       "      <td>8.4116</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.10</td>\n",
       "      <td>23.3186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>-0.0006</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.32</td>\n",
       "      <td>1594.29</td>\n",
       "      <td>1400.15</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.06</td>\n",
       "      <td>2388.12</td>\n",
       "      <td>8130.70</td>\n",
       "      <td>8.4074</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.94</td>\n",
       "      <td>23.3971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.25</td>\n",
       "      <td>1582.43</td>\n",
       "      <td>1400.23</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.26</td>\n",
       "      <td>2388.08</td>\n",
       "      <td>8128.65</td>\n",
       "      <td>8.4007</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.96</td>\n",
       "      <td>23.3785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.48</td>\n",
       "      <td>1583.28</td>\n",
       "      <td>1408.07</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>521.95</td>\n",
       "      <td>2388.07</td>\n",
       "      <td>8129.12</td>\n",
       "      <td>8.3949</td>\n",
       "      <td>0.03</td>\n",
       "      <td>391</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.77</td>\n",
       "      <td>23.3557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>-0.0007</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.08</td>\n",
       "      <td>1586.65</td>\n",
       "      <td>1400.31</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>521.82</td>\n",
       "      <td>2388.02</td>\n",
       "      <td>8127.24</td>\n",
       "      <td>8.4494</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.87</td>\n",
       "      <td>23.3931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.93</td>\n",
       "      <td>1594.25</td>\n",
       "      <td>1401.29</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>521.84</td>\n",
       "      <td>2388.07</td>\n",
       "      <td>8134.89</td>\n",
       "      <td>8.4470</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.83</td>\n",
       "      <td>23.3502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.95</td>\n",
       "      <td>1587.15</td>\n",
       "      <td>1398.11</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.39</td>\n",
       "      <td>2388.07</td>\n",
       "      <td>8133.13</td>\n",
       "      <td>8.4212</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.02</td>\n",
       "      <td>23.3621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>-0.0025</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.79</td>\n",
       "      <td>1585.72</td>\n",
       "      <td>1400.97</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>521.78</td>\n",
       "      <td>2388.10</td>\n",
       "      <td>8134.79</td>\n",
       "      <td>8.4110</td>\n",
       "      <td>0.03</td>\n",
       "      <td>391</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.09</td>\n",
       "      <td>23.4069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>-0.0006</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.58</td>\n",
       "      <td>1581.22</td>\n",
       "      <td>1398.91</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>521.79</td>\n",
       "      <td>2388.06</td>\n",
       "      <td>8130.11</td>\n",
       "      <td>8.4024</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.81</td>\n",
       "      <td>23.3552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    EngineID  Cycle  OpSetting1  OpSetting2  OpSetting3      T2     T24  \\\n",
       "0          1      1      0.0023      0.0003       100.0  518.67  643.02   \n",
       "1          1      2     -0.0027     -0.0003       100.0  518.67  641.71   \n",
       "2          1      3      0.0003      0.0001       100.0  518.67  642.46   \n",
       "3          1      4      0.0042      0.0000       100.0  518.67  642.44   \n",
       "4          1      5      0.0014      0.0000       100.0  518.67  642.51   \n",
       "5          1      6      0.0012      0.0003       100.0  518.67  642.11   \n",
       "6          1      7     -0.0000      0.0002       100.0  518.67  642.11   \n",
       "7          1      8      0.0006     -0.0000       100.0  518.67  642.54   \n",
       "8          1      9     -0.0036      0.0000       100.0  518.67  641.88   \n",
       "9          1     10     -0.0025     -0.0001       100.0  518.67  642.07   \n",
       "10         1     11      0.0007     -0.0004       100.0  518.67  642.04   \n",
       "11         1     12      0.0026      0.0003       100.0  518.67  642.54   \n",
       "12         1     13     -0.0056      0.0003       100.0  518.67  641.94   \n",
       "13         1     14      0.0017     -0.0004       100.0  518.67  642.23   \n",
       "14         1     15     -0.0003     -0.0003       100.0  518.67  642.50   \n",
       "15         1     16     -0.0018      0.0003       100.0  518.67  642.32   \n",
       "16         1     17      0.0014      0.0002       100.0  518.67  642.19   \n",
       "17         1     18      0.0035      0.0001       100.0  518.67  642.59   \n",
       "18         1     19      0.0029      0.0001       100.0  518.67  642.43   \n",
       "19         1     20      0.0011     -0.0001       100.0  518.67  642.61   \n",
       "20         1     21      0.0038     -0.0002       100.0  518.67  642.70   \n",
       "21         1     22      0.0012      0.0001       100.0  518.67  642.45   \n",
       "22         1     23      0.0009     -0.0000       100.0  518.67  642.12   \n",
       "23         1     24     -0.0006     -0.0001       100.0  518.67  642.32   \n",
       "24         1     25      0.0028     -0.0003       100.0  518.67  642.25   \n",
       "25         1     26      0.0047     -0.0005       100.0  518.67  642.48   \n",
       "26         1     27     -0.0007      0.0001       100.0  518.67  642.08   \n",
       "27         1     28      0.0022      0.0005       100.0  518.67  641.93   \n",
       "28         1     29      0.0014      0.0001       100.0  518.67  641.95   \n",
       "29         1     30     -0.0025      0.0004       100.0  518.67  642.79   \n",
       "30         1     31     -0.0006      0.0004       100.0  518.67  642.58   \n",
       "\n",
       "        T30      T50     P2   ...        phi      NRf      NRc     BPR  farB  \\\n",
       "0   1585.29  1398.21  14.62   ...     521.72  2388.03  8125.55  8.4052  0.03   \n",
       "1   1588.45  1395.42  14.62   ...     522.16  2388.06  8139.62  8.3803  0.03   \n",
       "2   1586.94  1401.34  14.62   ...     521.97  2388.03  8130.10  8.4441  0.03   \n",
       "3   1584.12  1406.42  14.62   ...     521.38  2388.05  8132.90  8.3917  0.03   \n",
       "4   1587.19  1401.92  14.62   ...     522.15  2388.03  8129.54  8.4031  0.03   \n",
       "5   1579.12  1395.13  14.62   ...     521.92  2388.08  8127.46  8.4238  0.03   \n",
       "6   1583.34  1404.84  14.62   ...     522.01  2388.06  8134.97  8.3914  0.03   \n",
       "7   1580.89  1400.89  14.62   ...     522.09  2388.06  8125.93  8.4213  0.03   \n",
       "8   1593.29  1412.28  14.62   ...     522.03  2388.05  8134.15  8.4353  0.03   \n",
       "9   1585.25  1398.64  14.62   ...     522.00  2388.06  8134.08  8.4093  0.03   \n",
       "10  1581.03  1403.83  14.62   ...     521.95  2388.06  8132.38  8.3919  0.03   \n",
       "11  1587.43  1397.82  14.62   ...     522.01  2388.06  8132.33  8.3984  0.03   \n",
       "12  1589.09  1403.94  14.62   ...     522.37  2388.03  8131.12  8.4166  0.03   \n",
       "13  1583.16  1402.88  14.62   ...     521.95  2388.06  8130.30  8.4293  0.03   \n",
       "14  1584.81  1398.79  14.62   ...     521.38  2388.00  8133.62  8.4163  0.03   \n",
       "15  1584.51  1407.76  14.62   ...     522.16  2388.10  8133.83  8.4300  0.03   \n",
       "16  1582.70  1404.12  14.62   ...     522.09  2388.02  8126.78  8.4577  0.03   \n",
       "17  1586.53  1403.69  14.62   ...     522.14  2388.06  8133.22  8.4323  0.03   \n",
       "18  1585.58  1402.30  14.62   ...     522.06  2388.01  8129.31  8.3892  0.03   \n",
       "19  1587.78  1400.70  14.62   ...     522.28  2388.05  8128.59  8.4099  0.03   \n",
       "20  1583.30  1399.20  14.62   ...     522.05  2388.11  8126.86  8.4174  0.03   \n",
       "21  1582.78  1404.06  14.62   ...     521.41  2388.04  8128.89  8.4557  0.03   \n",
       "22  1587.51  1395.09  14.62   ...     522.00  2388.06  8130.97  8.4116  0.03   \n",
       "23  1594.29  1400.15  14.62   ...     522.06  2388.12  8130.70  8.4074  0.03   \n",
       "24  1582.43  1400.23  14.62   ...     522.26  2388.08  8128.65  8.4007  0.03   \n",
       "25  1583.28  1408.07  14.62   ...     521.95  2388.07  8129.12  8.3949  0.03   \n",
       "26  1586.65  1400.31  14.62   ...     521.82  2388.02  8127.24  8.4494  0.03   \n",
       "27  1594.25  1401.29  14.62   ...     521.84  2388.07  8134.89  8.4470  0.03   \n",
       "28  1587.15  1398.11  14.62   ...     522.39  2388.07  8133.13  8.4212  0.03   \n",
       "29  1585.72  1400.97  14.62   ...     521.78  2388.10  8134.79  8.4110  0.03   \n",
       "30  1581.22  1398.91  14.62   ...     521.79  2388.06  8130.11  8.4024  0.03   \n",
       "\n",
       "    htBleed  Nf_dmd  PCNfR_dmd    W31      W32  \n",
       "0       392    2388      100.0  38.86  23.3735  \n",
       "1       393    2388      100.0  39.02  23.3916  \n",
       "2       393    2388      100.0  39.08  23.4166  \n",
       "3       391    2388      100.0  39.00  23.3737  \n",
       "4       390    2388      100.0  38.99  23.4130  \n",
       "5       392    2388      100.0  38.91  23.3467  \n",
       "6       391    2388      100.0  38.85  23.3952  \n",
       "7       393    2388      100.0  39.05  23.3224  \n",
       "8       391    2388      100.0  39.10  23.4521  \n",
       "9       391    2388      100.0  38.87  23.3820  \n",
       "10      391    2388      100.0  39.06  23.3609  \n",
       "11      391    2388      100.0  39.11  23.3845  \n",
       "12      392    2388      100.0  39.08  23.3677  \n",
       "13      392    2388      100.0  39.03  23.4572  \n",
       "14      392    2388      100.0  39.04  23.3672  \n",
       "15      390    2388      100.0  38.87  23.3484  \n",
       "16      391    2388      100.0  39.09  23.3409  \n",
       "17      391    2388      100.0  38.96  23.4481  \n",
       "18      391    2388      100.0  39.06  23.3809  \n",
       "19      392    2388      100.0  39.00  23.3325  \n",
       "20      392    2388      100.0  38.96  23.4025  \n",
       "21      392    2388      100.0  38.94  23.3770  \n",
       "22      393    2388      100.0  39.10  23.3186  \n",
       "23      393    2388      100.0  38.94  23.3971  \n",
       "24      393    2388      100.0  38.96  23.3785  \n",
       "25      391    2388      100.0  38.77  23.3557  \n",
       "26      392    2388      100.0  38.87  23.3931  \n",
       "27      392    2388      100.0  38.83  23.3502  \n",
       "28      392    2388      100.0  39.02  23.3621  \n",
       "29      391    2388      100.0  39.09  23.4069  \n",
       "30      393    2388      100.0  38.81  23.3552  \n",
       "\n",
       "[31 rows x 26 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('data/raw/test_FD001.txt', sep='\\s+', names=featurenames)\n",
    "test.loc[test['EngineID']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/syahrulhamdani/anaconda3/envs/100DaysOfMLCode/lib/python3.6/site-packages/ipykernel_launcher.py:1: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.72808356, -1.56517022,  1.0555991 ,  1.01567674,  0.        ,\n",
       "         0.        ,  0.6780771 , -0.85355045, -1.19148011, -1.        ,\n",
       "         0.14168333,  0.60140814, -0.798093  , -0.68257883,  0.        ,\n",
       "        -1.27739615,  0.41561385, -0.91984139, -0.95423545, -0.98510705,\n",
       "        -1.        , -0.78170979,  0.        ,  0.        ,  0.24194337,\n",
       "         0.77409693],\n",
       "       [-1.72808356, -1.55065208, -1.23036553, -1.03172035,  0.        ,\n",
       "         0.        , -1.94170729, -0.33813706, -1.50146679, -1.        ,\n",
       "         0.14168333,  1.67476859, -1.22072489, -0.49011738,  0.        ,\n",
       "        -0.15414109,  1.01219529, -0.50269485, -0.21664836, -1.64903395,\n",
       "        -1.        , -0.13601757,  0.        ,  0.        ,  1.12718287,\n",
       "         0.94130518],\n",
       "       [-1.72808356, -1.53613393,  0.14121325,  0.33321104,  0.        ,\n",
       "         0.        , -0.44183073, -0.58442637, -0.84371727, -1.        ,\n",
       "         0.14168333,  0.83867729, -0.6572157 , -0.37509336,  0.        ,\n",
       "        -0.15414109,  0.75458058, -0.91984139, -0.71571226,  0.05211208,\n",
       "        -1.        , -0.13601757,  0.        ,  0.        ,  1.45914768,\n",
       "         1.17225579],\n",
       "       [-1.72808356, -1.52161579,  1.92426566, -0.00802181,  0.        ,\n",
       "         0.        , -0.48182744, -1.04438389, -0.27929708, -1.        ,\n",
       "         0.14168333,  0.79348317, -0.9389703 , -0.90356986,  0.        ,\n",
       "        -0.97786146, -0.04538089, -0.64174369, -0.56892876, -1.34506742,\n",
       "        -1.        , -1.42740201,  0.        ,  0.        ,  1.01652793,\n",
       "         0.77594454],\n",
       "       [-1.72808356, -1.50709764,  0.64412547, -0.00802181,  0.        ,\n",
       "         0.        , -0.34183896, -0.54364999, -0.7792756 , -1.        ,\n",
       "         0.14168333,  0.89516995, -1.22072489, -0.93708079,  0.        ,\n",
       "        -0.86553596,  0.99863662, -0.91984139, -0.74506896, -1.04110089,\n",
       "        -1.        , -2.07309423,  0.        ,  0.        ,  0.96120046,\n",
       "         1.1389989 ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_test = scaler.transform(test)\n",
    "scaled_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferenceset = torch.from_numpy(scaled_train[:,2:]).type(torch.FloatTensor)\n",
    "with torch.no_grad():\n",
    "    infer_output = model(inferenceset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rul_test = pd.read_csv('data/raw/RUL_FD001.txt', sep='\\n', names=['RUL'])\n",
    "rul_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([170.1256])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer_output[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np_output = infer_output.numpy()\n",
    "np_output = np_output.reshape(np_output.shape[0]).astype(np.int)\n",
    "plt.plot(range(192), np_output[:192])\n",
    "plt.plot(range(192), train.target[:192])\n",
    "plt.ylim(bottom=0)\n",
    "plt.xlim(left=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_output[191]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([163, 175, 175, ...,   0,   0,   0])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
